<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>R for Statistical Learning</title>
  <meta name="description" content="R for Statistical Learning">
  <meta name="generator" content="bookdown 0.3.9 and GitBook 2.6.7">

  <meta property="og:title" content="R for Statistical Learning" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://daviddalpiaz.github.io/r4sl/" />
  
  
  <meta name="github-repo" content="daviddalpiaz/r4sl" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="R for Statistical Learning" />
  
  
  

<meta name="author" content="David Dalpiaz">


<meta name="date" content="2017-02-08">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  <link rel="shortcut icon" href="favicon.ico" type="image/x-icon">
<link rel="prev" href="classification.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">R for Statistical Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a><ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#about-this-book"><i class="fa fa-check"></i><b>0.1</b> About This Book</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#conventions"><i class="fa fa-check"></i><b>0.2</b> Conventions</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i><b>0.3</b> Acknowledgements</a></li>
<li class="chapter" data-level="0.4" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i><b>0.4</b> License</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="probability-review.html"><a href="probability-review.html"><i class="fa fa-check"></i><b>1</b> Probability Review</a><ul>
<li class="chapter" data-level="1.1" data-path="probability-review.html"><a href="probability-review.html#probability-models"><i class="fa fa-check"></i><b>1.1</b> Probability Models</a></li>
<li class="chapter" data-level="1.2" data-path="probability-review.html"><a href="probability-review.html#probability-axioms"><i class="fa fa-check"></i><b>1.2</b> Probability Axioms</a></li>
<li class="chapter" data-level="1.3" data-path="probability-review.html"><a href="probability-review.html#probability-rules"><i class="fa fa-check"></i><b>1.3</b> Probability Rules</a></li>
<li class="chapter" data-level="1.4" data-path="probability-review.html"><a href="probability-review.html#random-variables"><i class="fa fa-check"></i><b>1.4</b> Random Variables</a><ul>
<li class="chapter" data-level="1.4.1" data-path="probability-review.html"><a href="probability-review.html#distributions"><i class="fa fa-check"></i><b>1.4.1</b> Distributions</a></li>
<li class="chapter" data-level="1.4.2" data-path="probability-review.html"><a href="probability-review.html#discrete-random-variables"><i class="fa fa-check"></i><b>1.4.2</b> Discrete Random Variables</a></li>
<li class="chapter" data-level="1.4.3" data-path="probability-review.html"><a href="probability-review.html#continuous-random-variables"><i class="fa fa-check"></i><b>1.4.3</b> Continuous Random Variables</a></li>
<li class="chapter" data-level="1.4.4" data-path="probability-review.html"><a href="probability-review.html#several-random-variables"><i class="fa fa-check"></i><b>1.4.4</b> Several Random Variables</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="probability-review.html"><a href="probability-review.html#expectations"><i class="fa fa-check"></i><b>1.5</b> Expectations</a></li>
<li class="chapter" data-level="1.6" data-path="probability-review.html"><a href="probability-review.html#likelihood"><i class="fa fa-check"></i><b>1.6</b> Likelihood</a></li>
<li class="chapter" data-level="1.7" data-path="probability-review.html"><a href="probability-review.html#references"><i class="fa fa-check"></i><b>1.7</b> References</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction-to-r.html"><a href="introduction-to-r.html"><i class="fa fa-check"></i><b>2</b> Introduction to <code>R</code></a><ul>
<li class="chapter" data-level="2.1" data-path="introduction-to-r.html"><a href="introduction-to-r.html#r-resources"><i class="fa fa-check"></i><b>2.1</b> <code>R</code> Resources</a></li>
<li class="chapter" data-level="2.2" data-path="introduction-to-r.html"><a href="introduction-to-r.html#r-basics"><i class="fa fa-check"></i><b>2.2</b> <code>R</code> Basics</a><ul>
<li class="chapter" data-level="2.2.1" data-path="introduction-to-r.html"><a href="introduction-to-r.html#basic-calculations"><i class="fa fa-check"></i><b>2.2.1</b> Basic Calculations</a></li>
<li class="chapter" data-level="2.2.2" data-path="introduction-to-r.html"><a href="introduction-to-r.html#getting-help"><i class="fa fa-check"></i><b>2.2.2</b> Getting Help</a></li>
<li class="chapter" data-level="2.2.3" data-path="introduction-to-r.html"><a href="introduction-to-r.html#installing-packages"><i class="fa fa-check"></i><b>2.2.3</b> Installing Packages</a></li>
<li class="chapter" data-level="2.2.4" data-path="introduction-to-r.html"><a href="introduction-to-r.html#data-types"><i class="fa fa-check"></i><b>2.2.4</b> Data Types</a></li>
<li class="chapter" data-level="2.2.5" data-path="introduction-to-r.html"><a href="introduction-to-r.html#vectors"><i class="fa fa-check"></i><b>2.2.5</b> Vectors</a></li>
<li class="chapter" data-level="2.2.6" data-path="introduction-to-r.html"><a href="introduction-to-r.html#summary-statistics"><i class="fa fa-check"></i><b>2.2.6</b> Summary Statistics</a></li>
<li class="chapter" data-level="2.2.7" data-path="introduction-to-r.html"><a href="introduction-to-r.html#matrices"><i class="fa fa-check"></i><b>2.2.7</b> Matrices</a></li>
<li class="chapter" data-level="2.2.8" data-path="introduction-to-r.html"><a href="introduction-to-r.html#data-frames"><i class="fa fa-check"></i><b>2.2.8</b> Data Frames</a></li>
<li class="chapter" data-level="2.2.9" data-path="introduction-to-r.html"><a href="introduction-to-r.html#plotting"><i class="fa fa-check"></i><b>2.2.9</b> Plotting</a></li>
<li class="chapter" data-level="2.2.10" data-path="introduction-to-r.html"><a href="introduction-to-r.html#distributions-1"><i class="fa fa-check"></i><b>2.2.10</b> Distributions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="introduction-to-r.html"><a href="introduction-to-r.html#programming-basics"><i class="fa fa-check"></i><b>2.3</b> Programming Basics</a><ul>
<li class="chapter" data-level="2.3.1" data-path="introduction-to-r.html"><a href="introduction-to-r.html#logical-operators"><i class="fa fa-check"></i><b>2.3.1</b> Logical Operators</a></li>
<li class="chapter" data-level="2.3.2" data-path="introduction-to-r.html"><a href="introduction-to-r.html#control-flow"><i class="fa fa-check"></i><b>2.3.2</b> Control Flow</a></li>
<li class="chapter" data-level="2.3.3" data-path="introduction-to-r.html"><a href="introduction-to-r.html#functions"><i class="fa fa-check"></i><b>2.3.3</b> Functions</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="introduction-to-r.html"><a href="introduction-to-r.html#hypothesis-tests-in-r"><i class="fa fa-check"></i><b>2.4</b> Hypothesis Tests in <code>R</code></a><ul>
<li class="chapter" data-level="2.4.1" data-path="introduction-to-r.html"><a href="introduction-to-r.html#one-sample-t-test-review"><i class="fa fa-check"></i><b>2.4.1</b> One Sample t-Test: Review</a></li>
<li class="chapter" data-level="2.4.2" data-path="introduction-to-r.html"><a href="introduction-to-r.html#one-sample-t-test-example"><i class="fa fa-check"></i><b>2.4.2</b> One Sample t-Test: Example</a></li>
<li class="chapter" data-level="2.4.3" data-path="introduction-to-r.html"><a href="introduction-to-r.html#two-sample-t-test-review"><i class="fa fa-check"></i><b>2.4.3</b> Two Sample t-Test: Review</a></li>
<li class="chapter" data-level="2.4.4" data-path="introduction-to-r.html"><a href="introduction-to-r.html#two-sample-t-test-example"><i class="fa fa-check"></i><b>2.4.4</b> Two Sample t-Test: Example</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="introduction-to-r.html"><a href="introduction-to-r.html#simulation"><i class="fa fa-check"></i><b>2.5</b> Simulation</a><ul>
<li class="chapter" data-level="2.5.1" data-path="introduction-to-r.html"><a href="introduction-to-r.html#paired-differences"><i class="fa fa-check"></i><b>2.5.1</b> Paired Differences</a></li>
<li class="chapter" data-level="2.5.2" data-path="introduction-to-r.html"><a href="introduction-to-r.html#distribution-of-a-sample-mean"><i class="fa fa-check"></i><b>2.5.2</b> Distribution of a Sample Mean</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="rstudio-and-rmarkdown.html"><a href="rstudio-and-rmarkdown.html"><i class="fa fa-check"></i><b>3</b> RStudio and RMarkdown</a><ul>
<li class="chapter" data-level="3.1" data-path="rstudio-and-rmarkdown.html"><a href="rstudio-and-rmarkdown.html#template"><i class="fa fa-check"></i><b>3.1</b> Template</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="regression-basics-in-r.html"><a href="regression-basics-in-r.html"><i class="fa fa-check"></i><b>4</b> Regression Basics in <code>R</code></a><ul>
<li class="chapter" data-level="4.1" data-path="regression-basics-in-r.html"><a href="regression-basics-in-r.html#visualization-for-regression"><i class="fa fa-check"></i><b>4.1</b> Visualization for Regression</a></li>
<li class="chapter" data-level="4.2" data-path="regression-basics-in-r.html"><a href="regression-basics-in-r.html#the-lm-function"><i class="fa fa-check"></i><b>4.2</b> The <code>lm()</code> Function</a></li>
<li class="chapter" data-level="4.3" data-path="regression-basics-in-r.html"><a href="regression-basics-in-r.html#hypothesis-testing"><i class="fa fa-check"></i><b>4.3</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="4.4" data-path="regression-basics-in-r.html"><a href="regression-basics-in-r.html#prediction"><i class="fa fa-check"></i><b>4.4</b> Prediction</a></li>
<li class="chapter" data-level="4.5" data-path="regression-basics-in-r.html"><a href="regression-basics-in-r.html#unusual-observations"><i class="fa fa-check"></i><b>4.5</b> Unusual Observations</a></li>
<li class="chapter" data-level="4.6" data-path="regression-basics-in-r.html"><a href="regression-basics-in-r.html#adding-complexity"><i class="fa fa-check"></i><b>4.6</b> Adding Complexity</a><ul>
<li class="chapter" data-level="4.6.1" data-path="regression-basics-in-r.html"><a href="regression-basics-in-r.html#interactions"><i class="fa fa-check"></i><b>4.6.1</b> Interactions</a></li>
<li class="chapter" data-level="4.6.2" data-path="regression-basics-in-r.html"><a href="regression-basics-in-r.html#polynomials"><i class="fa fa-check"></i><b>4.6.2</b> Polynomials</a></li>
<li class="chapter" data-level="4.6.3" data-path="regression-basics-in-r.html"><a href="regression-basics-in-r.html#transformations"><i class="fa fa-check"></i><b>4.6.3</b> Transformations</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="regression-for-statistical-learning.html"><a href="regression-for-statistical-learning.html"><i class="fa fa-check"></i><b>5</b> Regression for Statistical Learning</a><ul>
<li class="chapter" data-level="5.1" data-path="regression-for-statistical-learning.html"><a href="regression-for-statistical-learning.html#assesing-model-accuracy"><i class="fa fa-check"></i><b>5.1</b> Assesing Model Accuracy</a></li>
<li class="chapter" data-level="5.2" data-path="regression-for-statistical-learning.html"><a href="regression-for-statistical-learning.html#model-complexity"><i class="fa fa-check"></i><b>5.2</b> Model Complexity</a></li>
<li class="chapter" data-level="5.3" data-path="regression-for-statistical-learning.html"><a href="regression-for-statistical-learning.html#test-train-split"><i class="fa fa-check"></i><b>5.3</b> Test-Train Split</a></li>
<li class="chapter" data-level="5.4" data-path="regression-for-statistical-learning.html"><a href="regression-for-statistical-learning.html#adding-flexibility-to-linear-models"><i class="fa fa-check"></i><b>5.4</b> Adding Flexibility to Linear Models</a></li>
<li class="chapter" data-level="5.5" data-path="regression-for-statistical-learning.html"><a href="regression-for-statistical-learning.html#choosing-a-model"><i class="fa fa-check"></i><b>5.5</b> Choosing a Model</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="simulating-the-biasvariance-tradeoff.html"><a href="simulating-the-biasvariance-tradeoff.html"><i class="fa fa-check"></i><b>6</b> Simulating the Bias–Variance Tradeoff</a><ul>
<li class="chapter" data-level="6.1" data-path="simulating-the-biasvariance-tradeoff.html"><a href="simulating-the-biasvariance-tradeoff.html#bias-variance-decomposition"><i class="fa fa-check"></i><b>6.1</b> Bias-Variance Decomposition</a></li>
<li class="chapter" data-level="6.2" data-path="simulating-the-biasvariance-tradeoff.html"><a href="simulating-the-biasvariance-tradeoff.html#simulation-1"><i class="fa fa-check"></i><b>6.2</b> Simulation</a></li>
<li class="chapter" data-level="6.3" data-path="simulating-the-biasvariance-tradeoff.html"><a href="simulating-the-biasvariance-tradeoff.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>6.3</b> Bias-Variance Tradeoff</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>7</b> Classification</a><ul>
<li class="chapter" data-level="7.1" data-path="classification.html"><a href="classification.html#visualization-for-classification"><i class="fa fa-check"></i><b>7.1</b> Visualization for Classification</a></li>
<li class="chapter" data-level="7.2" data-path="classification.html"><a href="classification.html#a-simple-classifier"><i class="fa fa-check"></i><b>7.2</b> A Simple Classifier</a></li>
<li class="chapter" data-level="7.3" data-path="classification.html"><a href="classification.html#metrics-for-classification"><i class="fa fa-check"></i><b>7.3</b> Metrics for Classification</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>8</b> Logistic Regression</a><ul>
<li class="chapter" data-level="8.1" data-path="logistic-regression.html"><a href="logistic-regression.html#linear-regression"><i class="fa fa-check"></i><b>8.1</b> Linear Regression</a></li>
<li class="chapter" data-level="8.2" data-path="logistic-regression.html"><a href="logistic-regression.html#bayes-classifier"><i class="fa fa-check"></i><b>8.2</b> Bayes Classifier</a></li>
<li class="chapter" data-level="8.3" data-path="logistic-regression.html"><a href="logistic-regression.html#logistic-regression-with-glm"><i class="fa fa-check"></i><b>8.3</b> Logistic Regression with <code>glm()</code></a></li>
<li class="chapter" data-level="8.4" data-path="logistic-regression.html"><a href="logistic-regression.html#roc-curves"><i class="fa fa-check"></i><b>8.4</b> ROC Curves</a></li>
<li class="chapter" data-level="8.5" data-path="logistic-regression.html"><a href="logistic-regression.html#multinomial-logistic-regression"><i class="fa fa-check"></i><b>8.5</b> Multinomial Logistic Regression</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/daviddalpiaz/r4sl" target="blank">&copy; 2017 David Dalpiaz</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">R for Statistical Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="logistic-regression" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> Logistic Regression</h1>
<p>In this chapter, we continue our discussion of classification. We introduce our first model for classification, logistic regression. To begin, we return to the <code>Default</code> dataset from the previous chapter.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ISLR)
<span class="kw">library</span>(tibble)
<span class="kw">as_tibble</span>(Default)</code></pre></div>
<pre><code>## # A tibble: 10,000 × 4
##    default student   balance    income
##     &lt;fctr&gt;  &lt;fctr&gt;     &lt;dbl&gt;     &lt;dbl&gt;
## 1       No      No  729.5265 44361.625
## 2       No     Yes  817.1804 12106.135
## 3       No      No 1073.5492 31767.139
## 4       No      No  529.2506 35704.494
## 5       No      No  785.6559 38463.496
## 6       No     Yes  919.5885  7491.559
## 7       No      No  825.5133 24905.227
## 8       No     Yes  808.6675 17600.451
## 9       No      No 1161.0579 37468.529
## 10      No      No    0.0000 29275.268
## # ... with 9,990 more rows</code></pre>
<p>We also repeat the test-train split from the previous chapter.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">42</span>)
default_index =<span class="st"> </span><span class="kw">sample</span>(<span class="kw">nrow</span>(Default), <span class="dv">5000</span>)
default_train =<span class="st"> </span>Default[default_index, ]
default_test =<span class="st"> </span>Default[-default_index, ]</code></pre></div>
<div id="linear-regression" class="section level2">
<h2><span class="header-section-number">8.1</span> Linear Regression</h2>
<p>Before moving on to logistic regression, why not plain, old, linear regression?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">default_train_lm =<span class="st"> </span>default_train
default_test_lm =<span class="st"> </span>default_test</code></pre></div>
<p>Since linear regression expects a numeric response variable, we coerce the response to be numeric. (Notice that we also shift the results, as we require <code>0</code> and <code>1</code>, not <code>1</code> and <code>2</code>.) Notice we have also copied the dataset so that we can return the original data with factors later.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">default_train_lm$default =<span class="st"> </span><span class="kw">as.numeric</span>(default_train_lm$default) -<span class="st"> </span><span class="dv">1</span>
default_test_lm$default =<span class="st"> </span><span class="kw">as.numeric</span>(default_test_lm$default) -<span class="st"> </span><span class="dv">1</span></code></pre></div>
<p>Why would we think this should work? Recall that,</p>
<p><span class="math display">\[
\hat{E}[Y \mid X = x] = X\hat{\beta}.
\]</span></p>
<p>Since <span class="math inline">\(Y\)</span> is limited to values of <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>, we have</p>
<p><span class="math display">\[
E[Y \mid X = x] = P[Y = 1 \mid X = x].
\]</span></p>
<p>It would then seem reasonable that <span class="math inline">\(X\hat{\beta}\)</span> is a reasonable estimate of <span class="math inline">\(P[Y = 1 \ X = x]\)</span>. We test this on the <code>Default</code> data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model_lm =<span class="st"> </span><span class="kw">lm</span>(default ~<span class="st"> </span>balance, <span class="dt">data =</span> default_train_lm)</code></pre></div>
<p>Everything seems to be working, until we plot the results.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(default ~<span class="st"> </span>balance, <span class="dt">data =</span> default_train_lm, 
     <span class="dt">col =</span> <span class="st">&quot;darkorange&quot;</span>, <span class="dt">pch =</span> <span class="st">&quot;|&quot;</span>, <span class="dt">ylim =</span> <span class="kw">c</span>(-<span class="fl">0.2</span>, <span class="dv">1</span>),
     <span class="dt">main =</span> <span class="st">&quot;Using Linear Regression for Classification&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h =</span> <span class="dv">0</span>, <span class="dt">lty =</span> <span class="dv">3</span>)
<span class="kw">abline</span>(<span class="dt">h =</span> <span class="dv">1</span>, <span class="dt">lty =</span> <span class="dv">3</span>)
<span class="kw">abline</span>(<span class="dt">h =</span> <span class="fl">0.5</span>, <span class="dt">lty =</span> <span class="dv">2</span>)
<span class="kw">abline</span>(model_lm, <span class="dt">lwd =</span> <span class="dv">3</span>, <span class="dt">col =</span> <span class="st">&quot;dodgerblue&quot;</span>)</code></pre></div>
<p><img src="08-logistic_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>Two issues arise. First, all of the predicted probabilities are below 0.5 That means, we would classify every observation as a <code>&quot;No&quot;</code>. This is certainly possible, but not what we would expect.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">all</span>(<span class="kw">predict</span>(model_lm) &lt;<span class="st"> </span><span class="fl">0.5</span>)</code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<p>The next, and bigger issue, is predicted probabilities less than 0.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">any</span>(<span class="kw">predict</span>(model_lm) &lt;<span class="st"> </span><span class="dv">0</span>)</code></pre></div>
<pre><code>## [1] TRUE</code></pre>
</div>
<div id="bayes-classifier" class="section level2">
<h2><span class="header-section-number">8.2</span> Bayes Classifier</h2>
<p>Why are we using a predicted probability of 0.5 as the cutoff for classification? Recall, the Bayes Classifier, which minimizes the classification error:</p>
<p><span class="math display">\[
C^B({\bf x}) = \underset{k}{\mathrm{argmax}} \ P[Y = k \mid {\bf X = x}]
\]</span></p>
<p>So, in the binary classification problem, we will use predicted probabilities</p>
<p><span class="math display">\[
\hat{p}({\bf x}) = \hat{P}[Y = 1 \mid {\bf X = x}]
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\hat{P}[Y = 0 \mid {\bf X = x}]
\]</span></p>
<p>and then classify to the larger of the two. We actually only need to consider a single probability, usually for <span class="math inline">\(\hat{P}[Y = 1 \mid {\bf X = x}]\)</span>. Since we use it so often, we give it the shorthand notation, <span class="math inline">\(\hat{p}({\bf x})\)</span>. Then the classifier is written,</p>
<p><span class="math display">\[
\hat{C}(\bf x) = 
\begin{cases} 
      1 &amp; \hat{p}({\bf x}) &gt; 0.5 \\
      0 &amp; \hat{p}({\bf x}) \leq 0.5 
\end{cases}
\]</span></p>
</div>
<div id="logistic-regression-with-glm" class="section level2">
<h2><span class="header-section-number">8.3</span> Logistic Regression with <code>glm()</code></h2>
<p>To better estimate the probability</p>
<p><span class="math display">\[
p({\bf x}) = P[Y = 1 \mid {\bf X = x}]
\]</span> we turn to logistic regression. The model is written</p>
<p><span class="math display">\[
\log\left(\frac{p({\bf x})}{1 - p({\bf x})}\right) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots  + \beta_p x_p.
\]</span></p>
<p>Rearranging, we see the probabilities can be written as</p>
<p><span class="math display">\[
p({\bf x}) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots  + \beta_p x_p)}} = \sigma(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots  + \beta_p x_p)
\]</span></p>
<p>Notice, we use the sigmoid function as shorthand notation, which appears often in deep learning literature. It takes any real input, and outputs a number between 0 and 1. How useful!</p>
<p><span class="math display">\[
\sigma(x) = \frac{1}{1 + e^{-x}}
\]</span></p>
<p>The model is fit by numerically maximizing the likelihood, which we will let <code>R</code> take care of.</p>
<p>We start with a single predictor example, again using <code>balance</code> as our single predictor.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model_glm =<span class="st"> </span><span class="kw">glm</span>(default ~<span class="st"> </span>balance, <span class="dt">data =</span> default_train, <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>)</code></pre></div>
<p>Fitting this model looks very similar to fitting a simple linear regression. Instead of <code>lm()</code> we use <code>glm()</code>. The only other difference is the use of <code>family = &quot;binomial&quot;</code> which indicates that we have a two-class categorical response. Using <code>glm()</code> with <code>family = &quot;gaussian&quot;</code> would perform the usual linear regression.</p>
<p>First, we can obtain the fitted coefficients the same way we did with linear regression.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">coef</span>(model_glm)</code></pre></div>
<pre><code>##   (Intercept)       balance 
## -10.452182876   0.005367655</code></pre>
<p>The next thing we should understand is how the <code>predict()</code> function works with <code>glm()</code>. So, let’s look a some predictions.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(<span class="kw">predict</span>(model_glm))</code></pre></div>
<pre><code>##       9149       9370       2861       8302       6415       5189 
## -6.9616496 -0.7089539 -4.8936916 -9.4123620 -9.0416096 -7.3600645</code></pre>
<p>By default, <code>predict.glm()</code> uses <code>type = &quot;link&quot;</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(<span class="kw">predict</span>(model_glm, <span class="dt">type =</span> <span class="st">&quot;link&quot;</span>))</code></pre></div>
<pre><code>##       9149       9370       2861       8302       6415       5189 
## -6.9616496 -0.7089539 -4.8936916 -9.4123620 -9.0416096 -7.3600645</code></pre>
<p>That is, <code>R</code> is returning</p>
<p><span class="math display">\[
\hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 + \cdots  + \hat{\beta}_p x_p
\]</span> for each observation.</p>
<p>Importantly, these are <strong>not</strong> predicted probabilities. To obtain the predicted probabilities</p>
<p><span class="math display">\[
\hat{p}({\bf x}) = \hat{P}[Y = 1 \mid {\bf X = x}]
\]</span></p>
<p>we need to use <code>type = &quot;response&quot;</code></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(<span class="kw">predict</span>(model_glm, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>))</code></pre></div>
<pre><code>##         9149         9370         2861         8302         6415 
## 9.466353e-04 3.298300e-01 7.437969e-03 8.170105e-05 1.183661e-04 
##         5189 
## 6.357530e-04</code></pre>
<p>Note that these are probabilities, <strong>not</strong> classifications. To obtain classifications, we will need to compare to the correct cutoff value with an <code>ifelse()</code> statement.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model_glm_pred =<span class="st"> </span><span class="kw">ifelse</span>(<span class="kw">predict</span>(model_glm, <span class="dt">type =</span> <span class="st">&quot;link&quot;</span>) &gt;<span class="st"> </span><span class="dv">0</span>, <span class="st">&quot;Yes&quot;</span>, <span class="st">&quot;No&quot;</span>)
<span class="co"># model_glm_pred = ifelse(predict(model_glm, type = &quot;response&quot;) &gt; 0.5, &quot;Yes&quot;, &quot;No&quot;)</span></code></pre></div>
<p>The line that is run is performing</p>
<p><span class="math display">\[
\hat{C}(\bf x) = 
\begin{cases} 
      1 &amp; \hat{f}({\bf x}) &gt; 0.5 \\
      0 &amp; \hat{f}({\bf x}) \leq 0.5 
\end{cases}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\hat{f}({\bf x}) =\hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 + \cdots  + \hat{\beta}_p x_p.
\]</span></p>
<p>The commented line, which would give the same results, is performing</p>
<p><span class="math display">\[
\hat{C}(\bf x) = 
\begin{cases} 
      1 &amp; \hat{p}({\bf x}) &gt; 0.5 \\
      0 &amp; \hat{p}({\bf x}) \leq 0.5 
\end{cases}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\hat{p}({\bf x}) = \hat{P}[Y = 1 \mid {\bf X = x}].
\]</span> Once we have classifications, we can calculate metrics such as accuracy.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(model_glm_pred ==<span class="st"> </span>default_train$default) <span class="co"># train accuracy</span></code></pre></div>
<pre><code>## [1] 0.9722</code></pre>
<p>As we saw previously, the <code>table()</code> and <code>confusionMatrix()</code> functions can be used to quickly obtain many more metrics.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">train_tab =<span class="st"> </span><span class="kw">table</span>(<span class="dt">predicted =</span> model_glm_pred, <span class="dt">actual =</span> default_train$default)
<span class="kw">library</span>(caret)
train_con_mat =<span class="st"> </span><span class="kw">confusionMatrix</span>(train_tab, <span class="dt">positive =</span> <span class="st">&quot;Yes&quot;</span>)
<span class="kw">c</span>(train_con_mat$overall[<span class="st">&quot;Accuracy&quot;</span>], 
  train_con_mat$byClass[<span class="st">&quot;Sensitivity&quot;</span>], 
  train_con_mat$byClass[<span class="st">&quot;Specificity&quot;</span>])</code></pre></div>
<pre><code>##    Accuracy Sensitivity Specificity 
##   0.9722000   0.2738095   0.9964818</code></pre>
<p>As we did with regression, we could also write a custom function for accuracy.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">get_accuracy =<span class="st"> </span>function(mod, data, <span class="dt">res =</span> <span class="st">&quot;y&quot;</span>, <span class="dt">pos =</span> <span class="dv">1</span>, <span class="dt">neg =</span> <span class="dv">0</span>, <span class="dt">cut =</span> <span class="fl">0.5</span>) {
  probs =<span class="st"> </span><span class="kw">predict</span>(mod, <span class="dt">newdata =</span> data, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)
  preds =<span class="st"> </span><span class="kw">ifelse</span>(probs &gt;<span class="st"> </span>cut, pos, neg)
  <span class="kw">mean</span>(data[, res] ==<span class="st"> </span>preds)
}</code></pre></div>
<p>This function will be useful later when calculating train and test accuracies for several models at the same time.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">get_accuracy</span>(model_glm, <span class="dt">data =</span> default_train, 
             <span class="dt">res =</span> <span class="st">&quot;default&quot;</span>, <span class="dt">pos =</span> <span class="st">&quot;Yes&quot;</span>, <span class="dt">neg =</span> <span class="st">&quot;No&quot;</span>, <span class="dt">cut =</span> <span class="fl">0.5</span>)</code></pre></div>
<pre><code>## [1] 0.9722</code></pre>
<p>To see how much better logistic regression is for this task, we create the same plot we used for linear regression.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(default ~<span class="st"> </span>balance, <span class="dt">data =</span> default_train_lm, 
     <span class="dt">col =</span> <span class="st">&quot;darkorange&quot;</span>, <span class="dt">pch =</span> <span class="st">&quot;|&quot;</span>, <span class="dt">ylim =</span> <span class="kw">c</span>(-<span class="fl">0.2</span>, <span class="dv">1</span>),
     <span class="dt">main =</span> <span class="st">&quot;Using Logistic Regression for Classification&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h =</span> <span class="dv">0</span>, <span class="dt">lty =</span> <span class="dv">3</span>)
<span class="kw">abline</span>(<span class="dt">h =</span> <span class="dv">1</span>, <span class="dt">lty =</span> <span class="dv">3</span>)
<span class="kw">abline</span>(<span class="dt">h =</span> <span class="fl">0.5</span>, <span class="dt">lty =</span> <span class="dv">2</span>)
<span class="kw">curve</span>(<span class="kw">predict</span>(model_glm, <span class="kw">data.frame</span>(<span class="dt">balance =</span> x), <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>), 
      <span class="dt">add =</span> <span class="ot">TRUE</span>, <span class="dt">lwd =</span> <span class="dv">3</span>, <span class="dt">col =</span> <span class="st">&quot;dodgerblue&quot;</span>)
<span class="kw">abline</span>(<span class="dt">v =</span> -<span class="kw">coef</span>(model_glm)[<span class="dv">1</span>] /<span class="st"> </span><span class="kw">coef</span>(model_glm)[<span class="dv">2</span>], <span class="dt">lwd =</span> <span class="dv">2</span>)</code></pre></div>
<p><img src="08-logistic_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>This plot contains a wealth of information.</p>
<ul>
<li>The orange <code>|</code> characters are the data, <span class="math inline">\((x_i, y_i)\)</span>.</li>
<li>The blue “curve” is the predicted probabilities given by the fitted logistic regression. That is, <span class="math display">\[
\hat{p}({\bf x}) = \hat{P}[Y = 1 \mid {\bf X = x}]
\]</span></li>
<li>The solid vertical black line represents the <strong><a href="https://en.wikipedia.org/wiki/Decision_boundary">decision boundary</a></strong>, the <code>balance</code> that obtains a predicted probability of 0.5. In this case <code>balance</code> = 1947.252994.</li>
</ul>
<p>The decision boundary is found by solving for points that satisfy</p>
<p><span class="math display">\[
\hat{p}({\bf x}) = \hat{P}[Y = 1 \mid {\bf X = x}] = 0.5
\]</span></p>
<p>This is equivalent to point that satisfy</p>
<p><span class="math display">\[
\hat{\beta}_0 + \hat{\beta}_1 x_1 = 0.
\]</span> Thus, for logistic regression with a single predictor, the decision boundary is given by the <em>point</em></p>
<p><span class="math display">\[
x_1 = \frac{-\hat{\beta}_0}{\hat{\beta}_1}.
\]</span></p>
<p>The following is not run, but an alternative way to add the logistic curve to the plot.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">grid =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="kw">max</span>(default_train$balance), <span class="dt">by =</span> <span class="fl">0.01</span>)

sigmoid =<span class="st"> </span>function(x) {
  <span class="dv">1</span> /<span class="st"> </span>(<span class="dv">1</span> +<span class="st"> </span><span class="kw">exp</span>(-x))
}

<span class="kw">lines</span>(grid, <span class="kw">sigmoid</span>(<span class="kw">coef</span>(model_glm)[<span class="dv">1</span>] +<span class="st"> </span><span class="kw">coef</span>(model_glm)[<span class="dv">2</span>] *<span class="st"> </span>grid), <span class="dt">lwd =</span> <span class="dv">3</span>)</code></pre></div>
<p>Using the usual formula syntax, it is easy to add complexity to logistic regressions.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model_1 =<span class="st"> </span><span class="kw">glm</span>(default ~<span class="st"> </span><span class="dv">1</span>, <span class="dt">data =</span> default_train, <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>)
model_2 =<span class="st"> </span><span class="kw">glm</span>(default ~<span class="st"> </span>., <span class="dt">data =</span> default_train, <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>)
model_3 =<span class="st"> </span><span class="kw">glm</span>(default ~<span class="st"> </span>. ^<span class="st"> </span><span class="dv">2</span> +<span class="st"> </span><span class="kw">I</span>(balance ^<span class="st"> </span><span class="dv">2</span>),
              <span class="dt">data =</span> default_train, <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>)</code></pre></div>
<p>Note that, using polynomial transformations of predictors will allow a linear model to have non-linear decision boundaries.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model_list =<span class="st"> </span><span class="kw">list</span>(model_1, model_2, model_3)

train_error =<span class="st"> </span><span class="dv">1</span> -<span class="st"> </span><span class="kw">sapply</span>(model_list, get_accuracy, <span class="dt">data =</span> default_train, 
                         <span class="dt">res =</span> <span class="st">&quot;default&quot;</span>, <span class="dt">pos =</span> <span class="st">&quot;Yes&quot;</span>, <span class="dt">neg =</span> <span class="st">&quot;No&quot;</span>, <span class="dt">cut =</span> <span class="fl">0.5</span>)
test_error =<span class="st"> </span><span class="dv">1</span> -<span class="st"> </span><span class="kw">sapply</span>(model_list, get_accuracy, <span class="dt">data =</span> default_test, 
                       <span class="dt">res =</span> <span class="st">&quot;default&quot;</span>, <span class="dt">pos =</span> <span class="st">&quot;Yes&quot;</span>, <span class="dt">neg =</span> <span class="st">&quot;No&quot;</span>, <span class="dt">cut =</span> <span class="fl">0.5</span>)</code></pre></div>
<p>Here we see the misclassification error rates for each model. The train decreases, and the test decreases, until it starts to increases. Everything we learned about the bias-variance tradeoff for regression also applies here.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">diff</span>(train_error)</code></pre></div>
<pre><code>## [1] -0.0058 -0.0002</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">diff</span>(test_error)</code></pre></div>
<pre><code>## [1] -0.0068  0.0004</code></pre>
<p>We call <code>model_2</code> the <strong>additive</strong> logistic model, which we will use quite often.</p>
</div>
<div id="roc-curves" class="section level2">
<h2><span class="header-section-number">8.4</span> ROC Curves</h2>
<p>Let’s return to our simple model with only balance as a predictor.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model_glm =<span class="st"> </span><span class="kw">glm</span>(default ~<span class="st"> </span>balance, <span class="dt">data =</span> default_train, <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>)</code></pre></div>
<p>We write a function which allows use to make predictions based on different probability cutoffs.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">get_pred =<span class="st"> </span>function(mod, data, <span class="dt">res =</span> <span class="st">&quot;y&quot;</span>, <span class="dt">pos =</span> <span class="dv">1</span>, <span class="dt">neg =</span> <span class="dv">0</span>, <span class="dt">cut =</span> <span class="fl">0.5</span>) {
  probs =<span class="st"> </span><span class="kw">predict</span>(mod, <span class="dt">newdata =</span> data, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)
  <span class="kw">ifelse</span>(probs &gt;<span class="st"> </span>cut, pos, neg)
}</code></pre></div>
<p><span class="math display">\[
\hat{C}(\bf x) = 
\begin{cases} 
      1 &amp; \hat{f}({\bf x}) &gt; c \\
      0 &amp; \hat{f}({\bf x}) \leq c 
\end{cases}
\]</span></p>
<p>Let’s use this to obtain predictions using a low, medium, and high cutoff. (0.1, 0.5, and 0.9)</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">test_pred_10 =<span class="st"> </span><span class="kw">get_pred</span>(model_glm, <span class="dt">data =</span> default_test, <span class="dt">res =</span> <span class="st">&quot;default&quot;</span>, <span class="dt">pos =</span> <span class="st">&quot;Yes&quot;</span>, <span class="dt">neg =</span> <span class="st">&quot;No&quot;</span>, <span class="dt">cut =</span> <span class="fl">0.1</span>)
test_pred_50 =<span class="st"> </span><span class="kw">get_pred</span>(model_glm, <span class="dt">data =</span> default_test, <span class="dt">res =</span> <span class="st">&quot;default&quot;</span>, <span class="dt">pos =</span> <span class="st">&quot;Yes&quot;</span>, <span class="dt">neg =</span> <span class="st">&quot;No&quot;</span>, <span class="dt">cut =</span> <span class="fl">0.5</span>)
test_pred_90 =<span class="st"> </span><span class="kw">get_pred</span>(model_glm, <span class="dt">data =</span> default_test, <span class="dt">res =</span> <span class="st">&quot;default&quot;</span>, <span class="dt">pos =</span> <span class="st">&quot;Yes&quot;</span>, <span class="dt">neg =</span> <span class="st">&quot;No&quot;</span>, <span class="dt">cut =</span> <span class="fl">0.9</span>)</code></pre></div>
<p>Now we evaluate accuracy, sensitivity, and specificity for these classifiers.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">test_tab_10 =<span class="st"> </span><span class="kw">table</span>(<span class="dt">predicted =</span> test_pred_10, <span class="dt">actual =</span> default_train$default)
test_tab_50 =<span class="st"> </span><span class="kw">table</span>(<span class="dt">predicted =</span> test_pred_50, <span class="dt">actual =</span> default_train$default)
test_tab_90 =<span class="st"> </span><span class="kw">table</span>(<span class="dt">predicted =</span> test_pred_90, <span class="dt">actual =</span> default_train$default)


test_con_mat_10 =<span class="st"> </span><span class="kw">confusionMatrix</span>(test_tab_10, <span class="dt">positive =</span> <span class="st">&quot;Yes&quot;</span>)
test_con_mat_50 =<span class="st"> </span><span class="kw">confusionMatrix</span>(test_tab_50, <span class="dt">positive =</span> <span class="st">&quot;Yes&quot;</span>)
test_con_mat_90 =<span class="st"> </span><span class="kw">confusionMatrix</span>(test_tab_90, <span class="dt">positive =</span> <span class="st">&quot;Yes&quot;</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">metrics =<span class="st"> </span><span class="kw">rbind</span>(
  
  <span class="kw">c</span>(test_con_mat_10$overall[<span class="st">&quot;Accuracy&quot;</span>], 
    test_con_mat_10$byClass[<span class="st">&quot;Sensitivity&quot;</span>], 
    test_con_mat_10$byClass[<span class="st">&quot;Specificity&quot;</span>]),
  
  <span class="kw">c</span>(test_con_mat_50$overall[<span class="st">&quot;Accuracy&quot;</span>], 
    test_con_mat_50$byClass[<span class="st">&quot;Sensitivity&quot;</span>], 
    test_con_mat_50$byClass[<span class="st">&quot;Specificity&quot;</span>]),
  
  <span class="kw">c</span>(test_con_mat_90$overall[<span class="st">&quot;Accuracy&quot;</span>], 
    test_con_mat_90$byClass[<span class="st">&quot;Sensitivity&quot;</span>], 
    test_con_mat_90$byClass[<span class="st">&quot;Specificity&quot;</span>])
  
)

<span class="kw">rownames</span>(metrics) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;c = 0.10&quot;</span>, <span class="st">&quot;c = 0.50&quot;</span>, <span class="st">&quot;c = 0.90&quot;</span>)
metrics</code></pre></div>
<pre><code>##          Accuracy Sensitivity Specificity
## c = 0.10   0.8914  0.04166667   0.9209437
## c = 0.50   0.9532  0.01190476   0.9859272
## c = 0.90   0.9656  0.00000000   0.9991722</code></pre>
<p>We see then sensitivity decreases as the cutoff is increases. Conversely, specificity increases as the cutoff increases. This is useful if we are more interested in a particular error, instead of giving them equal weight.</p>
<p>Note that usually the best accuracy will be seen near <span class="math inline">\(c = 0.50\)</span>, but our results here difference due to the randomness of the test-train split.</p>
<p>Instead of manually checking cutoffs, we can create an ROC curve (receiver operating characteristic curve) which will sweep through all possible cutoffs, and plot the sensitivity and specificity.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(pROC)
test_prob =<span class="st"> </span><span class="kw">predict</span>(model_glm, <span class="dt">newdata =</span> default_test, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)
test_roc =<span class="st"> </span><span class="kw">roc</span>(default_test$default ~<span class="st"> </span>test_prob, <span class="dt">plot =</span> <span class="ot">TRUE</span>, <span class="dt">print.auc =</span> <span class="ot">TRUE</span>)</code></pre></div>
<p><img src="08-logistic_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">as.numeric</span>(test_roc$auc)</code></pre></div>
<pre><code>## [1] 0.9515076</code></pre>
<p>A good model will have a high AUC, that is as often as possible a hgih sensitivity and specificity.</p>
</div>
<div id="multinomial-logistic-regression" class="section level2">
<h2><span class="header-section-number">8.5</span> Multinomial Logistic Regression</h2>
<p>What if the response contains more than two categories? For that we need multinomial logistic regression.</p>
<p><span class="math display">\[
P[Y = k \mid {\bf X = x}] = \frac{e^{\beta_{0k} + \beta_{1k} x_1 + \cdots +  + \beta_{pk} x_p}}{\sum_{j = 1}^{K} e^{\beta_{0j} + \beta_{1j} x_1 + \cdots +  + \beta_{pj} x_p}}
\]</span></p>
<p>We will omit the details, as ISL has as well. If you are interested, the <a href="https://en.wikipedia.org/wiki/Multinomial_logistic_regression">Wikipedia page</a> provides a rather thorough coverage. Also note that the above is an example of the <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax function</a>.</p>
<p>As an example of a dataset with a three category response, we use the <code>iris</code> dataset, which is so famous, it has its own <a href="https://en.wikipedia.org/wiki/Iris_flower_data_set">Wikipedia entry</a>. It is also a default dataset in <code>R</code>, so no need to load it.</p>
<p>Before proceeding, we test-train split this data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">iris_obs =<span class="st"> </span><span class="kw">nrow</span>(iris)
iris_index =<span class="st"> </span><span class="kw">sample</span>(iris_obs, <span class="dt">size =</span> <span class="kw">trunc</span>(<span class="fl">0.50</span> *<span class="st"> </span>iris_obs))
iris_train =<span class="st"> </span>iris[iris_index, ]
iris_test =<span class="st"> </span>iris[-iris_index, ]</code></pre></div>
<p>To perform multinomial logistic regression, we use the <code>multinom</code> function from the <code>nnet</code> package. Training using <code>multinom()</code> is done using similar syntax to <code>lm()</code> and <code>glm()</code>. We add the <code>trace = FALSE</code> argument to suppress information about updates to the optimization routine as the model is trained.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(nnet)
model_multi =<span class="st"> </span><span class="kw">multinom</span>(Species ~<span class="st"> </span>., <span class="dt">data =</span> iris_train, <span class="dt">trace =</span> <span class="ot">FALSE</span>)
<span class="kw">summary</span>(model_multi)$coefficients</code></pre></div>
<pre><code>##            (Intercept) Sepal.Length Sepal.Width Petal.Length Petal.Width
## versicolor     41.5588    -2.905333   -22.40524     13.04116     1.72182
## virginica     -45.8317   -13.152520   -31.08222     40.80352    27.96529</code></pre>
<p>Notice we are only given coefficients for two of the three class, much like only needing coefficients for one class in logistic regression.</p>
<p>A difference between <code>glm()</code> and <code>multinom()</code> is how the <code>predict()</code> function operates.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(<span class="kw">predict</span>(model_multi, <span class="dt">newdata =</span> iris_train))</code></pre></div>
<pre><code>## [1] versicolor versicolor virginica  versicolor setosa     versicolor
## Levels: setosa versicolor virginica</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(<span class="kw">predict</span>(model_multi, <span class="dt">newdata =</span> iris, <span class="dt">type =</span> <span class="st">&quot;prob&quot;</span>))</code></pre></div>
<pre><code>##   setosa   versicolor    virginica
## 1      1 4.323382e-15 9.039653e-70
## 2      1 5.667775e-10 7.046653e-62
## 3      1 3.113884e-12 3.300312e-65
## 4      1 5.312106e-10 9.633711e-60
## 5      1 6.151112e-16 1.504787e-70
## 6      1 1.635952e-17 3.874846e-69</code></pre>
<p>Notice that by default, classifications are returned. When obtaining probabilities, we are given the predicted probability for <strong>each</strong> class.</p>
<p>Interestingly, you’ve just fit a neural network, and you didn’t even know it! (Hence the <code>nnet</code> package.) Later we will discuss the connections between logistic regression, multinomial logistic regression, and simple neural networks.</p>

<div id="refs" class="references">

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="classification.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/daviddalpiaz/r4sl/edit/master/08-logistic.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
