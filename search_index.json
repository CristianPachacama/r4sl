[
["index.html", "R for Statistical Learning Introduction 0.1 About This Book 0.2 Caveat Emptor 0.3 Conventions 0.4 Acknowledgements 0.5 License", " R for Statistical Learning David Dalpiaz 2017-08-28 Introduction Welcome to R for Statistical Learning! 0.1 About This Book This book will serve as a supplement to An Introduction to Statistical Learning for STAT 430 - Basics of Statistical Learning at the University of Illinois at Urbana-Champaign. The end of each chapter will contain: Annotated links to additional information and resources. A link to the RMarkdown file that generates the chapter. 0.2 Caveat Emptor This “book” is under active development. Much of the text was hastily written during the Spring 2017 run of the course. While together with ISL the coverage is essentially complete, significant updates will occur during Fall 2017. When possible, it would be best to always access the text online to be sure you are using the most up-to-date version. Also, the html version provides additional features such as changing text size, font, and colors. If you are in need of a local copy, a pdf version is continuously maintained. Since this book is under active development you may encounter errors ranging from typos, to broken code, to poorly explained topics. If you do, please let us know! Simply send an email and we will make the changes as soon as possible. (dalpiaz2 AT illinois DOT edu) Or, if you know RMarkdown and are familiar with GitHub, make a pull request and fix an issue yourself! This process is partially automated by the edit button in the top-left corner of the html version. If your suggestion or fix becomes part of the book, you will be added to the list at the end of this chapter. We’ll also link to your GitHub account, or personal website upon request. 0.3 Conventions This text uses MathJax to render mathematical notation for the web. Occasionally, but rarely, a JavaScript error will prevent MathJax from rendering correctly. In this case, you will see the “code” instead of the expected mathematical equations. From experience, this is almost always fixed by simply refreshing the page. You’ll also notice that if you right-click any equation you can obtain the MathML Code (for copying into Microsoft Word) or the TeX command used to generate the equation. \\[ a^2 + b^2 = c^2 \\] R code will be typeset using a monospace font which is syntax highlighted. a = 3 b = 4 sqrt(a ^ 2 + b ^ 2) R output lines, which would appear in the console will begin with ##. They will generally not be syntax highlighted. ## [1] 5 Often the symbol \\(\\triangleq\\) will be used to mean “is defined to be.” We use the value \\(p\\) to mean the number of predictors. 0.4 Acknowledgements Your name could be here! Suggest an edit! Correct a typo! Pull requests encouraged! If you submit a correction and would like to be listed below, please provide your name as you would like it to appear, as well as a link to a GitHub, LinkedIn, or personal website. James Balamuta, Summer 2016 - ??? Korawat Tanwisuth, Spring 2017 Yiming Gao, Spring 2017 Binxiang Ni, Summer 2017 0.5 License This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],
["probability-review.html", "Chapter 1 Probability Review 1.1 Probability Models 1.2 Probability Axioms 1.3 Probability Rules 1.4 Random Variables 1.5 Expectations 1.6 Likelihood 1.7 Videos 1.8 References", " Chapter 1 Probability Review We give a very brief review of some necessary probability concepts. As the treatment is less than complete, a list of references is given at the end of the chapter. For example, we ignore the usual recap of basic set theory and omit proofs and examples. 1.1 Probability Models When discussing probability models, we speak of random experiments that produce one of a number of possible outcomes. A probability model that describes the uncertainty of an experiment consists of two elements: The sample space, often denoted as \\(\\Omega\\), which is a set that contains all possible outcomes. A probability function that assigns to an event \\(A\\) a nonnegative number, \\(P[A]\\), that represents how likely it is that event \\(A\\) occurs as a result of the experiment. We call \\(P[A]\\) the probability of event \\(A\\). An event \\(A\\) could be any subset of the sample space, not necessarily a single possible outcome. The probability law must follow a number of rules, which are the result of a set of axioms that we introduce now. 1.2 Probability Axioms Given a sample space \\(\\Omega\\) for a particular experiment, the probability function associated with the experiment must satisfy the following axioms. Nonnegativity: \\(P[A] \\geq 0\\) for any event \\(A \\subset \\Omega\\). Normalization: \\(P[\\Omega] = 1\\). That is, the probability of the entire space is 1. Additivity: For mutually exclusive events \\(E_1, E_2, \\ldots\\) \\[ P\\left[\\bigcup_{i = 1}^{\\infty} E_i\\right] = \\sum_{i = 1}^{\\infty} P[E_i] \\] Using these axioms, many additional probability rules can easily be derived. 1.3 Probability Rules Given an event \\(A\\), and its complement, \\(A^c\\), that is, the outcomes in \\(\\Omega\\) which are not in \\(A\\), we have the complement rule: \\[ P[A^c] = 1 - P[A] \\] In general, for two events \\(A\\) and \\(B\\), we have the addition rule: \\[ P[A \\cup B] = P[A] + P[B] - P[A \\cap B] \\] If \\(A\\) and \\(B\\) are also disjoint, then we have: \\[ P[A \\cup B] = P[A] + P[B] \\] If we have \\(n\\) mutually exclusive events, \\(E_1, E_2, \\ldots E_n\\), then we have: \\[ P\\left[\\textstyle\\bigcup_{i = 1}^{n} E_i\\right] = \\sum_{i = 1}^{n} P[E_i] \\] Often, we would like to understand the probability of an event \\(A\\), given some information about the outcome of event \\(B\\). In that case, we have the conditional probability rule provided \\(P[B] &gt; 0\\). \\[ P[A \\mid B] = \\frac{P[A \\cap B]}{P[B]} \\] Rearranging the conditional probability rule, we obtain the multiplication rule: \\[ P[A \\cap B] = P[B] \\cdot P[A \\mid B] \\cdot \\] For a number of events \\(E_1, E_2, \\ldots E_n\\), the multiplication rule can be expanded into the chain rule: \\[ P\\left[\\textstyle\\bigcap_{i = 1}^{n} E_i\\right] = P[E_1] \\cdot P[E_2 \\mid E_1] \\cdot P[E_3 \\mid E_1 \\cap E_2] \\cdots P\\left[E_n \\mid \\textstyle\\bigcap_{i = 1}^{n - 1} E_i\\right] \\] Define a partition of a sample space \\(\\Omega\\) to be a set of disjoint events \\(A_1, A_2, \\ldots, A_n\\) whose union is the sample space \\(\\Omega\\). That is \\[ A_i \\cap A_j = \\emptyset \\] for all \\(i \\neq j\\), and \\[ \\bigcup_{i = 1}^{n} A_i = \\Omega. \\] Now, let \\(A_1, A_2, \\ldots, A_n\\) form a partition of the sample space where \\(P[A_i] &gt; 0\\) for all \\(i\\). Then for any event \\(B\\) with \\(P[B] &gt; 0\\) we have Bayes’ Rule: \\[ P[A_i | B] = \\frac{P[A_i]P[B | A_i]}{P[B]} = \\frac{P[A_i]P[B | A_i]}{\\sum_{i = 1}^{n}P[A_i]P[B | A_i]} \\] The denominator of the latter equality is often called the law of total probability: \\[ P[B] = \\sum_{i = 1}^{n}P[A_i]P[B | A_i] \\] Two events \\(A\\) and \\(B\\) are said to be independent if they satisfy \\[ P[A \\cap B] = P[A] \\cdot P[B] \\] This becomes the new multiplication rule for independent events. A collection of events \\(E_1, E_2, \\ldots E_n\\) is said to be independent if \\[ P\\left[\\bigcap_{i \\in S} E_i \\right] = \\prod_{i \\in S}P[A_i] \\] for every subset \\(S\\) of \\(\\{1, 2, \\ldots n\\}\\). If this is the case, then the chain rule is greatly simplified to: \\[ P\\left[\\textstyle\\bigcap_{i = 1}^{n} E_i\\right] = \\prod_{i=1}^{n}P[A_i] \\] 1.4 Random Variables A random variable is simply a function which maps outcomes in the sample space to real numbers. 1.4.1 Distributions We often talk about the distribution of a random variable, which can be thought of as: \\[ \\text{distribution} = \\text{list of possible} \\textbf{ values} + \\text{associated} \\textbf{ probabilities} \\] This is not a strict mathematical definition, but is useful for conveying the idea. If the possible values of a random variables are discrete, it is called a discrete random variable. If the possible values of a random variables are continuous, it is called a continuous random variable. 1.4.2 Discrete Random Variables The distribution of a discrete random variable \\(X\\) is most often specified by a list of possible values and a probability mass function, \\(p(x)\\). The mass function directly gives probabilities, that is, \\[ p(x) = p_X(x) = P[X = x]. \\] Note we almost always drop the subscript from the more correct \\(p_X(x)\\) and simply refer to \\(p(x)\\). The relevant random variable is discerned from context The most common example of a discrete random variable is a binomial random variable. The mass function of a binomial random variable \\(X\\), is given by \\[ p(x | n, p) = {n \\choose x} p^x(1 - p)^{n - x}, \\ \\ \\ x = 0, 1, \\ldots, n, \\ n \\in \\mathbb{N}, \\ 0 &lt; p &lt; 1. \\] This line conveys a large ammount of information. The function \\(p(x | n, p)\\) is the mass function. It is a function of \\(x\\), the possible values of the random variable \\(X\\). It is conditional on the parameters \\(n\\) and \\(p\\). Different values of these parameters specify different binomial distributions. \\(x = 0, 1, \\ldots, n\\) indicates the sample space, that is, the possible values of the random variable. \\(n \\in \\mathbb{N}\\) and \\(0 &lt; p &lt; 1\\) specify the parameter spaces. These are the possible values of the parameters that give a valid binomial distribution. Often all of this information is simply encoded by writing \\[ X \\sim \\text{bin}(n, p). \\] 1.4.3 Continuous Random Variables The distribution of a continuous random variable \\(X\\) is most often specified by a set of possible values and a probability density function, \\(f(x)\\). (A cumulative density or moment generating function would also suffice.) The probability of the event \\(a &lt; X &lt; b\\) is calculated as \\[ P[a &lt; X &lt; b] = \\int_{a}^{b} f(x)dx. \\] Note that densities are not probabilities. The most common example of a continuous random variable is a normal random variable. The density of a normal random variable \\(X\\), is given by \\[ f(x | \\mu, \\sigma^2) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\cdot \\exp\\left[\\frac{-1}{2} \\left(\\frac{x - \\mu}{\\sigma}\\right)^2 \\right], \\ \\ \\ -\\infty &lt; x &lt; \\infty, \\ -\\infty &lt; \\mu &lt; \\infty, \\ \\sigma &gt; 0. \\] The function \\(f(x | \\mu, \\sigma^2)\\) is the density function. It is a function of \\(x\\), the possible values of the random variable \\(X\\). It is conditional on the paramters \\(\\mu\\) and \\(\\sigma^2\\). Different values of these parameters specify different normal distributions. \\(-\\infty &lt; x &lt; \\infty\\) indicates the sample space. In this case, the random variable may take any value on the real line. \\(-\\infty &lt; \\mu &lt; \\infty\\) and \\(\\sigma &gt; 0\\) specify the parameter space. These are the possible values of the parameters that give a valid normal distribution. Often all of this information is simply encoded by writing \\[ X \\sim N(\\mu, \\sigma^2) \\] 1.4.4 Several Random Variables Consider two random variables \\(X\\) and \\(Y\\). We say they are independent if \\[ f(x, y) = f(x) \\cdot f(y) \\] for all \\(x\\) and \\(y\\). Here \\(f(x, y)\\) is the joint density (mass) function of \\(X\\) and \\(Y\\). We call \\(f(x)\\) the marginal density (mass) function of \\(X\\). Then \\(f(y)\\) the marginal density (mass) function of \\(Y\\). The joint density (mass) function \\(f(x, y)\\) together with the possible \\((x, y)\\) values specify the joint distribution of \\(X\\) and \\(Y\\). Similar notions exist for more than two variables. 1.5 Expectations For discrete random variables, we define the expectation of the function of a random variable \\(X\\) as follows. \\[ \\mathbb{E}[g(X)] \\triangleq \\sum_{x} g(x)p(x) \\] For continuous random variables we have a similar definition. \\[ \\mathbb{E}[g(X)] \\triangleq \\int g(x)f(x) dx \\] For specific functions \\(g\\), expectations are given names. The mean of a random variable \\(X\\) is given by \\[ \\mu_{X} = \\text{mean}[X] \\triangleq \\mathbb{E}[X]. \\] So for a discrete random variable, we would have \\[ \\text{mean}[X] = \\sum_{x} x \\cdot p(x) \\] For a continuous random variable we would simply replace the sum by an integral. The variance of a random variable \\(X\\) is given by \\[ \\sigma^2_{X} = \\text{var}[X] \\triangleq \\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2. \\] The **standard deviation of a random variable \\(X\\) is given by \\[ \\sigma_{X} = \\text{sd}[X] \\triangleq \\sqrt{\\sigma^2_{X}} = \\sqrt{\\text{var}[X]}. \\] The covariance or random variables \\(X\\) and \\(Y\\) is given by \\[ \\text{cov}[X, Y] \\triangleq \\mathbb{E}[(X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])] = \\mathbb{E}[XY] - \\mathbb{E}[X] \\cdot \\mathbb{E}[Y]. \\] 1.6 Likelihood Consider \\(n\\) iid random variables \\(X_1, X_2, \\ldots X_n\\). We can then write their likelihood as \\[ \\mathcal{L}(\\theta \\mid x_1, x_2, \\ldots x_n) = \\prod_{i = i}^n f(x_i; \\theta) \\] where \\(f(x_i; \\theta)\\) is the density (or mass) function of random variable \\(X_i\\) evaluated at \\(x_i\\) with parameter \\(\\theta\\). Whereas a probability is a function of a possible observed value given a particular parameter value, a likelihood is the opposite. It is a function of a possible parameter value given observed data. Maximumizing likelihood is a common techinque for fitting a model to data. 1.7 Videos The YouTube channel mathematicalmonk has a great Probability Primer playlist containing lectures on many fundamental probability concepts. Some of the more important concepts are covered in the following videos: Conditional Probability Independence More Independence Bayes Rule 1.8 References Any of the following are either dedicated to, or contain a good coverage of the details of the topics above. Probability Texts Introduction to Probability by Dimitri P. Bertsekas and John N. Tsitsiklis A First Course in Probability by Sheldon Ross Machine Learning Texts with Probability Focus Probability for Statistics and Machine Learning by Anirban DasGupta Machine Learning: A Probabilistic Perspective by Kevin P. Murphy Statistics Texts with Introduction to Probability Probability and Statistical Inference by Robert V. Hogg, Elliot Tanis, and Dale Zimmerman Introduction to Mathematical Statistics by Robert V. Hogg, Joseph McKean, and Allen T. Craig "],
["r-rstudio-rmarkdown.html", "Chapter 2 R, RStudio, RMarkdown 2.1 Videos 2.2 Template", " Chapter 2 R, RStudio, RMarkdown Materials for leaning R, RStudio, and RMarkdown can be found in another text from the same author, Applied Statistics with R. The chapters up to and including Chapter 6 - R Resources contain an introduction to using R, RStudio, and RMarkdown. This chapter in particular contains a number of videos to get you up to speed on R, RStudio, and RMarkdown, which are also linked below. Also linked is an RMarkdown template which is referenced in the videos. 2.1 Videos R and RStudio Playlist Data in R Playlist RMarkdown Playlist 2.2 Template RMarkdown Template "],
["regression-basics-in-r.html", "Chapter 3 Regression Basics in R 3.1 Visualization for Regression 3.2 The lm() Function 3.3 Hypothesis Testing 3.4 Prediction 3.5 Unusual Observations 3.6 Adding Complexity", " Chapter 3 Regression Basics in R This chapter will recap the basics of performing regression analyses in R. For more detailed coverage, see Applied Statistics with R. We will use the Advertising data associated with Introduction to Statistical Learning. library(readr) Advertising = read_csv(&quot;data/Advertising.csv&quot;) After loading data into R, our first step should always be to inspect the data. We will start by simply printing some observations in order to understand the basic structure of the data. Advertising ## # A tibble: 200 x 4 ## TV Radio Newspaper Sales ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 230.1 37.8 69.2 22.1 ## 2 44.5 39.3 45.1 10.4 ## 3 17.2 45.9 69.3 9.3 ## 4 151.5 41.3 58.5 18.5 ## 5 180.8 10.8 58.4 12.9 ## 6 8.7 48.9 75.0 7.2 ## 7 57.5 32.8 23.5 11.8 ## 8 120.2 19.6 11.6 13.2 ## 9 8.6 2.1 1.0 4.8 ## 10 199.8 2.6 21.2 10.6 ## # ... with 190 more rows Because the data was read using read_csv(), Advertising is a tibble. We see that there are a total of 200 observations and 4 variables, each of which is numeric. (Specifically double-precision vectors, but more importantly they are numbers.) For the purpose of this analysis, Sales will be the response variable. That is, we seek to understand the relationship between Sales, and the predictor variables: TV, Radio, and Newspaper. 3.1 Visualization for Regression After investigating the structure of the data, the next step should be to visualize the data. Since we have only numeric variables, we should consider scatter plots. We could do so for any individual predictor. plot(Sales ~ TV, data = Advertising, col = &quot;dodgerblue&quot;, pch = 20, cex = 1.5, main = &quot;Sales vs Television Advertising&quot;) The pairs() function is a useful way to quickly visualize a number of scatter plots. pairs(Advertising) Often, we will be most interested in only the relationship between each predictor and the response. For this, we can use the featurePlot() function from the caret package. (We will use the caret package more and more frequently as we introduce new topics.) library(caret) featurePlot(x = Advertising[ , c(&quot;TV&quot;, &quot;Radio&quot;, &quot;Newspaper&quot;)], y = Advertising$Sales) We see that there is a clear increase in Sales as Radio or TV are increased. The relationship between Sales and Newspaper is less clear. How all of the predictors work together is also unclear, as there is some obvious correlation between Radio and TV. To investigate further, we will need to model the data. 3.2 The lm() Function The following code fits an additive linear model with Sales as the response and each remaining variable as a predictor. Note, by not using attach() and instead specifying the data = argument, we are able to specify this model without using each of the variable names directly. mod_1 = lm(Sales ~ ., data = Advertising) # mod_1 = lm(Sales ~ TV + Radio + Newspaper, data = Advertising) Note that the commented line is equivalent to the line that is run, but we will often use the response ~ . syntax when possible. 3.3 Hypothesis Testing The summary() function will return a large amount of useful information about a model fit using lm(). Much of it will be helpful for hypothesis testing including individual tests about each predictor, as well as the significance of the regression test. summary(mod_1) ## ## Call: ## lm(formula = Sales ~ ., data = Advertising) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.8277 -0.8908 0.2418 1.1893 2.8292 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.938889 0.311908 9.422 &lt;2e-16 *** ## TV 0.045765 0.001395 32.809 &lt;2e-16 *** ## Radio 0.188530 0.008611 21.893 &lt;2e-16 *** ## Newspaper -0.001037 0.005871 -0.177 0.86 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.686 on 196 degrees of freedom ## Multiple R-squared: 0.8972, Adjusted R-squared: 0.8956 ## F-statistic: 570.3 on 3 and 196 DF, p-value: &lt; 2.2e-16 mod_0 = lm(Sales ~ TV + Radio, data = Advertising) The anova() function is useful for comparing two models. Here we compare the full additive model, mod_1, to a reduced model mod_0. Essentially we are testing for the significance of the Newspaper variable in the additive model. anova(mod_0, mod_1) ## Analysis of Variance Table ## ## Model 1: Sales ~ TV + Radio ## Model 2: Sales ~ TV + Radio + Newspaper ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 197 556.91 ## 2 196 556.83 1 0.088717 0.0312 0.8599 Note that hypothesis testing is not our focus, so we omit many details. 3.4 Prediction The predict() function is an extremely versatile function, for, prediction. When used on the result of a model fit using lm() it will, by default, return predictions for each of the data points used to fit the model. (Here, we limit the printed result to the first 10.) head(predict(mod_1), n = 10) ## 1 2 3 4 5 6 7 ## 20.523974 12.337855 12.307671 17.597830 13.188672 12.478348 11.729760 ## 8 9 10 ## 12.122953 3.727341 12.550849 Note that the effect of the predict() function is dependent on the input to the function. Here, we are supplying as the first argument a model object of class lm. Because of this, predict() then runs the predict.lm() function. Thus, we should use ?predict.lm() for details. We could also specify new data, which should be a data frame or tibble with the same column names as the predictors. new_obs = data.frame(TV = 150, Radio = 40, Newspaper = 1) We can then use the predict() function for point estimates, confidence intervals, and prediction intervals. Using only the first two arguments, R will simply return a point estimate, that is, the “predicted value,” \\(\\hat{y}\\). predict(mod_1, newdata = new_obs) ## 1 ## 17.34375 If we specify an additional argument interval with a value of &quot;confidence&quot;, R will return a 95% confidence interval for the mean response at the specified point. Note that here R also gives the point estimate as fit. predict(mod_1, newdata = new_obs, interval = &quot;confidence&quot;) ## fit lwr upr ## 1 17.34375 16.77654 17.91096 Lastly, we can alter the level using the level argument. Here we report a prediction interval instead of a confidence interval. predict(mod_1, newdata = new_obs, interval = &quot;prediction&quot;, level = 0.99) ## fit lwr upr ## 1 17.34375 12.89612 21.79138 3.5 Unusual Observations R provides several functions for obtaining metrics related to unusual observations. resid() provides the residual for each observation hatvalues() gives the leverage of each observation rstudent() give the studentized residual for each observation cooks.distance() calculates the influence of each observation head(resid(mod_1), n = 10) ## 1 2 3 4 5 6 ## 1.57602559 -1.93785482 -3.00767078 0.90217049 -0.28867186 -5.27834763 ## 7 8 9 10 ## 0.07024005 1.07704683 1.07265914 -1.95084872 head(hatvalues(mod_1), n = 10) ## 1 2 3 4 5 6 ## 0.025202848 0.019418228 0.039226158 0.016609666 0.023508833 0.047481074 ## 7 8 9 10 ## 0.014435091 0.009184456 0.030714427 0.017147645 head(rstudent(mod_1), n = 10) ## 1 2 3 4 5 6 ## 0.94680369 -1.16207937 -1.83138947 0.53877383 -0.17288663 -3.28803309 ## 7 8 9 10 ## 0.04186991 0.64099269 0.64544184 -1.16856434 head(cooks.distance(mod_1), n = 10) ## 1 2 3 4 5 ## 5.797287e-03 6.673622e-03 3.382760e-02 1.230165e-03 1.807925e-04 ## 6 7 8 9 10 ## 1.283058e-01 6.452021e-06 9.550237e-04 3.310088e-03 5.945006e-03 3.6 Adding Complexity We have a number of ways to add complexity to a linear model, even allowing a linear model to be used to model non-linear relationships. 3.6.1 Interactions Interactions can be introduced to the lm() procedure in a number of ways. We can use the : operator to introduce a single interaction of interest. mod_2 = lm(Sales ~ . + TV:Newspaper, data = Advertising) coef(mod_2) ## (Intercept) TV Radio Newspaper TV:Newspaper ## 3.8730824491 0.0392939602 0.1901312252 -0.0320449675 0.0002016962 The response ~ . ^ k syntax can be used to model all k-way interactions. (As well as the appropriate lower order terms.) Here we fit a model with all two-way interactions, and the lower order main effects. mod_3 = lm(Sales ~ . ^ 2, data = Advertising) coef(mod_3) ## (Intercept) TV Radio Newspaper ## 6.460158e+00 2.032710e-02 2.292919e-02 1.703394e-02 ## TV:Radio TV:Newspaper Radio:Newspaper ## 1.139280e-03 -7.971435e-05 -1.095976e-04 The * operator can be used to specify all interactions of a certain order, as well as all lower order terms according to the usual hierarchy. Here we see a three-way interaction and all lower order terms. mod_4 = lm(Sales ~ TV * Radio * Newspaper, data = Advertising) coef(mod_4) ## (Intercept) TV Radio ## 6.555887e+00 1.971030e-02 1.962160e-02 ## Newspaper TV:Radio TV:Newspaper ## 1.310565e-02 1.161523e-03 -5.545501e-05 ## Radio:Newspaper TV:Radio:Newspaper ## 9.062944e-06 -7.609955e-07 Note that, we have only been dealing with numeric predictors. Categorical predictors are often recorded as factor variables in R. library(tibble) cat_pred = tibble( x1 = factor(c(rep(&quot;A&quot;, 10), rep(&quot;B&quot;, 10), rep(&quot;C&quot;, 10))), x2 = runif(n = 30), y = rnorm(n = 30) ) cat_pred ## # A tibble: 30 x 3 ## x1 x2 y ## &lt;fctr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 0.6715925 1.5453660 ## 2 A 0.3028760 0.7908587 ## 3 A 0.4142105 0.7255688 ## 4 A 0.3735850 -0.8716121 ## 5 A 0.7020701 -0.2263088 ## 6 A 0.4253337 1.6300208 ## 7 A 0.4772296 -0.8339882 ## 8 A 0.6102040 -0.5760106 ## 9 A 0.3422006 0.1657533 ## 10 A 0.1719256 0.2161473 ## # ... with 20 more rows Notice that in this simple simulated tibble, we have coerced x1 to be a factor variable, although this is not strictly necessary since the variable took values A, B, and C. When using lm(), even if not a factor, R would have treated x1 as such. Coercion to factor is more important if a cateogical variable is coded for example as 1, 2 and 3. Otherwise it is treated as numeric, which creates a difference in the regression model. The following two models illustrate the effect of factor variables on linear models. cat_pred_mod_add = lm(y ~ x1 + x2, data = cat_pred) coef(cat_pred_mod_add) ## (Intercept) x1B x1C x2 ## 0.29106820 0.31768805 -0.31737687 -0.07679119 cat_pred_mod_int = lm(y ~ x1 * x2, data = cat_pred) coef(cat_pred_mod_int) ## (Intercept) x1B x1C x2 x1B:x2 x1C:x2 ## 0.2936442 0.7429869 -0.9239367 -0.0825267 -0.8740398 1.3742915 3.6.2 Polynomials Polynomial terms can be specified using the inhibit function I() or through the poly() function. Note that these two methods produce different coefficients, but the same residuals! This is due to the poly() function using orthogonal polynomials by default. mod_5 = lm(Sales ~ TV + I(TV ^ 2), data = Advertising) coef(mod_5) ## (Intercept) TV I(TV^2) ## 6.114120e+00 6.726593e-02 -6.846934e-05 mod_6 = lm(Sales ~ poly(TV, degree = 2), data = Advertising) coef(mod_6) ## (Intercept) poly(TV, degree = 2)1 poly(TV, degree = 2)2 ## 14.022500 57.572721 -6.228802 all.equal(resid(mod_5), resid(mod_6)) ## [1] TRUE Polynomials and interactions can be mixed to create even more complex models. mod_7 = lm(Sales ~ . ^ 2 + poly(TV, degree = 3), data = Advertising) # mod_7 = lm(Sales ~ . ^ 2 + I(TV ^ 2) + I(TV ^ 3), data = Advertising) coef(mod_7) ## (Intercept) TV Radio ## 6.206394e+00 2.092726e-02 3.766579e-02 ## Newspaper poly(TV, degree = 3)1 poly(TV, degree = 3)2 ## 1.405289e-02 NA -9.925605e+00 ## poly(TV, degree = 3)3 TV:Radio TV:Newspaper ## 5.309590e+00 1.082074e-03 -5.690107e-05 ## Radio:Newspaper ## -9.924992e-05 Notice here that R ignores the first order term from poly(TV, degree = 3) as it is already in the model. We could consider using the commented line instead. 3.6.3 Transformations Note that we could also create more complex models, which allow for non-linearity, using transformations. Be aware, when doing so to the response variable, that this will affect the units of said variable. You may need to un-transform to compare to non-transformed models. mod_8 = lm(log(Sales) ~ ., data = Advertising) sqrt(mean(resid(mod_8) ^ 2)) # incorrect RMSE for Model 8 ## [1] 0.1849483 sqrt(mean(resid(mod_7) ^ 2)) # RMSE for Model 7 ## [1] 0.4813215 sqrt(mean(exp(resid(mod_8)) ^ 2)) # correct RMSE for Model 8 ## [1] 1.023205 "],
["regression-for-statistical-learning.html", "Chapter 4 Regression for Statistical Learning 4.1 Assesing Model Accuracy 4.2 Model Complexity 4.3 Test-Train Split 4.4 Adding Flexibility to Linear Models 4.5 Choosing a Model", " Chapter 4 Regression for Statistical Learning When using linear models in the past, we often emphasized distributional results, which were useful for creating and performing hypothesis tests. Frequently, when developing a linear regression model, part of our goal was to explain a relationship. Now, we will ignore much of what we have learned and instead simply use regression as a tool to predict. Instead of a model which explains relationships, we seek a model which minimizes errors. First, note that a linear model is one of many methods used in regression. Regression is a form of supervised learning. Supervised learning deals with problems where there are both an input and an output. Regression problems are the subset of supervised learning problems with a numeric output. Often one of the biggest differences between statistical learning, machine learning, artificial intelligence are the names used to describe variables and methods. The input can be called: input vector, feature vector, or predictors. The elements of these would be an input, feature, or predictor. The individual features can be either numeric or categorical. The output may be called: output, response, outcome, or target. The response must be numeric. As an aside, some textbooks and statisticians use the terms independent and dependent variables to describe the response and the predictors. However, this practice can be confusing as those terms have specific meanings in probability theory. Our goal is to find a rule, algorithm, or function which takes as input a feature vector, and outputs a response which is as close to the true value as possible. We often write the true, unknown relationship between the input and output \\(f(\\bf{x})\\). The relationship we learn, based on data, is written \\(\\hat{f}(\\bf{x})\\). From a statistical learning point-of-view, we write, \\[ Y = f(\\bf{x}) + \\epsilon \\] to indicate that the true response is a function of both the unknown relationship, as well as some unlearnable noise. To discuss linear models in the context of prediction, we return to the Advertising data from the previous chapter. Advertising ## # A tibble: 200 x 4 ## TV Radio Newspaper Sales ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 230.1 37.8 69.2 22.1 ## 2 44.5 39.3 45.1 10.4 ## 3 17.2 45.9 69.3 9.3 ## 4 151.5 41.3 58.5 18.5 ## 5 180.8 10.8 58.4 12.9 ## 6 8.7 48.9 75.0 7.2 ## 7 57.5 32.8 23.5 11.8 ## 8 120.2 19.6 11.6 13.2 ## 9 8.6 2.1 1.0 4.8 ## 10 199.8 2.6 21.2 10.6 ## # ... with 190 more rows library(caret) featurePlot(x = Advertising[ , c(&quot;TV&quot;, &quot;Radio&quot;, &quot;Newspaper&quot;)], y = Advertising$Sales) 4.1 Assesing Model Accuracy There are many metrics to assess the accuracy of a regression model. Most of these measure in some way the average error that the model makes. The metric that we will be most interested in is the root-mean-square error. \\[ \\text{RMSE}(\\hat{f}, \\text{Data}) = \\sqrt{\\frac{1}{n}\\displaystyle\\sum_{i = 1}^{n}\\left(y_i - \\hat{f}(\\bf{x}_i)\\right)^2} \\] While for the sake of comparing models, the choice between RMSE and MSE is arbitrary, we have a preference for RMSE, as it has the same units as the response variable. Also, notice that in the prediction context MSE refers to an average, whereas in an ANOVA context, the denominator for MSE may not be \\(n\\). For a linear model , the estimate of \\(f\\), \\(\\hat{f}\\), is given by the fitted regression line. \\[ \\hat{y}_i = \\hat{f}(\\bf{x}_i) \\] We can write an R function that will be useful for performing this calculation. rmse = function(actual, predicted) { sqrt(mean((actual - predicted) ^ 2)) } 4.2 Model Complexity Aside from how well a model predicts, we will also be very interested in the complexity (flexibility) of a model. For now, we will only considered nested linear models for simplicity. Then in that case, the more predictors that a model has, the more complex the model. For the sake of assigning a numerical value to the complexity of a linear model, we will use the number of predictors, \\(p\\). We write a simple R function to extract this information from a model. get_complexity = function(model) { length(coef(model)) - 1 } 4.3 Test-Train Split There is an issue with fitting a model to all available data then using RMSE to determine how well the model predicts. It is essentially cheating! As a linear model becomes more complex, the RSS, thus RMSE, can never go up. It will only go down, or in very specific cases, stay the same. This would suggest that to predict well, we should use the largest possible model! However, in reality we have hard fit to a specific dataset, but as soon as we see new data, a large model may in fact predict poorly. This is called overfitting. Frequently we will take a dataset of interest and split it in two. One part of the datasets will be used to fit (train) a model, which we will call the training data. The remainder of the original data will be used to assess how well the model is predicting, which we will call the test data. Test data should never be used to train a model. Note that sometimes the terms evaluation set and test set are used interchangeably. We will give somewhat specific definitions to these later. For now we will simply use a single test set for a training set. Here we use the sample() function to obtain a random sample of the rows of the original data. We then use those row numbers (and remaining row numbers) to split the data accordingly. Notice we used the set.seed() function to allow use to reproduce the same random split each time we perform this analysis. set.seed(9) num_obs = nrow(Advertising) train_index = sample(num_obs, size = trunc(0.50 * num_obs)) train_data = Advertising[train_index, ] test_data = Advertising[-train_index, ] We will look at two measures that assess how well a model is predicting, the train RMSE and the test RMSE. \\[ \\text{RMSE}_{\\text{Train}} = \\text{RMSE}(\\hat{f}, \\text{Train Data}) = \\sqrt{\\frac{1}{n_{\\text{Tr}}}\\displaystyle\\sum_{i \\in \\text{Train}}^{}\\left(y_i - \\hat{f}(\\bf{x}_i)\\right)^2} \\] Here \\(n_{Tr}\\) is the number of observations in the train set. Train RMSE will still always go down (or stay the same) as the complexity of a linear model increases. That means train RMSE will not be useful for comparing models, but checking that it decreases is a useful sanity check. \\[ \\text{RMSE}_{\\text{Test}} = \\text{RMSE}(\\hat{f}, \\text{Train Data}) = \\sqrt{\\frac{1}{n_{\\text{Te}}}\\displaystyle\\sum_{i \\in \\text{Test}}^{}\\left(y_i - \\hat{f}(\\bf{x}_i)\\right)^2} \\] Here \\(n_{Te}\\) is the number of observations in the test set. Test RMSE uses the model fit to the training data, but evaluated on the unused test data. This is a measure of how well the fitted model will predict in general, not simply how well it fits data used to train the model, as is the case with train RMSE. What happens to test RMSE as the size of the model increases? That is what we will investigate. We will start with the simplest possible linear model, that is, a model with no predictors. fit_0 = lm(Sales ~ 1, data = train_data) get_complexity(fit_0) ## [1] 0 # train RMSE sqrt(mean((train_data$Sales - predict(fit_0, train_data)) ^ 2)) ## [1] 4.788513 # test RMSE sqrt(mean((test_data$Sales - predict(fit_0, test_data)) ^ 2)) ## [1] 5.643574 The previous two operations obtain the train and test RMSE. Since these are operations we are about to use repeatedly, we should use the function that we happen to have already written. # train RMSE rmse(actual = train_data$Sales, predicted = predict(fit_0, train_data)) ## [1] 4.788513 # test RMSE rmse(actual = test_data$Sales, predicted = predict(fit_0, test_data)) ## [1] 5.643574 This function can actually be improved for the inputs that we are using. We would like to obtain train and test RMSE for a fitted model, given a train or test dataset, and the appropriate response variable. get_rmse = function(model, data, response) { rmse(actual = data[, response], predicted = predict(model, data)) } By using this function, our code becomes easier to read, and it is more obvious what task we are accomplishing. get_rmse(model = fit_0, data = train_data, response = &quot;Sales&quot;) # train RMSE ## [1] 4.788513 get_rmse(model = fit_0, data = test_data, response = &quot;Sales&quot;) # test RMSE ## [1] 5.643574 4.4 Adding Flexibility to Linear Models Each successive model we fit will be more and more flexible using both interactions and polynomial terms. We will see the training error decrease each time the model is made more flexible. We expect the test error to decrease a number of times, then eventually start going up, as a result of overfitting. fit_1 = lm(Sales ~ ., data = train_data) get_complexity(fit_1) ## [1] 3 get_rmse(model = fit_1, data = train_data, response = &quot;Sales&quot;) # train RMSE ## [1] 1.637699 get_rmse(model = fit_1, data = test_data, response = &quot;Sales&quot;) # test RMSE ## [1] 1.737574 fit_2 = lm(Sales ~ Radio * Newspaper * TV, data = train_data) get_complexity(fit_2) ## [1] 7 get_rmse(model = fit_2, data = train_data, response = &quot;Sales&quot;) # train RMSE ## [1] 0.7797226 get_rmse(model = fit_2, data = test_data, response = &quot;Sales&quot;) # test RMSE ## [1] 1.110372 fit_3 = lm(Sales ~ Radio * Newspaper * TV + I(TV ^ 2), data = train_data) get_complexity(fit_3) ## [1] 8 get_rmse(model = fit_3, data = train_data, response = &quot;Sales&quot;) # train RMSE ## [1] 0.4960149 get_rmse(model = fit_3, data = test_data, response = &quot;Sales&quot;) # test RMSE ## [1] 0.7320758 fit_4 = lm(Sales ~ Radio * Newspaper * TV + I(TV ^ 2) + I(Radio ^ 2) + I(Newspaper ^ 2), data = train_data) get_complexity(fit_4) ## [1] 10 get_rmse(model = fit_4, data = train_data, response = &quot;Sales&quot;) # train RMSE ## [1] 0.488771 get_rmse(model = fit_4, data = test_data, response = &quot;Sales&quot;) # test RMSE ## [1] 0.7466312 fit_5 = lm(Sales ~ Radio * Newspaper * TV + I(TV ^ 2) * I(Radio ^ 2) * I(Newspaper ^ 2), data = train_data) get_complexity(fit_5) ## [1] 14 get_rmse(model = fit_5, data = train_data, response = &quot;Sales&quot;) # train RMSE ## [1] 0.4705201 get_rmse(model = fit_5, data = test_data, response = &quot;Sales&quot;) # test RMSE ## [1] 0.8425384 4.5 Choosing a Model To better understand the relationship between train RMSE, test RMSE, and model complexity, we summarize our results, as the above is somewhat cluttered. First, we recap the models that we have fit. fit_1 = lm(Sales ~ ., data = train_data) fit_2 = lm(Sales ~ Radio * Newspaper * TV, data = train_data) fit_3 = lm(Sales ~ Radio * Newspaper * TV + I(TV ^ 2), data = train_data) fit_4 = lm(Sales ~ Radio * Newspaper * TV + I(TV ^ 2) + I(Radio ^ 2) + I(Newspaper ^ 2), data = train_data) fit_5 = lm(Sales ~ Radio * Newspaper * TV + I(TV ^ 2) * I(Radio ^ 2) * I(Newspaper ^ 2), data = train_data) Next, we create a list of the models fit. model_list = list(fit_1, fit_2, fit_3, fit_4, fit_5) We then obtain train RMSE, test RMSE, and model complexity for each. train_rmse = sapply(model_list, get_rmse, data = train_data, response = &quot;Sales&quot;) test_rmse = sapply(model_list, get_rmse, data = test_data, response = &quot;Sales&quot;) model_complexity = sapply(model_list, get_complexity) We then plot the results. The train RMSE can be seen in blue, while the test RMSE is given in orange. plot(model_complexity, train_rmse, type = &quot;b&quot;, ylim = c(min(c(train_rmse, test_rmse)) - 0.02, max(c(train_rmse, test_rmse)) + 0.02), col = &quot;dodgerblue&quot;, xlab = &quot;Model Size&quot;, ylab = &quot;RMSE&quot;) lines(model_complexity, test_rmse, type = &quot;b&quot;, col = &quot;darkorange&quot;) We also summarize the results as a table. fit_1 is the least flexible, and fit_5 is the most flexible. We see the Train RMSE decrease as flexibility increases. We see that the Test RMSE is smallest for fit_3, thus is the model we believe will perform the best on future data not used to train the model. Note this may not be the best model, but it is the best model of the models we have seen in this example. Model Train RMSE Test RMSE Predictors fit_1 1.6376991 1.7375736 3 fit_2 0.7797226 1.1103716 7 fit_3 0.4960149 0.7320758 8 fit_4 0.488771 0.7466312 10 fit_5 0.4705201 0.8425384 14 To summarize: Underfitting models: In general High Train RMSE, High Test RMSE. Seen in fit_1 and fit_2. Overfitting models: In general Low Train RMSE, High Test RMSE. Seen in fit_4 and fit_5. Specifically, we say that a model is overfitting if there exists a less complex model with lower Test RMSE. Then a model is underfitting if there exists a more complex model with lower Test RMSE. A number of notes on these results: The labels of under and overfitting are relative to the best model we see, fit_3. Any model more complex with higher Test RMSE is overfitting. Any model less complex with higher Test RMSE is underfitting. The train RMSE is guaranteed to follow this non-increasing pattern. The same is not true of test RMSE. Here we see a nice U-shaped curve. There are theoretical reasons why we should expect this, but that is on average. Because of the randomness of one test-train split, we may not always see this result. Re-perform this analysis with a different seed value and the pattern may not hold. We will discuss why we expect this next chapter. We will discuss how we can help create this U-shape much later. Often we expect train RMSE to be lower than test RMSE. Again, due to the randomness of the split, you may get lucky and this will not be true. A final note on the analysis performed here; we paid no attention whatsoever to the “assumptions” of a linear model. We only sought a model that predicted well, and paid no attention to a model for explaination. Hypothesis testing did not play a role in deciding the model, only prediction accuracy. Collinearity? We don’t care. Assumptions? Still don’t care. Diagnostics? Never heard of them. (These statements are a little over the top, and not completely true, but just to drive home the point that we only care about prediction. Often we latch onto methods that we have seen before, even when they are not needed.) "],
["simulating-the-biasvariance-tradeoff.html", "Chapter 5 Simulating the Bias–Variance Tradeoff 5.1 Bias-Variance Decomposition 5.2 Simulation 5.3 Bias-Variance Tradeoff", " Chapter 5 Simulating the Bias–Variance Tradeoff Consider the general regression setup \\[ y = f(\\mathbf x) + \\epsilon \\] with \\[ E[\\epsilon] = 0 \\quad \\text{and} \\quad \\text{var}(\\epsilon) = \\sigma^2. \\] 5.1 Bias-Variance Decomposition Using \\(\\hat{f}(\\mathbf x)\\), trained with data, to estimate \\(f(\\mathbf x)\\), we are interested in the expected prediction error. Specifically, considered making a prediction of \\(y_0 = f(\\mathbf x_0) + \\epsilon\\) at the point \\(\\mathbf x_0\\). In that case, we have \\[ E\\left[\\left(y_0 - \\hat{f}(\\mathbf x_0)\\right)^2\\right] = \\text{bias}\\left(\\hat{f}(\\mathbf x_0)\\right)^2 + \\text{var}\\left(\\hat{f}(\\mathbf x_0)\\right) + \\sigma^2. \\] Recall the definition of the bias of an estimate. \\[ \\text{bias}\\left(\\hat{f}(\\mathbf x_0)\\right) = E\\left[\\hat{f}(\\mathbf x_0)\\right] - f(\\mathbf x_0) \\] So, we have decomposed the error into two types; reducible and irreducible. The reducible can be further decomposed into the squared bias and variance of the estimate. We can “control” these through our choice of model. The irreducible, is noise, that should not and cannot be modeled. 5.2 Simulation We will illustrate this decomposition, and the resulting bias-variance tradeoff through simulation. Suppose we would like a train a model to learn the function \\(f(x) = x^2\\). f = function(x) { x ^ 2 } More specifically, \\[ y = x^2 + \\epsilon \\] where \\[ \\epsilon \\sim N(\\mu = 0, \\sigma^2 = 0.3^2). \\] We write a function which generates data accordingly. get_sim_data = function(f, sample_size = 100) { x = runif(n = sample_size, min = 0, max = 1) y = f(x) + rnorm(n = sample_size, mean = 0, sd = 0.3) data.frame(x, y) } To get a sense of the data, we generate one simulated dataset, and fit the four models that we will be of interest. sim_data = get_sim_data(f, sample_size = 100) fit_1 = lm(y ~ 1, data = sim_data) fit_2 = lm(y ~ poly(x, degree = 1), data = sim_data) fit_3 = lm(y ~ poly(x, degree = 2), data = sim_data) fit_4 = lm(y ~ poly(x, degree = 3), data = sim_data) Plotting these four trained models, we see that the zero predictor model (red) does very poorly. The single predictor model (blue) is reasonable, but we can see that the two (green) and three (orange) predictor models seem more appropriate. Between these latter two, it is hard to see which seems more appropriate. set.seed(430) plot(y ~ x, data = sim_data) grid = seq(from = 0, to = 1, by = 0.01) lines(grid, predict(fit_1, newdata = data.frame(x = grid)), col = &quot;red&quot;, lwd = 2, lty = 2) lines(grid, predict(fit_2, newdata = data.frame(x = grid)), col = &quot;blue&quot;, lwd = 2, lty = 3) lines(grid, predict(fit_3, newdata = data.frame(x = grid)), col = &quot;green&quot;, lwd = 2, lty = 4) lines(grid, predict(fit_4, newdata = data.frame(x = grid)), col = &quot;orange&quot;, lwd = 2, lty = 5) lines(grid, f(grid), col = &quot;black&quot;, lwd = 5) legend(x = 0.75, y = 0, c(&quot;y ~ 1&quot;, &quot;y ~ poly(x, 1)&quot;, &quot;y ~ poly(x, 2)&quot;, &quot;y ~ poly(x, 3)&quot;, &quot;truth&quot;), col = c(&quot;red&quot;, &quot;blue&quot;, &quot;green&quot;, &quot;orange&quot;, &quot;black&quot;), lty = c(2, 3, 4, 5, 1), lwd = 2) We will now use simulation to estimate the bias, variance, and mean squared error for the estimates for \\(f(x)\\) given by these models at the point \\(x_0 = 0.95\\). We use simulation to complete this task, as performing the exact calculations are always difficult, and often impossible. set.seed(1) n_sims = 1000 n_models = 4 x0 = 0.95 predictions = matrix(0, nrow = n_sims, ncol = n_models) sim_data = get_sim_data(f, sample_size = 100) plot(y ~ x, data = sim_data, col = &quot;white&quot;, xlim = c(0.75, 1), ylim = c(0, 1.5)) for (i in 1:n_sims) { sim_data = get_sim_data(f, sample_size = 100) fit_1 = lm(y ~ 1, data = sim_data) fit_2 = lm(y ~ poly(x, degree = 1), data = sim_data) fit_3 = lm(y ~ poly(x, degree = 2), data = sim_data) fit_4 = lm(y ~ poly(x, degree = 3), data = sim_data) lines(grid, predict(fit_1, newdata = data.frame(x = grid)), col = &quot;red&quot;, lwd = 1) # lines(grid, predict(fit_2, newdata = data.frame(x = grid)), col = &quot;blue&quot;, lwd = 1) # lines(grid, predict(fit_3, newdata = data.frame(x = grid)), col = &quot;green&quot;, lwd = 1) lines(grid, predict(fit_4, newdata = data.frame(x = grid)), col = &quot;orange&quot;, lwd = 1) predictions[i, ] = c( predict(fit_1, newdata = data.frame(x = x0)), predict(fit_2, newdata = data.frame(x = x0)), predict(fit_3, newdata = data.frame(x = x0)), predict(fit_4, newdata = data.frame(x = x0)) ) } points(x0, f(x0), col = &quot;black&quot;, pch = &quot;x&quot;, cex = 2) The above plot shows the 1000 trained models for each of the zero predictor and three predictor models. (We have exlcuded the one and two predictor models for clarity of the plot.) The truth at \\(x_0 = 0.95\\) is given by a black “X”. We see that the red lines for the zero predictor model are on average wrong, with some variability. The orange lines for the three predictor model are on average correct, but with more variance. 5.3 Bias-Variance Tradeoff To evaluate the bias and variance, we simulate values for the response \\(y\\) at \\(x_0 = 0.95\\) according to the true model. eps = rnorm(n = n_sims, mean = 0, sd = 0.3) y0 = f(x0) + eps R already has a function to calculate variance, however, we add functions for bias and mean squared error. get_bias = function(estimate, truth) { mean(estimate) - truth } get_mse = function(estimate, truth) { mean((estimate - truth) ^ 2) } When then use the predictions obtained from the above simulation to estimate the bias, variance and mean squared error for estimating \\(f(x)\\) at \\(x_0 = 0.95\\) for the four models. bias = apply(predictions, 2, get_bias, f(x0)) variance = apply(predictions, 2, var) mse = apply(predictions, 2, get_mse, y0) We summarize these results in the following table. Model Squared Bias Variance (Of Estimate) MSE fit_1 0.322916 0.001784 0.4201411 fit_2 0.0136794 0.0036355 0.1145159 fit_3 0.0000036 0.0058178 0.1031294 fit_4 0.0000009 0.0079906 0.1053599 A number of things to notice here: We use squared bias in this table. Since bias can be positive or negative, squared bias is more useful for observing the trend as complexity increases. The squared bias trend which we see here is decreasing bias as complexity increases, which we expect to see in general. The exact opposite is true of variance. As model complexity increases, variance increases. The mean squared error, which is a function of the bias and variance, decreases, then increases. This is a result of the bias-variance tradeoff. We can decrease bias, by increases variance. Or, we can decrease variance by increasing bias. By striking the correct balance, we can find a good mean squared error. We can check for these trends with the diff() function in R. all(diff(bias ^ 2) &lt; 0) ## [1] TRUE all(diff(variance) &gt; 0) ## [1] TRUE diff(mse) ## [1] -0.305625170 -0.011386537 0.002230515 Notice that the table lacks a column for the variance of the noise. Add this to squared bias and variance would give the mean squared error. However, notice that we are simulation to estiamte the bias and variance, so the relationship is not exact. If we used more replications of the simulation, these two values would move closer together. bias ^ 2 + variance + var(eps) ## [1] 0.4209744 0.1135892 0.1020958 0.1042659 mse ## [1] 0.4201411 0.1145159 0.1031294 0.1053599 "],
["classification.html", "Chapter 6 Classification 6.1 Visualization for Classification 6.2 A Simple Classifier 6.3 Metrics for Classification", " Chapter 6 Classification Classification is a form of supervised learning where the response variable is categorical, as opposed to numeric for regression. Our goal is to find a rule, algorithm, or function which takes as input a feature vector, and outputs a category which is the true category as often as possible. That is, the classifier \\(\\hat{C}\\) returns the predicted category \\(\\hat{y}\\). \\[ \\hat{y}_i = \\hat{C}(\\bf x_i) \\] To build our first classifier, we will use the Default dataset from the ISLR package. library(ISLR) library(tibble) as_tibble(Default) ## # A tibble: 10,000 x 4 ## default student balance income ## &lt;fctr&gt; &lt;fctr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 No No 729.5265 44361.625 ## 2 No Yes 817.1804 12106.135 ## 3 No No 1073.5492 31767.139 ## 4 No No 529.2506 35704.494 ## 5 No No 785.6559 38463.496 ## 6 No Yes 919.5885 7491.559 ## 7 No No 825.5133 24905.227 ## 8 No Yes 808.6675 17600.451 ## 9 No No 1161.0579 37468.529 ## 10 No No 0.0000 29275.268 ## # ... with 9,990 more rows Our goal is to properly classify individuals as defaulters based on student status, credit card balance, and income. Be aware that the response default is a factor, as is the predictor student. is.factor(Default$default) ## [1] TRUE is.factor(Default$student) ## [1] TRUE As we did with regression, we test-train split our data. In this case, using 50% for each. set.seed(42) train_index = sample(nrow(Default), 5000) train_default = Default[train_index, ] test_default = Default[-train_index, ] 6.1 Visualization for Classification Often, some simple visualizations can suggest simple classification rules. To quickly create some useful visualizations, we use the featurePlot() function from the caret() package. library(caret) A density plot can often suggest a simple split based on a numeric predictor. Essentially this plot graphs a density estimate \\[ f_{X_i}(x_i \\mid y = k) \\] for each numeric predictor \\(x_i\\) and each category \\(k\\) of the response \\(y\\). featurePlot(x = train_default[, c(&quot;balance&quot;, &quot;income&quot;)], y = train_default$default, plot = &quot;density&quot;, scales = list(x = list(relation = &quot;free&quot;), y = list(relation = &quot;free&quot;)), adjust = 1.5, pch = &quot;|&quot;, layout = c(2, 1), auto.key = list(columns = 2)) Some notes about the arguments to this function: x is a data frame containing only numeric predictors. It would be nonsensical to estimate a density for a categorical predictor. y is the response variable. It needs to be a factor variable. If coded as 0 and 1, you will need to coerce to factor for plotting. plot specifies the type of plot, here density. scales defines the scale of the axes for each plot. By default, the axis of each plot would be the same, which often is not useful, so the arguments here, a different axis for each plot, will almost always be used. adjust specifies the amount of smoothing used for the density estimate. pch specifies the plot character used for the bottom of the plot. layout places the individual plots into rows and columns. For some odd reason, it is given as (col, row). auto.key defines the key at the top of the plot. The number of columns should be the number of categories. It seems that the income variable by itself is not particularly useful. However, there seems to be a big difference in default status at a balance of about 1400. We will use this information shortly. featurePlot(x = train_default[, c(&quot;balance&quot;, &quot;income&quot;)], y = train_default$student, plot = &quot;density&quot;, scales = list(x = list(relation = &quot;free&quot;), y = list(relation = &quot;free&quot;)), adjust = 1.5, pch = &quot;|&quot;, layout = c(2, 1), auto.key = list(columns = 2)) Above, we create a similar plot, except with student as the response. We see that students often carry a slightly larger balance, and have far lower income. This will be useful to know when making more complicated classifiers. featurePlot(x = train_default[, c(&quot;student&quot;, &quot;balance&quot;, &quot;income&quot;)], y = train_default$default, plot = &quot;pairs&quot;, auto.key = list(columns = 2)) We can use plot = &quot;pairs&quot; to consider multiple variables at the same time. This plot reinforces using balance to create a classifier, and again shows that income seems not that useful. library(ellipse) featurePlot(x = train_default[, c(&quot;balance&quot;, &quot;income&quot;)], y = train_default$default, plot = &quot;ellipse&quot;, auto.key = list(columns = 2)) Similar to pairs is a plot of type ellipse, which requires the ellipse package. Here we only use numeric predictors, as essentially we are assuming multivariate normality. The ellipses mark points of equal density. This will be useful later when discussing LDA and QDA. 6.2 A Simple Classifier A very simple classifier is a rule based on a boundary \\(b\\) for a particular input variable \\(x\\). \\[ \\hat{C}(\\bf x) = \\begin{cases} 1 &amp; x &gt; b \\\\ 0 &amp; x \\leq b \\end{cases} \\] Based on the first plot, we believe we can use balance to create a reasonable classifier. In particular, \\[ \\hat{C}(\\text{balance}) = \\begin{cases} \\text{Yes} &amp; \\text{balance} &gt; 1400 \\\\ \\text{No} &amp; \\text{balance} \\leq 1400 \\end{cases} \\] So we predict an individual is a defaulter if their balance is above 1400, and not a defaulter if the balance is 1400 or less. simple_class = function(x, boundary, above = 1, below = 0) { ifelse(x &gt; boundary, above, below) } We write a simple R function that compares a variable to a boundary, then use it to make predictions on the train and test sets with our chosen variable and boundary. train_pred = simple_class(x = train_default$balance, boundary = 1400, above = &quot;Yes&quot;, below = &quot;No&quot;) test_pred = simple_class(x = test_default$balance, boundary = 1400, above = &quot;Yes&quot;, below = &quot;No&quot;) head(train_pred, n = 10) ## [1] &quot;No&quot; &quot;Yes&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; 6.3 Metrics for Classification In the classification setting, there are a large number of metrics to asses how well a classifier is performing. One of the most obvious things to do is arrange predictions and true values in a cross table. (train_tab = table(predicted = train_pred, actual = train_default$default)) ## actual ## predicted No Yes ## No 4319 29 ## Yes 513 139 (test_tab = table(predicted = test_pred, actual = test_default$default)) ## actual ## predicted No Yes ## No 4361 23 ## Yes 474 142 Often we give specific names to individual cells of these tables, and in the predictive setting, we would call this table a confusion matrix. Be aware, that the placement of Actual and Predicted values affects the names of the cells, and often the matrix may be presented transposed. In statistics, we label the errors Type I and Type II, but these are hard to remember. False Positive and False Negative are more descriptive, so we choose to use these. The confusionMatrix() function from the caret package can be used to obtain a wealth of additional information, which we see output below for the test data. Note that we specify which category is considered “positive.” train_con_mat = confusionMatrix(train_tab, positive = &quot;Yes&quot;) (test_con_mat = confusionMatrix(test_tab, positive = &quot;Yes&quot;)) ## Confusion Matrix and Statistics ## ## actual ## predicted No Yes ## No 4361 23 ## Yes 474 142 ## ## Accuracy : 0.9006 ## 95% CI : (0.892, 0.9088) ## No Information Rate : 0.967 ## P-Value [Acc &gt; NIR] : 1 ## ## Kappa : 0.3287 ## Mcnemar&#39;s Test P-Value : &lt;2e-16 ## ## Sensitivity : 0.8606 ## Specificity : 0.9020 ## Pos Pred Value : 0.2305 ## Neg Pred Value : 0.9948 ## Prevalence : 0.0330 ## Detection Rate : 0.0284 ## Detection Prevalence : 0.1232 ## Balanced Accuracy : 0.8813 ## ## &#39;Positive&#39; Class : Yes ## The most common, and most important metric is the classification accuracy. \\[ \\text{Acc}(\\hat{C}, \\text{Data}) = \\frac{1}{n}\\sum_{i = 1}^{n}I(y_i = \\hat{C}(\\bf x_i)) \\] Here, \\(I\\) is an indicator function, so we are essentially calculating the proportion of predicted classes that match the true class. \\[ I(y_i = \\hat{C}(x)) = \\begin{cases} 1 &amp; y_i = \\hat{C}(x) \\\\ 0 &amp; y_i \\neq \\hat{C}(x) \\\\ \\end{cases} \\] It is also common to discuss the misclassification rate, or classification error, which is simply one minus the accuracy. Like regression, we often split the data, and then consider Train Accuracy and Test Accuracy. Test Accuracy will be used as a measure of how well a classifier will work on unseen future data. \\[ \\text{Acc}_{\\text{Train}}(\\hat{C}, \\text{Train Data}) = \\frac{1}{n_{Tr}}\\sum_{i \\in \\text{Train}}^{}I(y_i = \\hat{C}(\\bf x_i)) \\] \\[ \\text{Acc}_{\\text{Test}}(\\hat{C}, \\text{Test Data}) = \\frac{1}{n_{Te}}\\sum_{i \\in \\text{Test}}^{}I(y_i = \\hat{C}(\\bf x_i)) \\] These accuracy values are given by calling confusionMatrix(), or, if stored, can be accessed directly. train_con_mat$overall[&quot;Accuracy&quot;] ## Accuracy ## 0.8916 test_con_mat$overall[&quot;Accuracy&quot;] ## Accuracy ## 0.9006 Sometimes guarding against making certain errors, FP or FN, are more important than simply finding the best accuracy. Thus, sometimes we will consider sensitivity and specificity. \\[ \\text{Sens} = \\text{True Positive Rate} = \\frac{\\text{TP}}{\\text{P}} = \\frac{\\text{TP}}{\\text{TP + FN}} \\] test_con_mat$byClass[&quot;Sensitivity&quot;] ## Sensitivity ## 0.8606061 \\[ \\text{Spec} = \\text{True Negative Rate} = \\frac{\\text{TN}}{\\text{N}} = \\frac{\\text{TN}}{\\text{TN + FP}} \\] test_con_mat$byClass[&quot;Specificity&quot;] ## Specificity ## 0.9019648 Like accuracy, these can easily be found using confusionMatrix(). When considering how well a classifier is performing, often, it is understandable to assume that any accuracy in a binary classification problem above 0.50, is a reasonable classifier. This however is not the case. We need to consider the balance of the classes. To do so, we look at the prevalence of positive cases. \\[ \\text{Prev} = \\frac{\\text{P}}{\\text{Total Obs}}= \\frac{\\text{TP + FN}}{\\text{Total Obs}} \\] train_con_mat$byClass[&quot;Prevalence&quot;] ## Prevalence ## 0.0336 test_con_mat$byClass[&quot;Prevalence&quot;] ## Prevalence ## 0.033 Here, we see an extremely low prevalence, which suggests an even simpler classifier than our current based on balance. \\[ \\hat{C}(\\text{balance}) = \\begin{cases} \\text{No} &amp; \\text{balance} &gt; 1400 \\\\ \\text{No} &amp; \\text{balance} \\leq 1400 \\end{cases} \\] This classifier simply classifies all observations as negative cases. pred_all_no = simple_class(test_default$balance, boundary = 1400, above = &quot;No&quot;, below = &quot;No&quot;) table(predicted = pred_all_no, actual = test_default$default) ## actual ## predicted No Yes ## No 4835 165 The confusionMatrix() function won’t even accept this table as input, because it isn’t a full matrix, only one row, so we calculate some metrics “by hand”. 4835 / (4835 + 165) # test accuracy ## [1] 0.967 1 - 0.0336 # 1 - (train prevelence) ## [1] 0.9664 1 - 0.033 # 1 - (test prevelence) ## [1] 0.967 This classifier does better than the previous. But the point is, in reality, to create a good classifier, we should obtain a test accuracy better than 0.967, which is obtained by simply manipulating the prevalence. Next chapter, we’ll introduce much better classifiers which should have no problem accomplishing this task. "],
["logistic-regression.html", "Chapter 7 Logistic Regression 7.1 Linear Regression 7.2 Bayes Classifier 7.3 Logistic Regression with glm() 7.4 ROC Curves 7.5 Multinomial Logistic Regression", " Chapter 7 Logistic Regression In this chapter, we continue our discussion of classification. We introduce our first model for classification, logistic regression. To begin, we return to the Default dataset from the previous chapter. library(ISLR) library(tibble) as_tibble(Default) ## # A tibble: 10,000 x 4 ## default student balance income ## &lt;fctr&gt; &lt;fctr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 No No 729.5265 44361.625 ## 2 No Yes 817.1804 12106.135 ## 3 No No 1073.5492 31767.139 ## 4 No No 529.2506 35704.494 ## 5 No No 785.6559 38463.496 ## 6 No Yes 919.5885 7491.559 ## 7 No No 825.5133 24905.227 ## 8 No Yes 808.6675 17600.451 ## 9 No No 1161.0579 37468.529 ## 10 No No 0.0000 29275.268 ## # ... with 9,990 more rows We also repeat the test-train split from the previous chapter. set.seed(42) default_index = sample(nrow(Default), 5000) default_train = Default[default_index, ] default_test = Default[-default_index, ] 7.1 Linear Regression Before moving on to logistic regression, why not plain, old, linear regression? default_train_lm = default_train default_test_lm = default_test Since linear regression expects a numeric response variable, we coerce the response to be numeric. (Notice that we also shift the results, as we require 0 and 1, not 1 and 2.) Notice we have also copied the dataset so that we can return the original data with factors later. default_train_lm$default = as.numeric(default_train_lm$default) - 1 default_test_lm$default = as.numeric(default_test_lm$default) - 1 Why would we think this should work? Recall that, \\[ \\hat{E}[Y \\mid X = x] = X\\hat{\\beta}. \\] Since \\(Y\\) is limited to values of \\(0\\) and \\(1\\), we have \\[ E[Y \\mid X = x] = P[Y = 1 \\mid X = x]. \\] It would then seem reasonable that \\(X\\hat{\\beta}\\) is a reasonable estimate of \\(P[Y = 1 \\ X = x]\\). We test this on the Default data. model_lm = lm(default ~ balance, data = default_train_lm) Everything seems to be working, until we plot the results. plot(default ~ balance, data = default_train_lm, col = &quot;darkorange&quot;, pch = &quot;|&quot;, ylim = c(-0.2, 1), main = &quot;Using Linear Regression for Classification&quot;) abline(h = 0, lty = 3) abline(h = 1, lty = 3) abline(h = 0.5, lty = 2) abline(model_lm, lwd = 3, col = &quot;dodgerblue&quot;) Two issues arise. First, all of the predicted probabilities are below 0.5 That means, we would classify every observation as a &quot;No&quot;. This is certainly possible, but not what we would expect. all(predict(model_lm) &lt; 0.5) ## [1] TRUE The next, and bigger issue, is predicted probabilities less than 0. any(predict(model_lm) &lt; 0) ## [1] TRUE 7.2 Bayes Classifier Why are we using a predicted probability of 0.5 as the cutoff for classification? Recall, the Bayes Classifier, which minimizes the classification error: \\[ C^B({\\bf x}) = \\underset{k}{\\mathrm{argmax}} \\ P[Y = k \\mid {\\bf X = x}] \\] So, in the binary classification problem, we will use predicted probabilities \\[ \\hat{p}({\\bf x}) = \\hat{P}[Y = 1 \\mid {\\bf X = x}] \\] and \\[ \\hat{P}[Y = 0 \\mid {\\bf X = x}] \\] and then classify to the larger of the two. We actually only need to consider a single probability, usually for \\(\\hat{P}[Y = 1 \\mid {\\bf X = x}]\\). Since we use it so often, we give it the shorthand notation, \\(\\hat{p}({\\bf x})\\). Then the classifier is written, \\[ \\hat{C}(\\bf x) = \\begin{cases} 1 &amp; \\hat{p}({\\bf x}) &gt; 0.5 \\\\ 0 &amp; \\hat{p}({\\bf x}) \\leq 0.5 \\end{cases} \\] 7.3 Logistic Regression with glm() To better estimate the probability \\[ p({\\bf x}) = P[Y = 1 \\mid {\\bf X = x}] \\] we turn to logistic regression. The model is written \\[ \\log\\left(\\frac{p({\\bf x})}{1 - p({\\bf x})}\\right) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p. \\] Rearranging, we see the probabilities can be written as \\[ p({\\bf x}) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p)}} = \\sigma(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p) \\] Notice, we use the sigmoid function as shorthand notation, which appears often in deep learning literature. It takes any real input, and outputs a number between 0 and 1. How useful! \\[ \\sigma(x) = \\frac{1}{1 + e^{-x}} \\] The model is fit by numerically maximizing the likelihood, which we will let R take care of. We start with a single predictor example, again using balance as our single predictor. model_glm = glm(default ~ balance, data = default_train, family = &quot;binomial&quot;) Fitting this model looks very similar to fitting a simple linear regression. Instead of lm() we use glm(). The only other difference is the use of family = &quot;binomial&quot; which indicates that we have a two-class categorical response. Using glm() with family = &quot;gaussian&quot; would perform the usual linear regression. First, we can obtain the fitted coefficients the same way we did with linear regression. coef(model_glm) ## (Intercept) balance ## -10.452182876 0.005367655 The next thing we should understand is how the predict() function works with glm(). So, let’s look a some predictions. head(predict(model_glm)) ## 9149 9370 2861 8302 6415 5189 ## -6.9616496 -0.7089539 -4.8936916 -9.4123620 -9.0416096 -7.3600645 By default, predict.glm() uses type = &quot;link&quot;. head(predict(model_glm, type = &quot;link&quot;)) ## 9149 9370 2861 8302 6415 5189 ## -6.9616496 -0.7089539 -4.8936916 -9.4123620 -9.0416096 -7.3600645 That is, R is returning \\[ \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\cdots + \\hat{\\beta}_p x_p \\] for each observation. Importantly, these are not predicted probabilities. To obtain the predicted probabilities \\[ \\hat{p}({\\bf x}) = \\hat{P}[Y = 1 \\mid {\\bf X = x}] \\] we need to use type = &quot;response&quot; head(predict(model_glm, type = &quot;response&quot;)) ## 9149 9370 2861 8302 6415 ## 9.466353e-04 3.298300e-01 7.437969e-03 8.170105e-05 1.183661e-04 ## 5189 ## 6.357530e-04 Note that these are probabilities, not classifications. To obtain classifications, we will need to compare to the correct cutoff value with an ifelse() statement. model_glm_pred = ifelse(predict(model_glm, type = &quot;link&quot;) &gt; 0, &quot;Yes&quot;, &quot;No&quot;) # model_glm_pred = ifelse(predict(model_glm, type = &quot;response&quot;) &gt; 0.5, &quot;Yes&quot;, &quot;No&quot;) The line that is run is performing \\[ \\hat{C}(\\bf x) = \\begin{cases} 1 &amp; \\hat{f}({\\bf x}) &gt; 0 \\\\ 0 &amp; \\hat{f}({\\bf x}) \\leq 0 \\end{cases} \\] where \\[ \\hat{f}({\\bf x}) =\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\cdots + \\hat{\\beta}_p x_p. \\] The commented line, which would give the same results, is performing \\[ \\hat{C}(\\bf x) = \\begin{cases} 1 &amp; \\hat{p}({\\bf x}) &gt; 0.5 \\\\ 0 &amp; \\hat{p}({\\bf x}) \\leq 0.5 \\end{cases} \\] where \\[ \\hat{p}({\\bf x}) = \\hat{P}[Y = 1 \\mid {\\bf X = x}]. \\] Once we have classifications, we can calculate metrics such as accuracy. mean(model_glm_pred == default_train$default) # train accuracy ## [1] 0.9722 As we saw previously, the table() and confusionMatrix() functions can be used to quickly obtain many more metrics. train_tab = table(predicted = model_glm_pred, actual = default_train$default) library(caret) train_con_mat = confusionMatrix(train_tab, positive = &quot;Yes&quot;) c(train_con_mat$overall[&quot;Accuracy&quot;], train_con_mat$byClass[&quot;Sensitivity&quot;], train_con_mat$byClass[&quot;Specificity&quot;]) ## Accuracy Sensitivity Specificity ## 0.9722000 0.2738095 0.9964818 As we did with regression, we could also write a custom function for accuracy. get_accuracy = function(mod, data, res = &quot;y&quot;, pos = 1, neg = 0, cut = 0.5) { probs = predict(mod, newdata = data, type = &quot;response&quot;) preds = ifelse(probs &gt; cut, pos, neg) mean(data[, res] == preds) } This function will be useful later when calculating train and test accuracies for several models at the same time. get_accuracy(model_glm, data = default_train, res = &quot;default&quot;, pos = &quot;Yes&quot;, neg = &quot;No&quot;, cut = 0.5) ## [1] 0.9722 To see how much better logistic regression is for this task, we create the same plot we used for linear regression. plot(default ~ balance, data = default_train_lm, col = &quot;darkorange&quot;, pch = &quot;|&quot;, ylim = c(-0.2, 1), main = &quot;Using Logistic Regression for Classification&quot;) abline(h = 0, lty = 3) abline(h = 1, lty = 3) abline(h = 0.5, lty = 2) curve(predict(model_glm, data.frame(balance = x), type = &quot;response&quot;), add = TRUE, lwd = 3, col = &quot;dodgerblue&quot;) abline(v = -coef(model_glm)[1] / coef(model_glm)[2], lwd = 2) This plot contains a wealth of information. The orange | characters are the data, \\((x_i, y_i)\\). The blue “curve” is the predicted probabilities given by the fitted logistic regression. That is, \\[ \\hat{p}({\\bf x}) = \\hat{P}[Y = 1 \\mid {\\bf X = x}] \\] The solid vertical black line represents the decision boundary, the balance that obtains a predicted probability of 0.5. In this case balance = 1947.252994. The decision boundary is found by solving for points that satisfy \\[ \\hat{p}({\\bf x}) = \\hat{P}[Y = 1 \\mid {\\bf X = x}] = 0.5 \\] This is equivalent to point that satisfy \\[ \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 = 0. \\] Thus, for logistic regression with a single predictor, the decision boundary is given by the point \\[ x_1 = \\frac{-\\hat{\\beta}_0}{\\hat{\\beta}_1}. \\] The following is not run, but an alternative way to add the logistic curve to the plot. grid = seq(0, max(default_train$balance), by = 0.01) sigmoid = function(x) { 1 / (1 + exp(-x)) } lines(grid, sigmoid(coef(model_glm)[1] + coef(model_glm)[2] * grid), lwd = 3) Using the usual formula syntax, it is easy to add complexity to logistic regressions. model_1 = glm(default ~ 1, data = default_train, family = &quot;binomial&quot;) model_2 = glm(default ~ ., data = default_train, family = &quot;binomial&quot;) model_3 = glm(default ~ . ^ 2 + I(balance ^ 2), data = default_train, family = &quot;binomial&quot;) Note that, using polynomial transformations of predictors will allow a linear model to have non-linear decision boundaries. model_list = list(model_1, model_2, model_3) train_error = 1 - sapply(model_list, get_accuracy, data = default_train, res = &quot;default&quot;, pos = &quot;Yes&quot;, neg = &quot;No&quot;, cut = 0.5) test_error = 1 - sapply(model_list, get_accuracy, data = default_test, res = &quot;default&quot;, pos = &quot;Yes&quot;, neg = &quot;No&quot;, cut = 0.5) Here we see the misclassification error rates for each model. The train decreases, and the test decreases, until it starts to increases. Everything we learned about the bias-variance tradeoff for regression also applies here. diff(train_error) ## [1] -0.0058 -0.0002 diff(test_error) ## [1] -0.0068 0.0004 We call model_2 the additive logistic model, which we will use quite often. 7.4 ROC Curves Let’s return to our simple model with only balance as a predictor. model_glm = glm(default ~ balance, data = default_train, family = &quot;binomial&quot;) We write a function which allows use to make predictions based on different probability cutoffs. get_pred = function(mod, data, res = &quot;y&quot;, pos = 1, neg = 0, cut = 0.5) { probs = predict(mod, newdata = data, type = &quot;response&quot;) ifelse(probs &gt; cut, pos, neg) } \\[ \\hat{C}(\\bf x) = \\begin{cases} 1 &amp; \\hat{f}({\\bf x}) &gt; c \\\\ 0 &amp; \\hat{f}({\\bf x}) \\leq c \\end{cases} \\] Let’s use this to obtain predictions using a low, medium, and high cutoff. (0.1, 0.5, and 0.9) test_pred_10 = get_pred(model_glm, data = default_test, res = &quot;default&quot;, pos = &quot;Yes&quot;, neg = &quot;No&quot;, cut = 0.1) test_pred_50 = get_pred(model_glm, data = default_test, res = &quot;default&quot;, pos = &quot;Yes&quot;, neg = &quot;No&quot;, cut = 0.5) test_pred_90 = get_pred(model_glm, data = default_test, res = &quot;default&quot;, pos = &quot;Yes&quot;, neg = &quot;No&quot;, cut = 0.9) Now we evaluate accuracy, sensitivity, and specificity for these classifiers. test_tab_10 = table(predicted = test_pred_10, actual = default_test$default) test_tab_50 = table(predicted = test_pred_50, actual = default_test$default) test_tab_90 = table(predicted = test_pred_90, actual = default_test$default) test_con_mat_10 = confusionMatrix(test_tab_10, positive = &quot;Yes&quot;) test_con_mat_50 = confusionMatrix(test_tab_50, positive = &quot;Yes&quot;) test_con_mat_90 = confusionMatrix(test_tab_90, positive = &quot;Yes&quot;) metrics = rbind( c(test_con_mat_10$overall[&quot;Accuracy&quot;], test_con_mat_10$byClass[&quot;Sensitivity&quot;], test_con_mat_10$byClass[&quot;Specificity&quot;]), c(test_con_mat_50$overall[&quot;Accuracy&quot;], test_con_mat_50$byClass[&quot;Sensitivity&quot;], test_con_mat_50$byClass[&quot;Specificity&quot;]), c(test_con_mat_90$overall[&quot;Accuracy&quot;], test_con_mat_90$byClass[&quot;Sensitivity&quot;], test_con_mat_90$byClass[&quot;Specificity&quot;]) ) rownames(metrics) = c(&quot;c = 0.10&quot;, &quot;c = 0.50&quot;, &quot;c = 0.90&quot;) metrics ## Accuracy Sensitivity Specificity ## c = 0.10 0.9404 0.77575758 0.9460186 ## c = 0.50 0.9738 0.31515152 0.9962771 ## c = 0.90 0.9674 0.01818182 0.9997932 We see then sensitivity decreases as the cutoff is increased. Conversely, specificity increases as the cutoff increases. This is useful if we are more interested in a particular error, instead of giving them equal weight. Note that usually the best accuracy will be seen near \\(c = 0.50\\). Instead of manually checking cutoffs, we can create an ROC curve (receiver operating characteristic curve) which will sweep through all possible cutoffs, and plot the sensitivity and specificity. library(pROC) test_prob = predict(model_glm, newdata = default_test, type = &quot;response&quot;) test_roc = roc(default_test$default ~ test_prob, plot = TRUE, print.auc = TRUE) as.numeric(test_roc$auc) ## [1] 0.9515076 A good model will have a high AUC, that is as often as possible a high sensitivity and specificity. 7.5 Multinomial Logistic Regression What if the response contains more than two categories? For that we need multinomial logistic regression. \\[ P[Y = k \\mid {\\bf X = x}] = \\frac{e^{\\beta_{0k} + \\beta_{1k} x_1 + \\cdots + + \\beta_{pk} x_p}}{\\sum_{j = 1}^{K} e^{\\beta_{0j} + \\beta_{1j} x_1 + \\cdots + + \\beta_{pj} x_p}} \\] We will omit the details, as ISL has as well. If you are interested, the Wikipedia page provides a rather thorough coverage. Also note that the above is an example of the softmax function. As an example of a dataset with a three category response, we use the iris dataset, which is so famous, it has its own Wikipedia entry. It is also a default dataset in R, so no need to load it. Before proceeding, we test-train split this data. set.seed(430) iris_obs = nrow(iris) iris_index = sample(iris_obs, size = trunc(0.50 * iris_obs)) iris_train = iris[iris_index, ] iris_test = iris[-iris_index, ] To perform multinomial logistic regression, we use the multinom function from the nnet package. Training using multinom() is done using similar syntax to lm() and glm(). We add the trace = FALSE argument to suppress information about updates to the optimization routine as the model is trained. library(nnet) model_multi = multinom(Species ~ ., data = iris_train, trace = FALSE) summary(model_multi)$coefficients ## (Intercept) Sepal.Length Sepal.Width Petal.Length Petal.Width ## versicolor 26.81602 -6.983313 -16.24574 20.35750 3.218787 ## virginica -34.24228 -8.398869 -17.03985 31.94659 11.594518 Notice we are only given coefficients for two of the three class, much like only needing coefficients for one class in logistic regression. A difference between glm() and multinom() is how the predict() function operates. head(predict(model_multi, newdata = iris_train)) ## [1] setosa virginica setosa setosa virginica setosa ## Levels: setosa versicolor virginica head(predict(model_multi, newdata = iris, type = &quot;prob&quot;)) ## setosa versicolor virginica ## 1 1 1.386333e-16 1.137629e-39 ## 2 1 1.888634e-12 3.059666e-35 ## 3 1 3.868198e-14 2.226923e-37 ## 4 1 2.315067e-11 1.687874e-33 ## 5 1 5.490420e-17 4.794326e-40 ## 6 1 2.196721e-17 1.482366e-38 Notice that by default, classifications are returned. When obtaining probabilities, we are given the predicted probability for each class. Interestingly, you’ve just fit a neural network, and you didn’t even know it! (Hence the nnet package.) Later we will discuss the connections between logistic regression, multinomial logistic regression, and simple neural networks. "],
["generative-models.html", "Chapter 8 Generative Models 8.1 Linear Discriminant Analysis 8.2 Quadratic Discriminant Analysis 8.3 Naive Bayes 8.4 Discrete Inputs 8.5 RMarkdown", " Chapter 8 Generative Models In this chapter, we continue our discussion of classification methods. We introduce three new methods, each a generative method. This in comparison to logistic regression, which is a discriminative method. Generative methods model the joint probability, \\(p(x, y)\\), often by assuming some distribution for the conditional distribution of \\(X\\) given \\(Y\\), \\(f(x \\mid y)\\). Bayes theorem is then applied to classify according to \\(p(y \\mid x)\\). Discriminative methods directly model this conditional, \\(p(y \\mid x)\\). A detailed discussion and analysis can be found in Ng and Jordan, 2002. Each of the methods in this chapter will use Bayes theorem to build a classifier. \\[ p_k(x) = P[Y = k \\mid {\\mathbf X} = {\\mathbf x}] = \\frac{\\pi_k \\cdot f_k({\\mathbf x})}{\\sum_{i = 1 }^{K} \\pi_k \\cdot f_k({\\mathbf x})} \\] We call \\(p_k(x)\\) the posterior probability, which we will estimate then use to create classifications. The \\(\\pi_k\\) are called the prior probabilities for each class \\(k\\). That is, \\(P[y = k]\\), unconditioned on \\(X\\). The \\(f_k({\\mathbf x})\\) are called the likelihoods, which are indexed by \\(k\\) to denote that they are conditional on the classes. The denominator is often referred to as a normalizing constant. The methods will differ by placing different modeling assumptions on the likelihoods, \\(f_k({\\mathbf x})\\). For each method, the priors could be learned from data or pre-specified. For each method, classifications are made to the class with the highest estimated posterior probability, which is equivalent to the class with the largest \\[ \\log(\\hat{\\pi}_k \\cdot \\hat{f}_k({\\mathbf x})). \\] By substituting the corresponding likelihoods, simplifying, and eliminating unnecessary terms, we could derive the discriminant function for each. To illustrate these new methods, we return to the iris data, which you may remember has three classes. After a test-train split, we create a number of plots to refresh our memory. set.seed(430) iris_obs = nrow(iris) iris_index = sample(iris_obs, size = trunc(0.50 * iris_obs)) # iris_index = sample(iris_obs, size = trunc(0.10 * iris_obs)) iris_train = iris[iris_index, ] iris_test = iris[-iris_index, ] caret::featurePlot(x = iris_train[, c(&quot;Sepal.Length&quot;, &quot;Sepal.Width&quot;, &quot;Petal.Length&quot;, &quot;Petal.Width&quot;)], y = iris_train$Species, plot = &quot;density&quot;, scales = list(x = list(relation = &quot;free&quot;), y = list(relation = &quot;free&quot;)), adjust = 1.5, pch = &quot;|&quot;, layout = c(2, 2), auto.key = list(columns = 3)) caret::featurePlot(x = iris_train[, c(&quot;Sepal.Length&quot;, &quot;Sepal.Width&quot;, &quot;Petal.Length&quot;, &quot;Petal.Width&quot;)], y = iris_train$Species, plot = &quot;ellipse&quot;, auto.key = list(columns = 3)) caret::featurePlot(x = iris_train[, c(&quot;Sepal.Length&quot;, &quot;Sepal.Width&quot;, &quot;Petal.Length&quot;, &quot;Petal.Width&quot;)], y = iris_train$Species, plot = &quot;box&quot;, scales = list(y = list(relation=&quot;free&quot;), x = list(rot = 90)), layout = c(4, 1)) Especially based on the pairs plot, we see that it should not be too difficult to find a good classifier. Notice that we use caret::featurePlot to access the featurePlot() function without loading the entire caret package. 8.1 Linear Discriminant Analysis LDA assumes that the predictors are multivariate normal conditioned on the classes. \\[ {\\mathbf X} \\mid Y = k \\sim N(\\mu_k, \\Sigma) \\] \\[ f_k({\\mathbf x}) = \\frac{1}{(2\\pi)^{p/2}|\\Sigma|^{1/2}}\\exp\\left[-\\frac{1}{2}(\\mathbf x - \\mu_k)^{\\prime}\\Sigma^{-1}(\\mathbf x - \\mu_k)\\right] \\] Notice that \\(\\Sigma\\) does not depend on \\(k\\), that is, we are assuming the same \\(\\Sigma\\) for each class. We then use information from all the classes to estimate \\(\\Sigma\\). To fit an LDA model, we use the lda() function from the MASS package. library(MASS) iris_lda = lda(Species ~ ., data = iris_train) iris_lda ## Call: ## lda(Species ~ ., data = iris_train) ## ## Prior probabilities of groups: ## setosa versicolor virginica ## 0.3733333 0.3200000 0.3066667 ## ## Group means: ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## setosa 4.978571 3.378571 1.432143 0.2607143 ## versicolor 5.995833 2.808333 4.254167 1.3333333 ## virginica 6.669565 3.065217 5.717391 2.0956522 ## ## Coefficients of linear discriminants: ## LD1 LD2 ## Sepal.Length 0.7100013 -0.8446128 ## Sepal.Width 1.2435532 2.4773120 ## Petal.Length -2.3419418 -0.4065865 ## Petal.Width -1.8502355 2.3234441 ## ## Proportion of trace: ## LD1 LD2 ## 0.9908 0.0092 Here we see the estimated \\(\\hat{\\pi}_k\\) and \\(\\hat{\\mu}_k\\) for each class. is.list(predict(iris_lda, iris_train)) ## [1] TRUE names(predict(iris_lda, iris_train)) ## [1] &quot;class&quot; &quot;posterior&quot; &quot;x&quot; head(predict(iris_lda, iris_train)$class, n = 10) ## [1] setosa virginica setosa setosa virginica setosa ## [7] virginica setosa versicolor setosa ## Levels: setosa versicolor virginica head(predict(iris_lda, iris_train)$posterior, n = 10) ## setosa versicolor virginica ## 23 1.000000e+00 1.517145e-21 1.717663e-41 ## 106 2.894733e-43 1.643603e-06 9.999984e-01 ## 37 1.000000e+00 2.169066e-20 1.287216e-40 ## 40 1.000000e+00 3.979954e-17 8.243133e-36 ## 145 1.303566e-37 4.335258e-06 9.999957e-01 ## 36 1.000000e+00 1.947567e-18 5.996917e-38 ## 119 2.220147e-51 9.587514e-09 1.000000e+00 ## 16 1.000000e+00 5.981936e-23 1.344538e-42 ## 94 1.599359e-11 9.999999e-01 1.035129e-07 ## 27 1.000000e+00 8.154612e-15 4.862249e-32 As we should come to expect, the predict() function operates in a new way when called on an lda object. By default, it returns an entire list. Within that list class stores the classifications and posterior contains the estimated probability for each class. iris_lda_train_pred = predict(iris_lda, iris_train)$class iris_lda_test_pred = predict(iris_lda, iris_test)$class We store the predictions made on the train and test sets. accuracy = function(actual, predicted) { mean(actual == predicted) } accuracy(predicted = iris_lda_train_pred, actual = iris_train$Species) ## [1] 0.96 accuracy(predicted = iris_lda_test_pred, actual = iris_test$Species) ## [1] 0.9866667 As expected, LDA performs well on both the train and test data. table(predicted = iris_lda_test_pred, actual = iris_test$Species) ## actual ## predicted setosa versicolor virginica ## setosa 22 0 0 ## versicolor 0 26 1 ## virginica 0 0 26 Looking at the test set, we see that we are perfectly predicting both setosa and versicolor. The only error is labeling a virginica as a versicolor. iris_lda_flat = lda(Species ~ ., data = iris_train, prior = c(1, 1, 1) / 3) iris_lda_flat ## Call: ## lda(Species ~ ., data = iris_train, prior = c(1, 1, 1)/3) ## ## Prior probabilities of groups: ## setosa versicolor virginica ## 0.3333333 0.3333333 0.3333333 ## ## Group means: ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## setosa 4.978571 3.378571 1.432143 0.2607143 ## versicolor 5.995833 2.808333 4.254167 1.3333333 ## virginica 6.669565 3.065217 5.717391 2.0956522 ## ## Coefficients of linear discriminants: ## LD1 LD2 ## Sepal.Length 0.7136357 -0.8415442 ## Sepal.Width 1.2328623 2.4826497 ## Petal.Length -2.3401674 -0.4166784 ## Petal.Width -1.8602343 2.3154465 ## ## Proportion of trace: ## LD1 LD2 ## 0.9901 0.0099 Instead of learning (estimating) the proportion of the three species from the data, we could instead specify them ourselves. Here we choose a uniform distributions over the possible species. We would call this a “flat” prior. iris_lda_flat_train_pred = predict(iris_lda_flat, iris_train)$class iris_lda_flat_test_pred = predict(iris_lda_flat, iris_test)$class accuracy(predicted = iris_lda_flat_train_pred, actual = iris_train$Species) ## [1] 0.96 accuracy(predicted = iris_lda_flat_test_pred, actual = iris_test$Species) ## [1] 1 This actually gives a better test accuracy! 8.2 Quadratic Discriminant Analysis QDA also assumes that the predictors are multivariate normal conditioned on the classes. \\[ {\\mathbf X} \\mid Y = k \\sim N(\\mu_k, \\Sigma_k) \\] \\[ f_k({\\mathbf x}) = \\frac{1}{(2\\pi)^{p/2}|\\Sigma_k|^{1/2}}\\exp\\left[-\\frac{1}{2}(\\mathbf x - \\mu_k)^{\\prime}\\Sigma_{k}^{-1}(\\mathbf x - \\mu_k)\\right] \\] Notice that now \\(\\Sigma_k\\) does depend on \\(k\\), that is, we are allowing a different \\(\\Sigma_k\\) for each class. We only use information from class \\(k\\) to estimate \\(\\Sigma_k\\). iris_qda = qda(Species ~ ., data = iris_train) iris_qda ## Call: ## qda(Species ~ ., data = iris_train) ## ## Prior probabilities of groups: ## setosa versicolor virginica ## 0.3733333 0.3200000 0.3066667 ## ## Group means: ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## setosa 4.978571 3.378571 1.432143 0.2607143 ## versicolor 5.995833 2.808333 4.254167 1.3333333 ## virginica 6.669565 3.065217 5.717391 2.0956522 Here the output is similar to LDA, again giving the estimated \\(\\hat{\\pi}_k\\) and \\(\\hat{\\mu}_k\\) for each class. Like lda(), the qda() function is found in the MASS package. Consider trying to fit QDA again, but this time with a smaller training set. (Use the commented line above to obtain a smaller test set.) This will cause an error because there are not enough observations within each class to estimate the large number of parameters in the \\(\\Sigma_k\\) matrices. This is less of a problem with LDA, since all observations, no matter the class, are being use to estimate the shared \\(\\Sigma\\) matrix. iris_qda_train_pred = predict(iris_qda, iris_train)$class iris_qda_test_pred = predict(iris_qda, iris_test)$class The predict() function operates the same as the predict() function for LDA. accuracy(predicted = iris_qda_train_pred, actual = iris_train$Species) ## [1] 0.9866667 accuracy(predicted = iris_qda_test_pred, actual = iris_test$Species) ## [1] 0.96 table(predicted = iris_qda_test_pred, actual = iris_test$Species) ## actual ## predicted setosa versicolor virginica ## setosa 22 0 0 ## versicolor 0 23 0 ## virginica 0 3 27 Here we find that QDA is not performing as well as LDA. It is misclassifying versicolors. Since QDA is a more complex model than LDA (many more parameters) we would say that QDA is overfitting here. Also note that, QDA creates quadratic decision boundaries, while LDA creates linear decision boundaries. We could also add quadratic terms to LDA to allow it to create quadratic decision boundaries. 8.3 Naive Bayes Naive Bayes comes in many forms. With only numeric predictors, it often assumes a multivariate normal conditioned on the classes, but a very specific multivariate normal. \\[ {\\mathbf X} \\mid Y = k \\sim N(\\mu_k, \\Sigma_k) \\] Naive Bayes assumes that the predictors \\(X_1, X_2, \\ldots, X_p\\) are independent. This is the “naive” part of naive Bayes. The Bayes part is nothing new. Since \\(X_1, X_2, \\ldots, X_p\\) are assumed independent, each \\(\\Sigma_k\\) is diagonal, that is, we assume no correlation between predictors. Independence implies zero correlation. This will allow us to write the (joint) likelihood as a product of univariate distributions. In this case, the product of univariate normal distributions instead of a (joint) multivariate distribution. \\[ f_k({\\mathbf x}) = \\prod_{j = 1}^{j = p} f_{kj}(x_j) \\] Here, \\(f_{kj}(x_j)\\) is the density for the \\(j\\)-th predictor conditioned on the \\(k\\)-th class. Notice that there is a \\(\\sigma_{kj}\\) for each predictor for each class. \\[ f_{kj}(x_j) = \\frac{1}{\\sigma_{kj}\\sqrt{2\\pi}}\\exp\\left[-\\frac{1}{2}\\left(\\frac{x_j - \\mu_{kj}}{\\sigma_{kj}}\\right)^2\\right] \\] When \\(p = 1\\), this version of naive Bayes is equivalent to QDA. library(e1071) iris_nb = naiveBayes(Species ~ ., data = iris_train) iris_nb ## ## Naive Bayes Classifier for Discrete Predictors ## ## Call: ## naiveBayes.default(x = X, y = Y, laplace = laplace) ## ## A-priori probabilities: ## Y ## setosa versicolor virginica ## 0.3733333 0.3200000 0.3066667 ## ## Conditional probabilities: ## Sepal.Length ## Y [,1] [,2] ## setosa 4.978571 0.3774742 ## versicolor 5.995833 0.5812125 ## virginica 6.669565 0.6392003 ## ## Sepal.Width ## Y [,1] [,2] ## setosa 3.378571 0.4349177 ## versicolor 2.808333 0.3269313 ## virginica 3.065217 0.3600615 ## ## Petal.Length ## Y [,1] [,2] ## setosa 1.432143 0.1743848 ## versicolor 4.254167 0.5166608 ## virginica 5.717391 0.5540366 ## ## Petal.Width ## Y [,1] [,2] ## setosa 0.2607143 0.1133310 ## versicolor 1.3333333 0.2334368 ## virginica 2.0956522 0.3022315 Many packages implement naive Bayes. Here we choose to use naiveBayes() from the package e1071. (The name of this package has an interesting history. Based on the name you wouldn’t know it, but the package contains many functions related to machine learning.) The Conditional probabilities: portion of the output gives the mean and standard deviation of the normal distribution for each predictor in each class. Notice how these mean estimates match those for LDA and QDA above. Note that naiveBayes() will work without a factor response, but functions much better with one. (Especially when making predictions.) If you are using a 0 and 1 response, you might consider coercing to a factor first. head(predict(iris_nb, iris_train)) ## [1] setosa virginica setosa setosa virginica setosa ## Levels: setosa versicolor virginica head(predict(iris_nb, iris_train, type = &quot;class&quot;)) ## [1] setosa virginica setosa setosa virginica setosa ## Levels: setosa versicolor virginica head(predict(iris_nb, iris_train, type = &quot;raw&quot;)) ## setosa versicolor virginica ## [1,] 1.000000e+00 3.134201e-16 2.948226e-27 ## [2,] 4.400050e-257 5.188308e-08 9.999999e-01 ## [3,] 1.000000e+00 2.263278e-14 1.168760e-24 ## [4,] 1.000000e+00 4.855740e-14 2.167253e-24 ## [5,] 1.897732e-218 6.189883e-08 9.999999e-01 ## [6,] 1.000000e+00 8.184097e-15 6.816322e-26 Oh look, predict() has another new mode of operation. If only there were a way to unify the predict() function across all of these methods… iris_nb_train_pred = predict(iris_nb, iris_train) iris_nb_test_pred = predict(iris_nb, iris_test) accuracy(predicted = iris_nb_train_pred, actual = iris_train$Species) ## [1] 0.9466667 accuracy(predicted = iris_nb_test_pred, actual = iris_test$Species) ## [1] 0.9466667 table(predicted = iris_nb_test_pred, actual = iris_test$Species) ## actual ## predicted setosa versicolor virginica ## setosa 22 0 0 ## versicolor 0 26 4 ## virginica 0 0 23 Like LDA, naive Bayes is having trouble with virginica. Method Train Accuracy Test Accuracy LDA 0.9600000 0.9866667 LDA, Flat Prior 0.9600000 1.0000000 QDA 0.9866667 0.9600000 Naive Bayes 0.9466667 0.9466667 Summarizing the results, we see that Naive Bayes is the worst of LDA, QDA, and NB for this data. So why should we care about naive Bayes? The strength of naive Bayes comes from its ability to handle a large number of predictors, \\(p\\), even with a limited sample size \\(n\\). Even with the naive independence assumption, naive Bayes works rather well in practice. Also because of this assumption, we can often train naive Bayes where LDA and QDA may be impossible to train because of the large number of parameters relative to the number of observations. Here naive Bayes doesn’t get a chance to show its strength since LDA and QDA already perform well, and the number of predictors is low. The choice between LDA and QDA is mostly down to a consideration about the amount of complexity needed. 8.4 Discrete Inputs So far, we have assumed that all predictors are numeric. What happens with categorical predictors? iris_train_mod = iris_train iris_train_mod$Sepal.Width = ifelse(iris_train$Sepal.Width &gt; 3, ifelse(iris_train$Sepal.Width &gt; 4, &quot;Large&quot;, &quot;Medium&quot;), &quot;Small&quot;) unique(iris_train_mod$Sepal.Width) ## [1] &quot;Medium&quot; &quot;Small&quot; &quot;Large&quot; Here we make a new dataset where Sepal.Width is categorical, with levels Small, Medium, and Large. We then try to train classifiers using only the sepal variables. naiveBayes(Species ~ Sepal.Length + Sepal.Width, data = iris_train_mod) ## ## Naive Bayes Classifier for Discrete Predictors ## ## Call: ## naiveBayes.default(x = X, y = Y, laplace = laplace) ## ## A-priori probabilities: ## Y ## setosa versicolor virginica ## 0.3733333 0.3200000 0.3066667 ## ## Conditional probabilities: ## Sepal.Length ## Y [,1] [,2] ## setosa 4.978571 0.3774742 ## versicolor 5.995833 0.5812125 ## virginica 6.669565 0.6392003 ## ## Sepal.Width ## Y Large Medium Small ## setosa 0.07142857 0.67857143 0.25000000 ## versicolor 0.00000000 0.25000000 0.75000000 ## virginica 0.00000000 0.43478261 0.56521739 Naive Bayes makes a somewhat obvious and intelligent choice to model the categorical variable as a multinomial. It then estimates the probability parameters of a multinomial distribution. lda(Species ~ Sepal.Length + Sepal.Width, data = iris_train_mod) ## Call: ## lda(Species ~ Sepal.Length + Sepal.Width, data = iris_train_mod) ## ## Prior probabilities of groups: ## setosa versicolor virginica ## 0.3733333 0.3200000 0.3066667 ## ## Group means: ## Sepal.Length Sepal.WidthMedium Sepal.WidthSmall ## setosa 4.978571 0.6785714 0.2500000 ## versicolor 5.995833 0.2500000 0.7500000 ## virginica 6.669565 0.4347826 0.5652174 ## ## Coefficients of linear discriminants: ## LD1 LD2 ## Sepal.Length 2.051602 0.4768608 ## Sepal.WidthMedium 1.728698 -0.4433340 ## Sepal.WidthSmall 3.173903 -2.2804034 ## ## Proportion of trace: ## LD1 LD2 ## 0.9764 0.0236 LDA however creates dummy variables, here with Large is the reference level, then continues to model them as normally distributed. Not great, but better then not using a categorical variable. 8.5 RMarkdown The RMarkdown file for this chapter can be found here. The file was created using R version 3.4.1 and the following packages: Base Packages, Attached ## [1] &quot;stats&quot; &quot;graphics&quot; &quot;grDevices&quot; &quot;utils&quot; &quot;datasets&quot; &quot;base&quot; Additional Packages, Attached ## [1] &quot;e1071&quot; &quot;MASS&quot; Additional Packages, Not Attached ## [1] &quot;Rcpp&quot; &quot;highr&quot; &quot;compiler&quot; &quot;nloptr&quot; ## [5] &quot;plyr&quot; &quot;class&quot; &quot;methods&quot; &quot;iterators&quot; ## [9] &quot;tools&quot; &quot;digest&quot; &quot;lme4&quot; &quot;evaluate&quot; ## [13] &quot;tibble&quot; &quot;gtable&quot; &quot;nlme&quot; &quot;lattice&quot; ## [17] &quot;mgcv&quot; &quot;rlang&quot; &quot;Matrix&quot; &quot;foreach&quot; ## [21] &quot;parallel&quot; &quot;yaml&quot; &quot;SparseM&quot; &quot;stringr&quot; ## [25] &quot;knitr&quot; &quot;MatrixModels&quot; &quot;stats4&quot; &quot;rprojroot&quot; ## [29] &quot;grid&quot; &quot;caret&quot; &quot;nnet&quot; &quot;ellipse&quot; ## [33] &quot;rmarkdown&quot; &quot;bookdown&quot; &quot;minqa&quot; &quot;ggplot2&quot; ## [37] &quot;reshape2&quot; &quot;car&quot; &quot;magrittr&quot; &quot;backports&quot; ## [41] &quot;scales&quot; &quot;codetools&quot; &quot;ModelMetrics&quot; &quot;htmltools&quot; ## [45] &quot;splines&quot; &quot;pbkrtest&quot; &quot;colorspace&quot; &quot;quantreg&quot; ## [49] &quot;stringi&quot; &quot;lazyeval&quot; &quot;munsell&quot; "],
["k-nearest-neighbors.html", "Chapter 9 k-Nearest Neighbors 9.1 Classification 9.2 Regression 9.3 External Links 9.4 RMarkdown", " Chapter 9 k-Nearest Neighbors In this chapter we introduce our first non-parametric method, \\(k\\)-nearest neighbors, which can be used for both classification and regression. Each method we have seen so far has been parametric. For example, logistic regression had the form \\[ \\log\\left(\\frac{p({\\bf x})}{1 - p({\\bf x})}\\right) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p. \\] In this case, the \\(\\beta_i\\) are the parameters of the model, which we learned (estimated) by training (fitting) the model. \\(k\\)-nearest neighbors has no such parameters. Instead, it has a tuning parameter, \\(k\\). This is a parameter which determines how the model is trained, instead of a parameter that is learned through training. Note that tuning parameters are not used exclusively used with non-parametric methods. Later we will see examples of tuning parameters for parametric methods. 9.1 Classification library(ISLR) library(class) library(MASS) We first load some necessary libraries. We’ll begin discussing classification by returning to the Default data from the ISLR package. To illustrate regression, we’ll also return to the Boston data from the MASS package. To perform \\(k\\)-nearest neighbors, we will use the knn() function from the class package. 9.1.1 Default Data Unlike many of our previous methods, knn() requires that all predictors be numeric, so we coerce student to be a 0 and 1 variable instead of a factor. (We can leave the response as a factor.) set.seed(42) Default$student = as.numeric(Default$student) - 1 default_index = sample(nrow(Default), 5000) default_train = Default[default_index, ] default_test = Default[-default_index, ] Also unlike previous methods, knn() does not utilize the formula syntax, rather, requires the predictors be their own data frame or matrix, and the class labels be a separate factor variable. Note that the \\(y\\) data should be a factor vector, not a data frame containing a factor vector. # training data X_default_train = default_train[, -1] y_default_train = default_train$default # testing data X_default_test = default_test[, -1] y_default_test = default_test$default There is very little “training” with \\(k\\)-nearest neighbors. Essentially the only training is to simply remember the inputs. Because of this, we say that \\(k\\)-nearest neighbors is fast at training time. However, at test time, \\(k\\)-nearest neighbors is very slow. For each test case, the method must find the \\(k\\)-nearest neighbors, which is not computationally cheap. (Note that knn() uses Euclidean distance.) head(knn(train = X_default_train, test = X_default_test, cl = y_default_train, k = 3), n = 25) ## [1] No No No No No No No No No No No No No No No No No No No No No No No ## [24] No No ## Levels: No Yes Because of the lack of any need for training, the knn() function essentially replaces the predict() function, and immediately returns classifications. Here, knn() used four arguments: train, the predictors for the train set. test, the predictors for the test set. knn() will output results for these cases. cl, the true class labels for the train set. k, the number of neighbors to consider. accuracy = function(actual, predicted) { mean(actual == predicted) } We’ll use our usual accuracy() function to asses how well knn() works with this data. accuracy(actual = y_default_test, predicted = knn(train = X_default_train, test = X_default_test, cl = y_default_train, k = 5)) ## [1] 0.9684 Often with knn() we need to consider the scale of the predictors variables. If one variable is contains much larger numbers because of the units or range of the variable, it will dominate other variables in the distance measurements. But this doesn’t necessarily mean that it should be such an important variable. It is common practice to scale the predictors to have 0 mean and unit variance. Be sure to apply the scaling to both the train and test data. accuracy(actual = y_default_test, predicted = knn(train = scale(X_default_train), test = scale(X_default_test), cl = y_default_train, k = 5)) ## [1] 0.9722 Here we see the scaling improves the classification accuracy. This may not always be the case, and often, it is normal to attempt classification with and without scaling. How do we choose \\(k\\)? Try different values and see which works best. set.seed(42) k_to_try = 1:100 acc_k = rep(x = 0, times = length(k_to_try)) for(i in seq_along(k_to_try)) { pred = knn(train = scale(X_default_train), test = scale(X_default_test), cl = y_default_train, k = k_to_try[i]) acc_k[i] = accuracy(y_default_test, pred) } The seq_along() function can be very useful for looping over a vector that stores non-consecutive numbers. It often removes the need for an additional counter variable. We actually didn’t need it in the above knn() example, but it is still a good habit. Here we see an example where we would have otherwise needed another variable. ex_seq = seq(from = 1, to = 100, by = 5) seq_along(ex_seq) ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ex_storage = rep(x = 0, times = length(ex_seq)) for(i in seq_along(ex_seq)) { ex_storage[i] = mean(rnorm(n = 10, mean = ex_seq[i], sd = 1)) } ex_storage ## [1] 0.948629 5.792671 11.090760 15.915397 21.422372 26.106009 30.857772 ## [8] 35.593119 40.958334 46.338667 50.672116 55.733392 60.387860 65.747387 ## [15] 71.037306 76.066974 80.956349 85.173316 91.077993 95.882329 Naturally, we plot the \\(k\\)-nearest neighbor results. # plot accuracy vs choice of k plot(acc_k, type = &quot;b&quot;, col = &quot;dodgerblue&quot;, cex = 1, pch = 20, xlab = &quot;k, number of neighbors&quot;, ylab = &quot;classification accuracy&quot;, main = &quot;Accuracy vs Neighbors&quot;) # add lines indicating k with best accuracy abline(v = which(acc_k == max(acc_k)), col = &quot;darkorange&quot;, lwd = 1.5) # add line for max accuracy seen abline(h = max(acc_k), col = &quot;grey&quot;, lty = 2) # add line for prevalence in test set abline(h = mean(y_default_test == &quot;No&quot;), col = &quot;grey&quot;, lty = 2) max(acc_k) ## [1] 0.9746 max(which(acc_k == max(acc_k))) ## [1] 22 We see that four different values of \\(k\\) are tied for the highest accuracy. Given a choice of these four values of \\(k\\), we select the largest, as it is the least variable, and has the least chance of overfitting. Also notice that, as \\(k\\) increases, eventually the accuracy approaches the test prevalence. mean(y_default_test == &quot;No&quot;) ## [1] 0.967 9.1.2 Iris Data Like LDA and QDA, KNN can be used for both binary and multi-class problems. As an example, we return to the iris data. set.seed(430) iris_obs = nrow(iris) iris_index = sample(iris_obs, size = trunc(0.50 * iris_obs)) iris_train = iris[iris_index, ] iris_test = iris[-iris_index, ] All the predictors here are numeric, so we proceed to splitting the data into predictors and classes. # training data X_iris_train = iris_train[, -5] y_iris_train = iris_train$Species # testing data X_iris_test = iris_test[, -5] y_iris_test = iris_test$Species Like previous methods, we can obtain predicted probabilities given test predictors. To do so, we add an argument, prob = TRUE iris_pred = knn(train = scale(X_iris_train), test = scale(X_iris_test), cl = y_iris_train, k = 10, prob = TRUE) iris_pred ## [1] setosa setosa setosa setosa setosa setosa ## [7] setosa setosa setosa setosa setosa setosa ## [13] setosa setosa setosa setosa setosa setosa ## [19] setosa setosa setosa setosa versicolor versicolor ## [25] versicolor versicolor versicolor versicolor versicolor versicolor ## [31] versicolor versicolor versicolor versicolor versicolor versicolor ## [37] versicolor versicolor versicolor versicolor versicolor versicolor ## [43] versicolor versicolor versicolor versicolor versicolor versicolor ## [49] virginica versicolor virginica virginica virginica virginica ## [55] virginica virginica virginica versicolor versicolor virginica ## [61] virginica virginica virginica versicolor virginica virginica ## [67] virginica virginica virginica versicolor virginica virginica ## [73] virginica virginica versicolor ## attr(,&quot;prob&quot;) ## [1] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 ## [8] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 ## [15] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 ## [22] 1.0000000 0.9000000 1.0000000 0.8000000 1.0000000 0.9000000 0.9000000 ## [29] 0.9000000 0.8000000 1.0000000 0.9000000 1.0000000 0.8000000 0.5000000 ## [36] 0.8000000 0.9000000 0.8000000 1.0000000 1.0000000 0.7272727 0.9000000 ## [43] 0.8000000 0.9000000 1.0000000 1.0000000 0.9000000 0.9000000 0.9000000 ## [50] 0.7000000 0.8000000 0.7272727 0.8000000 0.8000000 0.8000000 0.9000000 ## [57] 0.6000000 0.6000000 0.5000000 0.9000000 0.6000000 1.0000000 0.6000000 ## [64] 0.5000000 0.7000000 0.9000000 1.0000000 0.9000000 0.6000000 0.7000000 ## [71] 0.8000000 0.9000000 0.8000000 0.9000000 0.5000000 ## Levels: setosa versicolor virginica Unfortunately, this only returns the predicted probability of the most common class. In the binary case, this would be sufficient, however, for multi-class problems, we cannot recover each of the probabilities of interest. attributes(iris_pred)$prob ## [1] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 ## [8] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 ## [15] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 ## [22] 1.0000000 0.9000000 1.0000000 0.8000000 1.0000000 0.9000000 0.9000000 ## [29] 0.9000000 0.8000000 1.0000000 0.9000000 1.0000000 0.8000000 0.5000000 ## [36] 0.8000000 0.9000000 0.8000000 1.0000000 1.0000000 0.7272727 0.9000000 ## [43] 0.8000000 0.9000000 1.0000000 1.0000000 0.9000000 0.9000000 0.9000000 ## [50] 0.7000000 0.8000000 0.7272727 0.8000000 0.8000000 0.8000000 0.9000000 ## [57] 0.6000000 0.6000000 0.5000000 0.9000000 0.6000000 1.0000000 0.6000000 ## [64] 0.5000000 0.7000000 0.9000000 1.0000000 0.9000000 0.6000000 0.7000000 ## [71] 0.8000000 0.9000000 0.8000000 0.9000000 0.5000000 9.2 Regression We quickly illustrate KNN for regression using the Boston data. We’ll only use lstat as a predictor, and medv as the response. We won’t test-train split for this example since won’t be checking RMSE, but instead plotting fitted models. There is also no need to worry about scaling since there is only one predictor. X_boston = Boston[&quot;lstat&quot;] y_boston = Boston$medv We create a “test” set, that is a grid of lstat values at which we will predict medv. lstat_grid = data.frame(lstat = seq(range(X_boston$lstat)[1], range(X_boston$lstat)[2], by = 0.01)) Unfortunately, knn() from class only handles classification. To perform regression, we will need knn.reg() from the FNN package. Notice that, we do not load this package, but instead use FNN::knn.reg to access the function. This is useful since FNN also contains a function knn() and would then mask knn() from class. pred_001 = FNN::knn.reg(train = X_boston, test = lstat_grid, y = y_boston, k = 1) pred_005 = FNN::knn.reg(train = X_boston, test = lstat_grid, y = y_boston, k = 5) pred_010 = FNN::knn.reg(train = X_boston, test = lstat_grid, y = y_boston, k = 10) pred_050 = FNN::knn.reg(train = X_boston, test = lstat_grid, y = y_boston, k = 50) pred_100 = FNN::knn.reg(train = X_boston, test = lstat_grid, y = y_boston, k = 100) pred_506 = FNN::knn.reg(train = X_boston, test = lstat_grid, y = y_boston, k = 506) We make predictions for various values of k. Note that 506 is the number of observations in this dataset. par(mfrow = c(3, 2)) plot(medv ~ lstat, data = Boston, cex = .8, col = &quot;dodgerblue&quot;, main = &quot;k = 1&quot;) lines(lstat_grid$lstat, pred_001$pred, col = &quot;darkorange&quot;, lwd = 0.25) plot(medv ~ lstat, data = Boston, cex = .8, col = &quot;dodgerblue&quot;, main = &quot;k = 5&quot;) lines(lstat_grid$lstat, pred_005$pred, col = &quot;darkorange&quot;, lwd = 0.75) plot(medv ~ lstat, data = Boston, cex = .8, col = &quot;dodgerblue&quot;, main = &quot;k = 10&quot;) lines(lstat_grid$lstat, pred_010$pred, col = &quot;darkorange&quot;, lwd = 1) plot(medv ~ lstat, data = Boston, cex = .8, col = &quot;dodgerblue&quot;, main = &quot;k = 25&quot;) lines(lstat_grid$lstat, pred_050$pred, col = &quot;darkorange&quot;, lwd = 1.5) plot(medv ~ lstat, data = Boston, cex = .8, col = &quot;dodgerblue&quot;, main = &quot;k = 50&quot;) lines(lstat_grid$lstat, pred_100$pred, col = &quot;darkorange&quot;, lwd = 2) plot(medv ~ lstat, data = Boston, cex = .8, col = &quot;dodgerblue&quot;, main = &quot;k = 506&quot;) lines(lstat_grid$lstat, pred_506$pred, col = &quot;darkorange&quot;, lwd = 2) We see that k = 1 is clearly overfitting, as k = 1 is a very complex, highly variable model. Conversely, k = 506 is clearly underfitting the data, as k = 506 is a very simple, low variance model. In fact, here it is predicting a simple average of all the data at each point. 9.3 External Links YouTube: \\(k\\)-Nearest Neighbor Classification Algorithm - Video from user “mathematicalmonk” which gives a brief but thorough introduction to the method. 9.4 RMarkdown The RMarkdown file for this chapter can be found here. The file was created using R version 3.4.1 and the following packages: Base Packages, Attached ## [1] &quot;stats&quot; &quot;graphics&quot; &quot;grDevices&quot; &quot;utils&quot; &quot;datasets&quot; &quot;base&quot; Additional Packages, Attached ## [1] &quot;MASS&quot; &quot;class&quot; &quot;ISLR&quot; Additional Packages, Not Attached ## [1] &quot;Rcpp&quot; &quot;bookdown&quot; &quot;FNN&quot; &quot;digest&quot; &quot;rprojroot&quot; ## [6] &quot;backports&quot; &quot;magrittr&quot; &quot;evaluate&quot; &quot;stringi&quot; &quot;rmarkdown&quot; ## [11] &quot;tools&quot; &quot;stringr&quot; &quot;yaml&quot; &quot;compiler&quot; &quot;htmltools&quot; ## [16] &quot;knitr&quot; &quot;methods&quot; "],
["resampling.html", "Chapter 10 Resampling 10.1 Test-Train Split 10.2 Cross-Validation 10.3 Bootstrap 10.4 External Links 10.5 RMarkdown", " Chapter 10 Resampling In this chapter we introduce resampling methods including cross-validation and the bootstrap. library(ISLR) Here, we will use the Auto data from ISLR and attempt to predict mpg (a numeric variable) from horsepower. ## # A tibble: 392 x 9 ## mpg cylinders displacement horsepower weight acceleration year ## * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 18 8 307 130 3504 12.0 70 ## 2 15 8 350 165 3693 11.5 70 ## 3 18 8 318 150 3436 11.0 70 ## 4 16 8 304 150 3433 12.0 70 ## 5 17 8 302 140 3449 10.5 70 ## 6 15 8 429 198 4341 10.0 70 ## 7 14 8 454 220 4354 9.0 70 ## 8 14 8 440 215 4312 8.5 70 ## 9 14 8 455 225 4425 10.0 70 ## 10 15 8 390 190 3850 8.5 70 ## # ... with 382 more rows, and 2 more variables: origin &lt;dbl&gt;, name &lt;fctr&gt; 10.1 Test-Train Split First, let’s return to the usual test-train split procedure that we have used so far. Let’s evaluate what happens if we repeat the process a large number of times, each time storing the test RMSE. We’ll consider three models: An underfitting model: mpg ~ horsepower A reasonable model: mpg ~ poly(horsepower, 2) A ridiculous, overfitting model: mpg ~ poly(horsepower, 8) set.seed(42) num_reps = 100 lin_rmse = rep(0, times = num_reps) quad_rmse = rep(0, times = num_reps) huge_rmse = rep(0, times = num_reps) for(i in 1:100) { train_idx = sample(392, size = 196) lin_fit = lm(mpg ~ horsepower, data = Auto, subset = train_idx) lin_rmse[i] = sqrt(mean((Auto$mpg - predict(lin_fit, Auto))[-train_idx] ^ 2)) quad_fit = lm(mpg ~ poly(horsepower, 2), data = Auto, subset = train_idx) quad_rmse[i] = sqrt(mean((Auto$mpg - predict(quad_fit, Auto))[-train_idx] ^ 2)) huge_fit = lm(mpg ~ poly(horsepower, 8), data = Auto, subset = train_idx) huge_rmse[i] = sqrt(mean((Auto$mpg - predict(huge_fit, Auto))[-train_idx] ^ 2)) } Notice two things, first that the “Reasonable” model has on average the smallest error. Second, notice large variability in the RMSE. We see this in the “Reasonable” model, but it is very clear in the “Ridiculous” model. Here it is very clear that if we use an “unlucky” split, our test error will be much larger than the likely reality. 10.2 Cross-Validation Instead of using a single test-train split, we instead look to use cross-validation. There are many ways to perform cross-validation R, depending on the method of interest. 10.2.1 Method Specific Some method, for example glm() through cv.glm() and knn() through knn.cv() have cross-validation capabilities built-in. We’ll use glm() for illustration. First we need to convince ourselves that glm() can be used to perform the same tasks as lm(). glm_fit = glm(mpg ~ horsepower, data = Auto) coef(glm_fit) ## (Intercept) horsepower ## 39.9358610 -0.1578447 lm_fit = lm(mpg ~ horsepower, data = Auto) coef(lm_fit) ## (Intercept) horsepower ## 39.9358610 -0.1578447 By default, cv.glm() will report leave-one-out cross-validation (LOOCV). library(boot) glm_fit = glm(mpg ~ horsepower, data = Auto) loocv_rmse = sqrt(cv.glm(Auto, glm_fit)$delta) loocv_rmse ## [1] 4.922552 4.922514 loocv_rmse[1] ## [1] 4.922552 We are actually given two values. The first is exactly the LOOCV-RMSE. The second is a minor correct that we will not worry about. We take a square root to obtain LOOCV-RMSE. loocv_rmse_poly = rep(0, times = 10) for (i in seq_along(loocv_rmse_poly)) { glm_fit = glm(mpg ~ poly(horsepower, i), data = Auto) loocv_rmse_poly[i] = sqrt(cv.glm(Auto, glm_fit)$delta[1]) } loocv_rmse_poly ## [1] 4.922552 4.387279 4.397156 4.407316 4.362707 4.356449 4.339706 ## [8] 4.354440 4.366764 4.414854 plot(loocv_rmse_poly, type = &quot;b&quot;, col = &quot;dodgerblue&quot;, main = &quot;LOOCV-RMSE vs Polynomial Degree&quot;, ylab = &quot;LOOCV-RMSE&quot;, xlab = &quot;Polynomial Degree&quot;) If you run the above code locally, you will notice that is painfully slow. We are fitting each of the 10 models 392 times, that is, each model \\(n\\) times, once with each data point left out. (Note: in this case, for a linear model, there is actually a shortcut formula which would allow us to obtain LOOCV-RMSE from a single fit to the data. See details in ISL as well as a link below.) We could instead use \\(k\\)-fold cross-validation. set.seed(17) cv_10_rmse_poly = rep(0, times = 10) for (i in seq_along(cv_10_rmse_poly)){ glm_fit = glm(mpg ~ poly(horsepower, i), data = Auto) cv_10_rmse_poly[i] = sqrt(cv.glm(Auto, glm_fit, K = 10)$delta[1]) } cv_10_rmse_poly ## [1] 4.919878 4.380552 4.393929 4.397498 4.345010 4.361311 4.346963 ## [8] 4.439821 4.353321 4.416102 plot(cv_10_rmse_poly, type = &quot;b&quot;, col = &quot;dodgerblue&quot;, main = &quot;10 Fold CV-RMSE vs Polynomial Degree&quot;, ylab = &quot;10 Fold CV-RMSE&quot;, xlab = &quot;Polynomial Degree&quot;) Here we chose 10-fold cross-validation. Notice it is much faster. In practice, we usually stick to 5 or 10-fold CV. set.seed(42) num_reps = 100 lin_rmse_10_fold = rep(0, times = num_reps) quad_rmse_10_fold = rep(0, times = num_reps) huge_rmse_10_fold = rep(0, times = num_reps) for(i in 1:100) { lin_fit = glm(mpg ~ poly(horsepower, 1), data = Auto) quad_fit = glm(mpg ~ poly(horsepower, 2), data = Auto) huge_fit = glm(mpg ~ poly(horsepower, 8), data = Auto) lin_rmse_10_fold[i] = sqrt(cv.glm(Auto, lin_fit, K = 10)$delta[1]) quad_rmse_10_fold[i] = sqrt(cv.glm(Auto, quad_fit, K = 10)$delta[1]) huge_rmse_10_fold[i] = sqrt(cv.glm(Auto, huge_fit, K = 10)$delta[1]) } Repeating the test-train split analysis from above, this time with 10-fold CV, see that that the resulting RMSE are much less variable. That means, will cross-validation still has some inherent randomness, it has a much smaller effect on the results. 10.2.2 Manual Cross-Validation For methods that do not have a built-in ability to perform cross-validation, or for methods that have limited cross-validation capability, we will need to write our own code for cross-validation. (Spoiler: This is not true, but let’s pretend it is, so we can see how to perform cross-validation from scratch.) This essentially amounts to randomly splitting the data, then looping over the splits. The createFolds() function from the caret() package will make this much easier. caret::createFolds(Auto$mpg) ## $Fold01 ## [1] 17 25 44 56 58 59 62 68 69 82 96 98 108 140 145 151 157 ## [18] 160 163 174 181 190 194 200 214 216 240 242 278 282 288 321 323 330 ## [35] 353 374 375 376 383 ## ## $Fold02 ## [1] 21 22 33 46 47 64 70 81 85 95 121 130 134 148 156 158 161 ## [18] 169 171 176 217 221 250 263 270 277 279 283 289 291 297 316 346 358 ## [35] 371 377 380 384 386 392 ## ## $Fold03 ## [1] 12 15 23 29 31 40 48 73 79 80 86 91 93 113 137 144 146 ## [18] 177 183 188 199 201 203 206 208 225 231 243 247 251 265 267 273 296 ## [35] 307 324 340 357 373 ## ## $Fold04 ## [1] 3 18 34 42 50 51 52 54 71 76 87 88 103 106 107 164 170 ## [18] 182 198 205 211 212 213 219 226 274 281 292 298 319 320 327 328 332 ## [35] 337 360 364 381 385 ## ## $Fold05 ## [1] 8 26 28 32 55 60 61 75 83 92 94 97 100 118 123 133 154 ## [18] 159 172 184 209 220 222 236 237 241 244 253 272 310 322 335 341 349 ## [35] 352 366 367 379 382 ## ## $Fold06 ## [1] 7 13 30 43 63 101 102 109 114 135 153 155 168 178 180 191 192 ## [18] 227 228 234 246 252 256 257 262 268 276 285 286 308 309 312 313 314 ## [35] 318 326 331 334 363 ## ## $Fold07 ## [1] 1 6 53 57 65 78 84 89 90 105 115 126 143 149 165 175 185 ## [18] 207 224 229 230 233 258 260 264 271 290 293 300 305 343 344 345 348 ## [35] 359 369 372 388 389 391 ## ## $Fold08 ## [1] 2 4 11 16 41 45 49 66 77 104 110 119 120 122 124 136 138 ## [18] 162 166 167 173 195 196 202 210 215 235 254 275 301 302 317 329 338 ## [35] 342 350 362 368 378 ## ## $Fold09 ## [1] 10 14 19 27 67 72 99 111 112 116 117 127 129 139 141 147 186 ## [18] 187 193 204 218 223 238 248 249 261 294 295 303 306 315 325 336 339 ## [35] 347 351 356 365 ## ## $Fold10 ## [1] 5 9 20 24 35 36 37 38 39 74 125 128 131 132 142 150 152 ## [18] 179 189 197 232 239 245 255 259 266 269 280 284 287 299 304 311 333 ## [35] 354 355 361 370 387 390 Can you use this to verify the 10-fold CV results from above? 10.2.3 Test Data The following example illustrates the need for a test set which is never used in model training. If for no other reason, it gives us a quick sanity check that we have cross-validated correctly. To be specific we will test-train split the data, then perform cross-validation on the training data. accuracy = function(actual, predicted) { mean(actual == predicted) } # simulate data # y is 0/1 # X are independent N(0,1) variables # X has no relationship with the response # p &gt;&gt;&gt; n set.seed(430) n = 400 p = 5000 X = replicate(p, rnorm(n)) y = c(rep(0, times = n / 4), rep(1, times = n / 4), rep(0, times = n / 4), rep(1, times = n / 4)) # first n/2 observations are used for training # last n/2 observations used for testing # both are 50% 0s and 50% 1s # cv will be done inside train data full_data = data.frame(y, X) train = full_data[1:(n / 2), ] test = full_data[((n / 2) + 1):n, ] First, we use the screen-then-validate approach. # find correlation between y and each predictor variable correlations = apply(train[, -1], 2, cor, y = train$y) hist(correlations) # select the 25 largest (absolute) correlation # these should be &quot;useful&quot; for prediction selected = order(abs(correlations), decreasing = TRUE)[1:25] correlations[selected] ## X424 X4779 X2484 X1154 X2617 X1603 ## -0.2577389 0.2491598 0.2379113 -0.2373367 0.2336055 0.2327971 ## X2963 X1091 X2806 X4586 X2569 X4532 ## 0.2318932 -0.2281451 -0.2271382 0.2252979 0.2239974 -0.2225698 ## X3167 X741 X3329 X3862 X1741 X654 ## -0.2201853 -0.2188919 -0.2186248 -0.2174146 -0.2150666 0.2130732 ## X3786 X4617 X3296 X2295 X999 X4349 ## 0.2090650 -0.2086551 -0.2075271 -0.2072127 0.2055167 -0.1995252 ## X1409 ## 0.1977006 # subset the test and training data based on the selected predictors train_screen = train[c(1, selected)] test_screen = test[c(1, selected)] # fit an additive logistic regression # use 10-fold cross-validation to obtain an estimate of test accuracy # horribly optimistic library(boot) glm_fit = glm(y ~ ., data = train_screen, family = &quot;binomial&quot;) 1 - cv.glm(train_screen, glm_fit, K = 10)$delta[1] ## [1] 0.709234 # get test accuracy, which we expect to be 0.50 # no better than guessing glm_pred = (predict(glm_fit, newdata = test_screen, type = &quot;response&quot;) &gt; 0.5) * 1 accuracy(predicted = glm_pred, actual = test_screen$y) ## [1] 0.46 Now, we will correctly screen-while-validating. # use the caret package to obtain 10 &quot;folds&quot; folds = caret::createFolds(train_screen$y) # for each fold # - pre-screen variables on the 9 training folds # - fit model to these variables # - get accuracy on validation fold fold_acc = rep(0, length(folds)) for(i in seq_along(folds)) { # split for fold i train_fold = train[-folds[[i]],] validate_fold = train[folds[[i]],] # screening for fold i correlations = apply(train_fold[, -1], 2, cor, y = train_fold[,1]) selected = order(abs(correlations), decreasing = TRUE)[1:25] train_fold_screen = train_fold[ ,c(1,selected)] validate_fold_screen = validate_fold[ ,c(1,selected)] # accuracy for fold i glm_fit = glm(y ~ ., data = train_fold_screen, family = &quot;binomial&quot;) glm_pred = (predict(glm_fit, newdata = validate_fold_screen, type = &quot;response&quot;) &gt; 0.5)*1 fold_acc[i] = mean(glm_pred == validate_fold_screen$y) } # report all 10 validation fold accuracies fold_acc ## [1] 0.45 0.40 0.50 0.35 0.50 0.35 0.45 0.50 0.60 0.50 # properly cross-validated error # this roughly matches what we expect in the test set mean(fold_acc) ## [1] 0.46 10.3 Bootstrap ISL also discusses the bootstrap, which is another resampling method. However, it is less relevant to the statistical learning tasks we will encounter. It could be useful if we were to attempt to calculate the bias and variance of a prediction (estimate) without access to the data generating process. Return to the bias-variance tradeoff chapter and think about how the bootstrap could be used to obtain estimates of bias and variance with a single dataset, instead of repeated simulated datasets. For fun, write-up a simulation study which compares the strategy in the bias-variance tradeoff chapter to a strategy using bootstrap resampling of a single dataset. Submit it to be added to this chapter! 10.4 External Links YouTube: Cross-Validation, Part 1 - Video from user “mathematicalmonk” which introduces \\(K\\)-fold cross-validation in greater detail. YouTube: Cross-Validation, Part 2 - Continuation which discusses selection and resampling strategies. YouTube: Cross-Validation, Part 3 - Continuation which discusses choice of \\(K\\). Blog: Fast Computation of Cross-Validation in Linear Models - Details for using leverage to speed-up LOOCV for linear models. OTexts: Bootstrap - Some brief mathematical details of the bootstrap. 10.5 RMarkdown The RMarkdown file for this chapter can be found here. The file was created using R version 3.4.1 and the following packages: Base Packages, Attached ## [1] &quot;stats&quot; &quot;graphics&quot; &quot;grDevices&quot; &quot;utils&quot; &quot;datasets&quot; &quot;base&quot; Additional Packages, Attached ## [1] &quot;boot&quot; &quot;ISLR&quot; Additional Packages, Not Attached ## [1] &quot;Rcpp&quot; &quot;nloptr&quot; &quot;compiler&quot; &quot;plyr&quot; ## [5] &quot;methods&quot; &quot;iterators&quot; &quot;tools&quot; &quot;digest&quot; ## [9] &quot;lme4&quot; &quot;evaluate&quot; &quot;tibble&quot; &quot;gtable&quot; ## [13] &quot;nlme&quot; &quot;lattice&quot; &quot;mgcv&quot; &quot;rlang&quot; ## [17] &quot;Matrix&quot; &quot;foreach&quot; &quot;parallel&quot; &quot;yaml&quot; ## [21] &quot;SparseM&quot; &quot;stringr&quot; &quot;knitr&quot; &quot;MatrixModels&quot; ## [25] &quot;stats4&quot; &quot;rprojroot&quot; &quot;grid&quot; &quot;caret&quot; ## [29] &quot;nnet&quot; &quot;rmarkdown&quot; &quot;bookdown&quot; &quot;minqa&quot; ## [33] &quot;ggplot2&quot; &quot;reshape2&quot; &quot;car&quot; &quot;magrittr&quot; ## [37] &quot;backports&quot; &quot;scales&quot; &quot;codetools&quot; &quot;ModelMetrics&quot; ## [41] &quot;htmltools&quot; &quot;MASS&quot; &quot;splines&quot; &quot;pbkrtest&quot; ## [45] &quot;colorspace&quot; &quot;quantreg&quot; &quot;stringi&quot; &quot;lazyeval&quot; ## [49] &quot;munsell&quot; "],
["classification-overview.html", "Chapter 11 Classification Overview 11.1 External Links 11.2 RMarkdown", " Chapter 11 Classification Overview At this point, you should know… Bayes Classifier Classify to the class with the highest probability given a particular input \\(x\\). \\[ C^B({\\bf x}) = \\underset{k}{\\mathrm{argmax}} \\ P[Y = k \\mid {\\bf X = x}] \\] Since we rarely, if ever, know the true probabilities, use a classification method to estimate them using data. The Bias-Variance Tradeoff As model complexity increases, bias decreases. As model complexity increases, variance increases. As a result, there is a model somewhere in the middle with the best accuracy. (Or lowest RMSE for regression.) The Test-Train Split Never use test data to train a model. Test accuracy is a measure of how well a method works in general. We can identify underfitting and overfitting models relative to the best test accuracy. A less complex model than the model with the best test accuracy is underfitting. A more complex model than the model with the best test accuracy is overfitting. Classification Methods Logistic Regression Linear Discriminant Analysis (LDA) Quadratic Discriminant Analysis (QDA) Naive Bayes (NB) \\(k\\)-Nearest Neighbors (KNN) For each, we can: Obtain predicted probabilities. Make classifications. Find decision boundaries. (Seen only for some.) Discriminative versus Generative Methods Discriminative methods learn the conditional distribution \\(p(y \\mid x)\\), thus could only simulate \\(y\\) given a fixed \\(x\\). Generative methods learn the joint distribution \\(p(x, y)\\), thus could only simulate new data \\((x, y)\\). Parametric and Non-Parametric Methods Parametric methods models \\(P[Y = k \\mid X = x]\\) as a specific function of parameters which are learned through data. Non-Parametric use an algorithmic approach to estimate \\(P[Y = k \\mid X = x]\\) for each possible input \\(x\\). Tuning Parameters Specify how to train a model. This in contrast to model parameters, which are learned through training. Cross-Validation A method to estimate test metrics with training data. Repeats the train-validate split inside the training data. Curse of Dimensionality As feature space grows, that is as \\(p\\) grows, “neighborhoods” must become much larger to contain “neighbors,” thus local methods are not so local. No-Free-Lunch Theorem There is no one classifier that will be best across all datasets. 11.1 External Links Wikipedia: No-Free-Lunch Do we Need Hundreds of Classifiers to Solve Real World Classification Problems? - A paper that argues that No-Free-Lunch may be true in theory, but in practice there a only a few classifiers that outperform most others. 11.2 RMarkdown The RMarkdown file for this chapter can be found here. The file was created using R version 3.4.1. "],
["the-caret-package.html", "Chapter 12 The caret Package 12.1 External Links 12.2 RMarkdown", " Chapter 12 The caret Package Instructor’s Note: This chapter is currently missing the usual narrative text. Hopefully it will be added later. Now that we have seen a number of classification (and regression) methods, and introduced cross-validation, we see the general outline of a predictive analysis: Select a method Test-train split the available data Decide on a set of candidate models via tuning parameters Select the best model (tuning parameters) using a cross-validated metric Use chosen model to make predictions Calculate relevant metrics on the test data At face value it would seem like it should be easy to repeat this process for a number of different methods, however we have run into a number of difficulties attempting to do so with R. The predict() function seems to have a different behavior for each new method we see. Many methods have different cross-validation functions, or worse yet, no built-in process for cross-validation. Not all methods expect the same data format. Some methods do not use formula syntax. Different methods have different handling of categorical predictors. Thankfully, the R community has essentially provided a silver bullet for these issues, the caret package. Returning to the above list, we will see that a number of these tasks are directly addressed in the caret package. Test-train split the available data createDataPartition() will take the place of our manual data splitting. It will also do some extra work to ensure that the train and test samples are somewhat similar. Decide on a set of candidate models via tuning parameters expand.grid() is not a function in caret, but we will get in the habit of using it to specify a grid of tuning parameters. Select the best model (tuning parameters) using a cross-validated metric trainControl() will setup cross-validation train() is the workhorse of caret. It takes the following information then trains the requested model: form, a formula, such as y ~ . data method, from a long list of possibilities preProcess which allows for specification of things such as centering and scaling tuneGrid which specifies the tuning parameters to train over trControl which specifies the resampling scheme, that is, how cross-validation should be performed Use chosen model to make predictions predict() used on objects of type train will be magical! To illustrate caret, we return to our familiar Default data. data(Default, package = &quot;ISLR&quot;) library(caret) We first test-train split the data using createDataPartition. Here we are using 75% of the data for training. set.seed(430) default_idx = createDataPartition(Default$default, p = 0.75, list = FALSE) default_trn = Default[default_idx, ] default_tst = Default[-default_idx, ] default_glm = train( form = default ~ ., data = default_trn, method = &quot;glm&quot;, family = &quot;binomial&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 5) ) default_glm ## Generalized Linear Model ## ## 7501 samples ## 3 predictor ## 2 classes: &#39;No&#39;, &#39;Yes&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 6000, 6001, 6001, 6001, 6001 ## Resampling results: ## ## Accuracy Kappa ## 0.9729372 0.4147209 names(default_glm) ## [1] &quot;method&quot; &quot;modelInfo&quot; &quot;modelType&quot; &quot;results&quot; ## [5] &quot;pred&quot; &quot;bestTune&quot; &quot;call&quot; &quot;dots&quot; ## [9] &quot;metric&quot; &quot;control&quot; &quot;finalModel&quot; &quot;preProcess&quot; ## [13] &quot;trainingData&quot; &quot;resample&quot; &quot;resampledCM&quot; &quot;perfNames&quot; ## [17] &quot;maximize&quot; &quot;yLimits&quot; &quot;times&quot; &quot;levels&quot; ## [21] &quot;terms&quot; &quot;coefnames&quot; &quot;contrasts&quot; &quot;xlevels&quot; default_glm$results ## parameter Accuracy Kappa AccuracySD KappaSD ## 1 none 0.9729372 0.4147209 0.001527574 0.04620646 default_glm$finalModel ## ## Call: NULL ## ## Coefficients: ## (Intercept) studentYes balance income ## -1.066e+01 -6.254e-01 5.647e-03 1.395e-06 ## ## Degrees of Freedom: 7500 Total (i.e. Null); 7497 Residual ## Null Deviance: 2192 ## Residual Deviance: 1204 AIC: 1212 accuracy = function(actual, predicted) { mean(actual == predicted) } # make predictions head(predict(default_glm, newdata = default_trn)) ## [1] No No No No No No ## Levels: No Yes # train acc accuracy(actual = default_trn$default, predicted = predict(default_glm, newdata = default_trn)) ## [1] 0.9730703 # test acc accuracy(actual = default_tst$default, predicted = predict(default_glm, newdata = default_tst)) ## [1] 0.9739896 # get probs head(predict(default_glm, newdata = default_trn, type = &quot;prob&quot;)) ## No Yes ## 1 0.9984674 0.001532637 ## 3 0.9895850 0.010414985 ## 5 0.9979141 0.002085863 ## 6 0.9977233 0.002276746 ## 8 0.9987645 0.001235527 ## 9 0.9829081 0.017091877 default_knn = train( default ~ ., data = default_trn, method = &quot;knn&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 5) ) default_knn ## k-Nearest Neighbors ## ## 7501 samples ## 3 predictor ## 2 classes: &#39;No&#39;, &#39;Yes&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 6000, 6001, 6001, 6001, 6001 ## Resampling results across tuning parameters: ## ## k Accuracy Kappa ## 5 0.9656046 0.1770996 ## 7 0.9657378 0.1295425 ## 9 0.9676045 0.1092291 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was k = 9. default_knn = train( default ~ ., data = default_trn, method = &quot;knn&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 5), preProcess = c(&quot;center&quot;, &quot;scale&quot;), tuneGrid = expand.grid(k = seq(1, 100, by = 1)) ) default_knn ## k-Nearest Neighbors ## ## 7501 samples ## 3 predictor ## 2 classes: &#39;No&#39;, &#39;Yes&#39; ## ## Pre-processing: centered (3), scaled (3) ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 6001, 6001, 6001, 6001, 6000 ## Resampling results across tuning parameters: ## ## k Accuracy Kappa ## 1 0.9566725 0.3102526809 ## 2 0.9557393 0.3105592402 ## 3 0.9676047 0.3737110726 ## 4 0.9688045 0.3905217578 ## 5 0.9693378 0.3720650032 ## 6 0.9706708 0.3878199028 ## 7 0.9705374 0.3805478527 ## 8 0.9708038 0.3847869416 ## 9 0.9718704 0.4008912524 ## 10 0.9716037 0.3887596806 ## 11 0.9718702 0.3893164746 ## 12 0.9720036 0.4017431900 ## 13 0.9722702 0.3885361750 ## 14 0.9722703 0.3944481266 ## 15 0.9730702 0.4007241418 ## 16 0.9720039 0.3811342289 ## 17 0.9722703 0.3792775497 ## 18 0.9720038 0.3788523329 ## 19 0.9720038 0.3697985519 ## 20 0.9721370 0.3740347410 ## 21 0.9718704 0.3615639366 ## 22 0.9718705 0.3680145319 ## 23 0.9717371 0.3539511129 ## 24 0.9717371 0.3541232524 ## 25 0.9717371 0.3499644208 ## 26 0.9716037 0.3455285320 ## 27 0.9720037 0.3528854219 ## 28 0.9722703 0.3638828006 ## 29 0.9724036 0.3687985599 ## 30 0.9726703 0.3779447479 ## 31 0.9722704 0.3584335333 ## 32 0.9721371 0.3547676501 ## 33 0.9718704 0.3473184965 ## 34 0.9713372 0.3291867049 ## 35 0.9718703 0.3367666962 ## 36 0.9716038 0.3260879521 ## 37 0.9717371 0.3320811820 ## 38 0.9716037 0.3215128939 ## 39 0.9716037 0.3219513284 ## 40 0.9717371 0.3276712249 ## 41 0.9718704 0.3277674417 ## 42 0.9717371 0.3188637072 ## 43 0.9714704 0.3066997114 ## 44 0.9714704 0.3077206051 ## 45 0.9710704 0.2961400483 ## 46 0.9712037 0.2961184626 ## 47 0.9710704 0.2914235196 ## 48 0.9709372 0.2860070606 ## 49 0.9708037 0.2847306864 ## 50 0.9708037 0.2847306864 ## 51 0.9708036 0.2812029517 ## 52 0.9710703 0.2867649638 ## 53 0.9708037 0.2771131971 ## 54 0.9709372 0.2817510947 ## 55 0.9709371 0.2781721329 ## 56 0.9709371 0.2774089321 ## 57 0.9706705 0.2714234284 ## 58 0.9706704 0.2638759429 ## 59 0.9706703 0.2638759634 ## 60 0.9706703 0.2585241342 ## 61 0.9701371 0.2368125620 ## 62 0.9701372 0.2327890464 ## 63 0.9701372 0.2327890464 ## 64 0.9701372 0.2279714026 ## 65 0.9697372 0.2110842159 ## 66 0.9694708 0.1945370488 ## 67 0.9697372 0.1951671666 ## 68 0.9700038 0.2030358192 ## 69 0.9696040 0.1849170701 ## 70 0.9693373 0.1717143623 ## 71 0.9690708 0.1588168415 ## 72 0.9690708 0.1593832545 ## 73 0.9688041 0.1399844247 ## 74 0.9684042 0.1264961122 ## 75 0.9684041 0.1201537186 ## 76 0.9682708 0.1133391796 ## 77 0.9681375 0.1068555860 ## 78 0.9677375 0.0857547640 ## 79 0.9677376 0.0921848301 ## 80 0.9677376 0.0853775175 ## 81 0.9680043 0.0866367370 ## 82 0.9677377 0.0790611308 ## 83 0.9677376 0.0725173945 ## 84 0.9676043 0.0658085644 ## 85 0.9676043 0.0651989918 ## 86 0.9674709 0.0578805892 ## 87 0.9676043 0.0651989918 ## 88 0.9674709 0.0584901617 ## 89 0.9676043 0.0651989918 ## 90 0.9670710 0.0362615841 ## 91 0.9669378 0.0277870004 ## 92 0.9669377 0.0286699611 ## 93 0.9668044 0.0210781703 ## 94 0.9666711 0.0141321593 ## 95 0.9666711 0.0073298429 ## 96 0.9666711 0.0073298429 ## 97 0.9665378 -0.0002617801 ## 98 0.9665378 -0.0002617801 ## 99 0.9666711 0.0000000000 ## 100 0.9666711 0.0000000000 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was k = 15. plot(default_knn) ggplot(default_knn) + theme_bw() default_knn$bestTune ## k ## 15 15 get_best_result = function(caret_fit) { best_result = caret_fit$results[as.numeric(rownames(caret_fit$bestTune)), ] rownames(best_result) = NULL best_result } get_best_result(default_knn) ## k Accuracy Kappa AccuracySD KappaSD ## 1 15 0.9730702 0.4007241 0.002241574 0.08402323 default_knn$finalModel ## 15-nearest neighbor classification model ## Training set class distribution: ## ## No Yes ## 7251 250 Notes to add later: Fewer ties with CV than simple test-train approach Default grid vs specified grid. tuneLength Create table summarizing results for knn() and glm(). Test, train, and CV accuracy. Maybe also show SD for CV. 12.1 External Links The caret Package - Reference documentation for the caret package in bookdown format. caret Model List - List of available models in caret. 12.2 RMarkdown The RMarkdown file for this chapter can be found here. The file was created using R version 3.4.1 and the following packages: Base Packages, Attached ## [1] &quot;methods&quot; &quot;stats&quot; &quot;graphics&quot; &quot;grDevices&quot; &quot;utils&quot; &quot;datasets&quot; ## [7] &quot;base&quot; Additional Packages, Attached ## [1] &quot;caret&quot; &quot;ggplot2&quot; &quot;lattice&quot; Additional Packages, Not Attached ## [1] &quot;Rcpp&quot; &quot;nloptr&quot; &quot;compiler&quot; &quot;plyr&quot; ## [5] &quot;class&quot; &quot;iterators&quot; &quot;tools&quot; &quot;digest&quot; ## [9] &quot;lme4&quot; &quot;evaluate&quot; &quot;tibble&quot; &quot;gtable&quot; ## [13] &quot;nlme&quot; &quot;mgcv&quot; &quot;rlang&quot; &quot;Matrix&quot; ## [17] &quot;foreach&quot; &quot;parallel&quot; &quot;yaml&quot; &quot;SparseM&quot; ## [21] &quot;e1071&quot; &quot;stringr&quot; &quot;knitr&quot; &quot;MatrixModels&quot; ## [25] &quot;stats4&quot; &quot;rprojroot&quot; &quot;grid&quot; &quot;nnet&quot; ## [29] &quot;rmarkdown&quot; &quot;bookdown&quot; &quot;minqa&quot; &quot;reshape2&quot; ## [33] &quot;car&quot; &quot;magrittr&quot; &quot;backports&quot; &quot;scales&quot; ## [37] &quot;codetools&quot; &quot;ModelMetrics&quot; &quot;htmltools&quot; &quot;MASS&quot; ## [41] &quot;splines&quot; &quot;pbkrtest&quot; &quot;colorspace&quot; &quot;labeling&quot; ## [45] &quot;quantreg&quot; &quot;stringi&quot; &quot;lazyeval&quot; &quot;munsell&quot; "],
["subset-selection.html", "Chapter 13 Subset Selection 13.1 AIC, BIC, and Cp 13.2 Validated RMSE 13.3 External Links 13.4 RMarkdown", " Chapter 13 Subset Selection Instructor’s Note: This chapter is currently missing the usual narrative text. Hopefully it will be added later. data(Hitters, package = &quot;ISLR&quot;) sum(is.na(Hitters)) ## [1] 59 sum(is.na(Hitters$Salary)) ## [1] 59 Hitters = na.omit(Hitters) sum(is.na(Hitters)) ## [1] 0 13.1 AIC, BIC, and Cp 13.1.1 leaps Package library(leaps) 13.1.2 Best Subset fit_all = regsubsets(Salary ~ ., Hitters) summary(fit_all) ## Subset selection object ## Call: regsubsets.formula(Salary ~ ., Hitters) ## 19 Variables (and intercept) ## Forced in Forced out ## AtBat FALSE FALSE ## Hits FALSE FALSE ## HmRun FALSE FALSE ## Runs FALSE FALSE ## RBI FALSE FALSE ## Walks FALSE FALSE ## Years FALSE FALSE ## CAtBat FALSE FALSE ## CHits FALSE FALSE ## CHmRun FALSE FALSE ## CRuns FALSE FALSE ## CRBI FALSE FALSE ## CWalks FALSE FALSE ## LeagueN FALSE FALSE ## DivisionW FALSE FALSE ## PutOuts FALSE FALSE ## Assists FALSE FALSE ## Errors FALSE FALSE ## NewLeagueN FALSE FALSE ## 1 subsets of each size up to 8 ## Selection Algorithm: exhaustive ## AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns ## 1 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 3 ( 1 ) &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 4 ( 1 ) &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 5 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 6 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 7 ( 1 ) &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; ## 8 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; ## CRBI CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN ## 1 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 3 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 4 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 5 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 6 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 7 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 8 ( 1 ) &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; fit_all = regsubsets(Salary ~ ., data = Hitters, nvmax = 19) fit_all_sum = summary(fit_all) names(fit_all_sum) ## [1] &quot;which&quot; &quot;rsq&quot; &quot;rss&quot; &quot;adjr2&quot; &quot;cp&quot; &quot;bic&quot; &quot;outmat&quot; &quot;obj&quot; fit_all_sum$bic ## [1] -90.84637 -128.92622 -135.62693 -141.80892 -144.07143 -147.91690 ## [7] -145.25594 -147.61525 -145.44316 -143.21651 -138.86077 -133.87283 ## [13] -128.77759 -123.64420 -118.21832 -112.81768 -107.35339 -101.86391 ## [19] -96.30412 par(mfrow = c(2, 2)) plot(fit_all_sum$rss, xlab = &quot;Number of Variables&quot;, ylab = &quot;RSS&quot;, type = &quot;b&quot;) plot(fit_all_sum$adjr2, xlab = &quot;Number of Variables&quot;, ylab = &quot;Adjusted RSq&quot;, type = &quot;b&quot;) best_adj_r2 = which.max(fit_all_sum$adjr2) points(best_adj_r2, fit_all_sum$adjr2[best_adj_r2], col = &quot;red&quot;,cex = 2, pch = 20) plot(fit_all_sum$cp, xlab = &quot;Number of Variables&quot;, ylab = &quot;Cp&quot;, type = &#39;b&#39;) best_cp = which.min(fit_all_sum$cp) points(best_cp, fit_all_sum$cp[best_cp], col = &quot;red&quot;, cex = 2, pch = 20) plot(fit_all_sum$bic, xlab = &quot;Number of Variables&quot;, ylab = &quot;BIC&quot;, type = &#39;b&#39;) best_bic = which.min(fit_all_sum$bic) points(best_bic, fit_all_sum$bic[best_bic], col = &quot;red&quot;, cex = 2, pch = 20) 13.1.3 Stepwise Methods fit_fwd = regsubsets(Salary ~ ., data = Hitters, nvmax = 19, method = &quot;forward&quot;) fit_fwd_sum = summary(fit_fwd) fit_bwd = regsubsets(Salary ~ ., data = Hitters, nvmax = 19, method = &quot;backward&quot;) fit_bwd_sum = summary(fit_bwd) coef(fit_fwd, 7) ## (Intercept) AtBat Hits Walks CRBI ## 109.7873062 -1.9588851 7.4498772 4.9131401 0.8537622 ## CWalks DivisionW PutOuts ## -0.3053070 -127.1223928 0.2533404 coef(fit_bwd, 7) ## (Intercept) AtBat Hits Walks CRuns ## 105.6487488 -1.9762838 6.7574914 6.0558691 1.1293095 ## CWalks DivisionW PutOuts ## -0.7163346 -116.1692169 0.3028847 coef(fit_all, 7) ## (Intercept) Hits Walks CAtBat CHits ## 79.4509472 1.2833513 3.2274264 -0.3752350 1.4957073 ## CHmRun DivisionW PutOuts ## 1.4420538 -129.9866432 0.2366813 fit_bwd_sum = summary(fit_bwd) which.min(fit_bwd_sum$cp) ## [1] 10 coef(fit_bwd, which.min(fit_bwd_sum$cp)) ## (Intercept) AtBat Hits Walks CAtBat ## 162.5354420 -2.1686501 6.9180175 5.7732246 -0.1300798 ## CRuns CRBI CWalks DivisionW PutOuts ## 1.4082490 0.7743122 -0.8308264 -112.3800575 0.2973726 ## Assists ## 0.2831680 fit = lm(Salary ~ ., data = Hitters) fit_aic_back = step(fit, trace = FALSE) coef(fit_aic_back) ## (Intercept) AtBat Hits Walks CAtBat ## 162.5354420 -2.1686501 6.9180175 5.7732246 -0.1300798 ## CRuns CRBI CWalks DivisionW PutOuts ## 1.4082490 0.7743122 -0.8308264 -112.3800575 0.2973726 ## Assists ## 0.2831680 13.2 Validated RMSE set.seed(42) num_vars = ncol(Hitters) - 1 trn_idx = sample(c(TRUE, FALSE), nrow(Hitters), rep = TRUE) tst_idx = (!trn_idx) fit_all = regsubsets(Salary ~ ., data = Hitters[trn_idx, ], nvmax = num_vars) test_mat = model.matrix(Salary ~ ., data = Hitters[tst_idx, ]) test_err = rep(0, times = num_vars) for (i in seq_along(test_err)) { coefs = coef(fit_all, id = i) pred = test_mat[, names(coefs)] %*% coefs test_err[i] &lt;- sqrt(mean((Hitters$Salary[tst_idx] - pred) ^ 2)) } test_err ## [1] 357.1226 333.8531 323.6408 320.5458 308.0303 295.1308 301.8142 ## [8] 309.2389 303.3976 307.9660 307.4841 306.9883 313.2374 314.3905 ## [15] 313.8258 314.0586 313.6674 313.3490 313.3424 plot(test_err, type=&#39;b&#39;, ylab = &quot;Test Set RMSE&quot;, xlab = &quot;Number of Predictors&quot;) which.min(test_err) ## [1] 6 coef(fit_all, which.min(test_err)) ## (Intercept) Walks CAtBat CHits CRBI ## 171.2082504 5.0067050 -0.4005457 1.2951923 0.7894534 ## DivisionW PutOuts ## -131.1212694 0.2682166 class(fit_all) ## [1] &quot;regsubsets&quot; predict.regsubsets = function(object, newdata, id, ...) { form = as.formula(object$call[[2]]) mat = model.matrix(form, newdata) coefs = coef(object, id = id) xvars = names(coefs) mat[, xvars] %*% coefs } rmse = function(actual, predicted) { sqrt(mean((actual - predicted) ^ 2)) } num_folds = 5 num_vars = 19 set.seed(1) folds = caret::createFolds(Hitters$Salary, k = num_folds) fold_error = matrix(0, nrow = num_folds, ncol = num_vars, dimnames = list(paste(1:5), paste(1:19))) for(j in 1:num_folds) { train_fold = Hitters[-folds[[j]], ] validate_fold = Hitters[ folds[[j]], ] best_fit = regsubsets(Salary ~ ., data = train_fold, nvmax = 19) for (i in 1:num_vars) { pred = predict(best_fit, validate_fold, id = i) fold_error[j, i] = rmse(actual = validate_fold$Salary, predicted = pred) } } cv_error = apply(fold_error, 2, mean) cv_error ## 1 2 3 4 5 6 7 8 ## 373.2202 363.0715 374.8356 362.2405 357.6623 350.0238 348.0589 342.9089 ## 9 10 11 12 13 14 15 16 ## 343.8661 341.6405 339.4228 341.9303 342.5545 342.0155 340.8147 343.4722 ## 17 18 19 ## 343.4259 343.8129 343.2279 plot(cv_error, type=&#39;b&#39;, ylab = &quot;Corss-Validated RMSE&quot;, xlab = &quot;Number of Predictors&quot;) fit_all = regsubsets(Salary ~ ., data = Hitters, nvmax = num_vars) coef(fit_all, which.min(cv_error)) ## (Intercept) AtBat Hits Walks CAtBat ## 135.7512195 -2.1277482 6.9236994 5.6202755 -0.1389914 ## CRuns CRBI CWalks LeagueN DivisionW ## 1.4553310 0.7852528 -0.8228559 43.1116152 -111.1460252 ## PutOuts Assists ## 0.2894087 0.2688277 13.3 External Links - 13.4 RMarkdown The RMarkdown file for this chapter can be found here. The file was created using R version 3.4.1 and the following packages: Base Packages, Attached ## [1] &quot;stats&quot; &quot;graphics&quot; &quot;grDevices&quot; &quot;utils&quot; &quot;datasets&quot; &quot;base&quot; Additional Packages, Attached ## [1] &quot;leaps&quot; Additional Packages, Not Attached ## [1] &quot;Rcpp&quot; &quot;nloptr&quot; &quot;compiler&quot; &quot;plyr&quot; ## [5] &quot;methods&quot; &quot;iterators&quot; &quot;tools&quot; &quot;digest&quot; ## [9] &quot;lme4&quot; &quot;evaluate&quot; &quot;tibble&quot; &quot;gtable&quot; ## [13] &quot;nlme&quot; &quot;lattice&quot; &quot;mgcv&quot; &quot;rlang&quot; ## [17] &quot;Matrix&quot; &quot;foreach&quot; &quot;parallel&quot; &quot;yaml&quot; ## [21] &quot;SparseM&quot; &quot;stringr&quot; &quot;knitr&quot; &quot;MatrixModels&quot; ## [25] &quot;stats4&quot; &quot;rprojroot&quot; &quot;grid&quot; &quot;caret&quot; ## [29] &quot;nnet&quot; &quot;rmarkdown&quot; &quot;bookdown&quot; &quot;minqa&quot; ## [33] &quot;ggplot2&quot; &quot;reshape2&quot; &quot;car&quot; &quot;magrittr&quot; ## [37] &quot;backports&quot; &quot;scales&quot; &quot;codetools&quot; &quot;ModelMetrics&quot; ## [41] &quot;htmltools&quot; &quot;MASS&quot; &quot;splines&quot; &quot;pbkrtest&quot; ## [45] &quot;colorspace&quot; &quot;quantreg&quot; &quot;stringi&quot; &quot;lazyeval&quot; ## [49] &quot;munsell&quot; "],
["shrinkage-methods.html", "Chapter 14 Shrinkage Methods 14.1 Ridge Regression 14.2 Lasso 14.3 broom 14.4 Simulation Study, p &gt; n 14.5 External Links 14.6 RMarkdown", " Chapter 14 Shrinkage Methods We will use the Hitters dataset from the ISLR package to explore two shrinkage methods: ridge and lasso. These are otherwise known as penalized regression methods. data(Hitters, package = &quot;ISLR&quot;) This dataset has some missing data in the response Salaray. We use the na.omit() function the clean the dataset. sum(is.na(Hitters)) ## [1] 59 sum(is.na(Hitters$Salary)) ## [1] 59 Hitters = na.omit(Hitters) sum(is.na(Hitters)) ## [1] 0 The predictors variables are offensive and defensive statistics for a number of baseball players. names(Hitters) ## [1] &quot;AtBat&quot; &quot;Hits&quot; &quot;HmRun&quot; &quot;Runs&quot; &quot;RBI&quot; ## [6] &quot;Walks&quot; &quot;Years&quot; &quot;CAtBat&quot; &quot;CHits&quot; &quot;CHmRun&quot; ## [11] &quot;CRuns&quot; &quot;CRBI&quot; &quot;CWalks&quot; &quot;League&quot; &quot;Division&quot; ## [16] &quot;PutOuts&quot; &quot;Assists&quot; &quot;Errors&quot; &quot;Salary&quot; &quot;NewLeague&quot; We use the glmnet() and cv.glmnet() functions in the glmnet package to fit penalized regressions. # this is a temporary workaround for an issue with glmnet, Matrix, and R version 3.3.3 # see here: http://stackoverflow.com/questions/43282720/r-error-in-validobject-object-when-running-as-script-but-not-in-console library(methods) library(glmnet) The glmnet function does not allow the use of model formulas, so we setup the data for ease of use with glmnet. X = model.matrix(Salary ~ ., Hitters)[, -1] y = Hitters$Salary First, we fit a regular linear regression, and note the size of the predictors’ coefficients, and predictors’ coefficients squared. (The two penalties we will use.) fit = lm(Salary ~ ., Hitters) coef(fit) ## (Intercept) AtBat Hits HmRun Runs ## 163.1035878 -1.9798729 7.5007675 4.3308829 -2.3762100 ## RBI Walks Years CAtBat CHits ## -1.0449620 6.2312863 -3.4890543 -0.1713405 0.1339910 ## CHmRun CRuns CRBI CWalks LeagueN ## -0.1728611 1.4543049 0.8077088 -0.8115709 62.5994230 ## DivisionW PutOuts Assists Errors NewLeagueN ## -116.8492456 0.2818925 0.3710692 -3.3607605 -24.7623251 sum(abs(coef(fit)[-1])) ## [1] 238.7295 sum(coef(fit)[-1] ^ 2) ## [1] 18337.3 14.1 Ridge Regression We first illustrate ridge regression, which can be fit using glmnet() with alpha = 0 and seeks to minimize \\[ \\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij} \\right) ^ 2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2 . \\] Notice that the intercept is not penalized. Also, note that that ridge regression is not scale invariant like the usual unpenalized regression. Thankfully, glmnet() takes care of this internally. It automatically standardizes input for fitting, then reports fitted coefficient using the original scale. The two plots illustrate how much the coefficients are penalized for different values of \\(\\lambda\\). Notice none of the coefficients are forced to be zero. fit_ridge = glmnet(X, y, alpha = 0) plot(fit_ridge) plot(fit_ridge, xvar = &quot;lambda&quot;, label = TRUE) dim(coef(fit_ridge)) ## [1] 20 100 We use cross-validation to select a good \\(\\lambda\\) value. The cv.glmnet()function uses 10 folds by default. The plot illustrates the MSE for the \\(\\lambda\\)s considered. Two lines are drawn. The first is the \\(\\lambda\\) that gives the smallest MSE. The second is the \\(\\lambda\\) that gives an MSE within one standard error of the smallest. fit_ridge_cv = cv.glmnet(X, y, alpha = 0) plot(fit_ridge_cv) The cv.glmnet() function returns several details of the fit for both \\(\\lambda\\) values in the plot. Notice the penalty terms are smaller than the full linear regression. (As we would expect.) coef(fit_ridge_cv) ## 20 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## 1 ## (Intercept) 254.518230141 ## AtBat 0.079408204 ## Hits 0.317375220 ## HmRun 1.065097243 ## Runs 0.515835607 ## RBI 0.520504723 ## Walks 0.661891621 ## Years 2.231379426 ## CAtBat 0.006679258 ## CHits 0.025455999 ## CHmRun 0.189661478 ## CRuns 0.051057906 ## CRBI 0.052776153 ## CWalks 0.052170266 ## LeagueN 2.114989228 ## DivisionW -17.479743519 ## PutOuts 0.043039515 ## Assists 0.006296277 ## Errors -0.099487300 ## NewLeagueN 2.085946064 coef(fit_ridge_cv, s = &quot;lambda.min&quot;) ## 20 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## 1 ## (Intercept) 7.645824e+01 ## AtBat -6.315180e-01 ## Hits 2.642160e+00 ## HmRun -1.388233e+00 ## Runs 1.045729e+00 ## RBI 7.315713e-01 ## Walks 3.278001e+00 ## Years -8.723734e+00 ## CAtBat 1.256354e-04 ## CHits 1.318975e-01 ## CHmRun 6.895578e-01 ## CRuns 2.830055e-01 ## CRBI 2.514905e-01 ## CWalks -2.599851e-01 ## LeagueN 5.233720e+01 ## DivisionW -1.224170e+02 ## PutOuts 2.623667e-01 ## Assists 1.629044e-01 ## Errors -3.644002e+00 ## NewLeagueN -1.702598e+01 sum(coef(fit_ridge_cv, s = &quot;lambda.min&quot;)[-1] ^ 2) # penalty term for lambda minimum ## [1] 18126.85 coef(fit_ridge_cv, s = &quot;lambda.1se&quot;) ## 20 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## 1 ## (Intercept) 254.518230141 ## AtBat 0.079408204 ## Hits 0.317375220 ## HmRun 1.065097243 ## Runs 0.515835607 ## RBI 0.520504723 ## Walks 0.661891621 ## Years 2.231379426 ## CAtBat 0.006679258 ## CHits 0.025455999 ## CHmRun 0.189661478 ## CRuns 0.051057906 ## CRBI 0.052776153 ## CWalks 0.052170266 ## LeagueN 2.114989228 ## DivisionW -17.479743519 ## PutOuts 0.043039515 ## Assists 0.006296277 ## Errors -0.099487300 ## NewLeagueN 2.085946064 sum(coef(fit_ridge_cv, s = &quot;lambda.1se&quot;)[-1] ^ 2) # penalty term for lambda one SE ## [1] 321.618 #predict(fit_ridge_cv, X, s = &quot;lambda.min&quot;) #predict(fit_ridge_cv, X) mean((y - predict(fit_ridge_cv, X)) ^ 2) # &quot;train error&quot; ## [1] 138732.7 sqrt(fit_ridge_cv$cvm) # CV-RMSEs ## [1] 453.8860 452.3053 451.4592 451.1916 450.8992 450.5796 450.2305 ## [8] 449.8494 449.4336 448.9802 448.4860 447.9478 447.3619 446.7248 ## [15] 446.0326 445.2809 444.4659 443.5831 442.6278 441.5957 440.4821 ## [22] 439.2823 437.9920 436.6068 435.1226 433.5357 431.8430 430.0418 ## [29] 428.1303 426.1075 423.9734 421.7292 419.3776 416.9224 414.3693 ## [36] 411.7254 408.9996 406.2020 403.3448 400.4415 397.5070 394.5572 ## [43] 391.6087 388.6788 385.7846 382.9431 380.1704 377.4818 374.8911 ## [50] 372.4100 370.0477 367.8126 365.7110 363.7462 361.9197 360.2309 ## [57] 358.6776 357.2562 355.9618 354.7883 353.7293 352.7775 351.9259 ## [64] 351.1642 350.4893 349.8946 349.3717 348.9146 348.5131 348.1695 ## [71] 347.8752 347.6187 347.4060 347.2272 347.0712 346.9474 346.8403 ## [78] 346.7540 346.6785 346.6202 346.5660 346.5176 346.4743 346.4268 ## [85] 346.3808 346.3309 346.2774 346.2170 346.1518 346.0767 345.9918 ## [92] 345.9048 345.8043 345.6976 345.5829 345.4590 345.3311 345.1996 ## [99] 345.0604 sqrt(fit_ridge_cv$cvm[fit_ridge_cv$lambda == fit_ridge_cv$lambda.min]) # CV-RMSE minimum ## [1] 345.0604 sqrt(fit_ridge_cv$cvm[fit_ridge_cv$lambda == fit_ridge_cv$lambda.1se]) # CV-RMSE one SE ## [1] 380.1704 14.2 Lasso We now illustrate lasso, which can be fit using glmnet() with alpha = 1 and seeks to minimize \\[ \\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij} \\right) ^ 2 + \\lambda \\sum_{j=1}^{p} |\\beta_j| . \\] Like ridge, lasso is not scale invariant. The two plots illustrate how much the coefficients are penalized for different values of \\(\\lambda\\). Notice some of the coefficients are forced to be zero. fit_lasso = glmnet(X, y, alpha = 1) plot(fit_lasso) plot(fit_lasso, xvar = &quot;lambda&quot;, label = TRUE) dim(coef(fit_lasso)) ## [1] 20 80 Again, to actually pick a \\(\\lambda\\), we will use cross-validation. The plot is similar to the ridge plot. Notice along the top is the number of features in the model. (Which changed in this plot.) fit_lasso_cv = cv.glmnet(X, y, alpha = 1) plot(fit_lasso_cv) cv.glmnet() returns several details of the fit for both \\(\\lambda\\) values in the plot. Notice the penalty terms are again smaller than the full linear regression. (As we would expect.) Some coefficients are 0. coef(fit_lasso_cv) ## 20 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## 1 ## (Intercept) 167.91202818 ## AtBat . ## Hits 1.29269756 ## HmRun . ## Runs . ## RBI . ## Walks 1.39817511 ## Years . ## CAtBat . ## CHits . ## CHmRun . ## CRuns 0.14167760 ## CRBI 0.32192558 ## CWalks . ## LeagueN . ## DivisionW . ## PutOuts 0.04675463 ## Assists . ## Errors . ## NewLeagueN . coef(fit_lasso_cv, s = &quot;lambda.min&quot;) ## 20 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## 1 ## (Intercept) 129.4155571 ## AtBat -1.6130155 ## Hits 5.8058915 ## HmRun . ## Runs . ## RBI . ## Walks 4.8469340 ## Years -9.9724045 ## CAtBat . ## CHits . ## CHmRun 0.5374550 ## CRuns 0.6811938 ## CRBI 0.3903563 ## CWalks -0.5560144 ## LeagueN 32.4646094 ## DivisionW -119.3480842 ## PutOuts 0.2741895 ## Assists 0.1855978 ## Errors -2.1650837 ## NewLeagueN . sum(abs(coef(fit_lasso_cv, s = &quot;lambda.min&quot;)[-1])) # penalty term for lambda minimum ## [1] 178.8408 coef(fit_lasso_cv, s = &quot;lambda.1se&quot;) ## 20 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## 1 ## (Intercept) 167.91202818 ## AtBat . ## Hits 1.29269756 ## HmRun . ## Runs . ## RBI . ## Walks 1.39817511 ## Years . ## CAtBat . ## CHits . ## CHmRun . ## CRuns 0.14167760 ## CRBI 0.32192558 ## CWalks . ## LeagueN . ## DivisionW . ## PutOuts 0.04675463 ## Assists . ## Errors . ## NewLeagueN . sum(abs(coef(fit_lasso_cv, s = &quot;lambda.1se&quot;)[-1])) # penalty term for lambda one SE ## [1] 3.20123 #predict(fit_lasso_cv, X, s = &quot;lambda.min&quot;) #predict(fit_lasso_cv, X) mean((y - predict(fit_lasso_cv, X)) ^ 2) # &quot;train error&quot; ## [1] 123931.3 sqrt(fit_lasso_cv$cvm) ## [1] 450.4667 439.5342 429.1014 420.2278 411.7974 403.1526 395.2030 ## [8] 388.1047 381.8172 376.4784 371.9706 367.9954 364.6244 361.8324 ## [15] 359.0267 355.8590 352.9123 350.4139 348.3320 346.5990 345.1582 ## [22] 343.9776 343.0062 342.2064 341.6185 341.1848 341.0257 341.0220 ## [29] 341.0680 341.1822 341.3420 341.5010 341.6557 341.8143 341.9259 ## [36] 342.0843 342.4935 342.7806 342.5019 341.8506 341.0687 340.2737 ## [43] 339.5857 339.0583 338.4835 337.8989 337.4145 337.1080 336.8034 ## [50] 336.6050 336.4913 336.5493 336.6815 336.8781 336.9805 337.0503 ## [57] 337.1468 337.1720 337.1742 337.2114 337.3021 337.4978 337.7147 ## [64] 337.9278 338.1234 338.3568 338.5477 338.7480 338.9406 339.1208 ## [71] 339.2701 339.4129 339.5202 339.6048 sqrt(fit_lasso_cv$cvm[fit_lasso_cv$lambda == fit_lasso_cv$lambda.min]) # CV-RMSE minimum ## [1] 336.4913 sqrt(fit_lasso_cv$cvm[fit_lasso_cv$lambda == fit_lasso_cv$lambda.1se]) # CV-RMSE one SE ## [1] 364.6244 14.3 broom Sometimes, the output from glmnet() can be overwhelming. The broom package can help with that. library(broom) #fit_lasso_cv tidy(fit_lasso_cv) ## lambda estimate std.error conf.high conf.low nzero ## 1 255.2820965 202920.3 25218.50 228138.8 177701.78 0 ## 2 232.6035386 193190.3 25203.86 218394.2 167986.46 1 ## 3 211.9396813 184128.0 24929.00 209057.0 159199.04 2 ## 4 193.1115442 176591.4 24686.50 201277.9 151904.87 2 ## 5 175.9560468 169577.1 24414.65 193991.7 145162.42 3 ## 6 160.3245966 162532.0 24377.15 186909.2 138154.87 4 ## 7 146.0818013 156185.4 24257.32 180442.7 131928.08 4 ## 8 133.1042967 150625.2 24049.18 174674.4 126576.05 4 ## 9 121.2796778 145784.4 23808.60 169593.0 121975.81 4 ## 10 110.5055255 141736.0 23620.54 165356.5 118115.44 4 ## 11 100.6885192 138362.1 23502.45 161864.6 114859.65 5 ## 12 91.7436287 135420.6 23435.61 158856.2 111984.99 5 ## 13 83.5933775 132951.0 23398.99 156350.0 109551.97 5 ## 14 76.1671723 130922.7 23398.34 154321.0 107524.35 5 ## 15 69.4006906 128900.2 23437.48 152337.7 105462.72 6 ## 16 63.2353245 126635.6 23389.74 150025.4 103245.89 6 ## 17 57.6176726 124547.1 23291.03 147838.1 101256.07 6 ## 18 52.4990774 122789.9 23206.06 145996.0 99583.87 6 ## 19 47.8352040 121335.2 23140.34 144475.5 98194.84 6 ## 20 43.5856563 120130.8 23089.89 143220.7 97040.94 6 ## 21 39.7136268 119134.2 23053.48 142187.7 96080.72 6 ## 22 36.1855776 118320.6 23029.81 141350.4 95290.80 6 ## 23 32.9709506 117653.2 23014.34 140667.6 94638.88 6 ## 24 30.0419022 117105.2 23004.92 140110.2 94100.30 6 ## 25 27.3730624 116703.2 23008.15 139711.4 93695.05 6 ## 26 24.9413150 116407.1 23019.55 139426.6 93387.53 6 ## 27 22.7255973 116298.5 23114.05 139412.6 93184.45 6 ## 28 20.7067179 116296.0 23191.03 139487.1 93105.01 6 ## 29 18.8671902 116327.4 23258.64 139586.0 93068.72 6 ## 30 17.1910810 116405.3 23321.28 139726.6 93084.03 7 ## 31 15.6638727 116514.3 23381.61 139895.9 93132.72 7 ## 32 14.2723374 116623.0 23440.59 140063.5 93182.37 7 ## 33 13.0044223 116728.6 23496.59 140225.2 93232.00 9 ## 34 11.8491453 116837.0 23547.75 140384.7 93289.24 9 ## 35 10.7964999 116913.3 23585.20 140498.5 93328.12 9 ## 36 9.8373686 117021.6 23626.34 140648.0 93395.30 9 ## 37 8.9634439 117301.8 23647.80 140949.6 93654.02 9 ## 38 8.1671562 117498.5 23683.21 141181.7 93815.30 11 ## 39 7.4416086 117307.6 23571.59 140879.2 93735.97 11 ## 40 6.7805166 116861.8 23398.00 140259.8 93463.83 12 ## 41 6.1781542 116327.8 23218.01 139545.9 93109.83 12 ## 42 5.6293040 115786.2 23055.36 138841.5 92730.82 13 ## 43 5.1292121 115318.4 22913.69 138232.1 92404.75 13 ## 44 4.6735471 114960.5 22802.85 137763.4 92157.66 13 ## 45 4.2583620 114571.1 22691.64 137262.7 91879.46 13 ## 46 3.8800609 114175.7 22554.23 136729.9 91621.46 13 ## 47 3.5353670 113848.5 22418.32 136266.8 91430.21 13 ## 48 3.2212947 113641.8 22283.59 135925.4 91358.24 13 ## 49 2.9351238 113436.5 22169.36 135605.9 91267.14 13 ## 50 2.6743755 113302.9 22065.47 135368.4 91237.43 13 ## 51 2.4367913 113226.4 21990.47 135216.9 91235.92 13 ## 52 2.2203135 113265.4 21912.06 135177.5 91353.38 14 ## 53 2.0230670 113354.4 21845.22 135199.7 91509.22 15 ## 54 1.8433433 113486.9 21789.57 135276.4 91697.30 15 ## 55 1.6795857 113555.8 21723.84 135279.7 91832.00 17 ## 56 1.5303760 113602.9 21675.98 135278.9 91926.92 17 ## 57 1.3944216 113668.0 21643.70 135311.7 92024.29 17 ## 58 1.2705450 113685.0 21620.15 135305.1 92064.83 17 ## 59 1.1576733 113686.5 21593.07 135279.5 92093.40 17 ## 60 1.0548288 113711.6 21573.64 135285.2 92137.92 17 ## 61 0.9611207 113772.7 21561.10 135333.8 92211.58 17 ## 62 0.8757374 113904.8 21548.51 135453.3 92356.24 17 ## 63 0.7979393 114051.2 21537.17 135588.4 92514.06 17 ## 64 0.7270526 114195.2 21517.33 135712.5 92677.88 17 ## 65 0.6624632 114327.4 21501.10 135828.5 92826.33 18 ## 66 0.6036118 114485.3 21484.99 135970.3 93000.31 18 ## 67 0.5499886 114614.6 21472.42 136087.0 93142.13 18 ## 68 0.5011291 114750.2 21459.91 136210.1 93290.27 17 ## 69 0.4566102 114880.7 21448.88 136329.6 93431.86 18 ## 70 0.4160462 115002.9 21450.62 136453.5 93552.26 18 ## 71 0.3790858 115104.2 21452.03 136556.2 93652.17 18 ## 72 0.3454089 115201.1 21452.72 136653.8 93748.38 18 ## 73 0.3147237 115274.0 21457.78 136731.8 93816.22 18 ## 74 0.2867645 115331.4 21461.84 136793.2 93869.57 18 glance(fit_lasso_cv) # the two lambda values of interest ## lambda.min lambda.1se ## 1 2.436791 83.59338 14.4 Simulation Study, p &gt; n Aside from simply shrinking coefficients (ridge) and setting some coefficients to 0 (lasso), penalized regression also has the advantage of being able to handle the \\(p &gt; n\\) case. set.seed(1234) n = 1000 p = 5500 X = replicate(p, rnorm(n = n)) beta = c(1, 1, 1, rep(0, 5497)) z = X %*% beta prob = exp(z) / (1 + exp(z)) y = as.factor(rbinom(length(z), size = 1, prob = prob)) We first simulate a classification example where \\(p &gt; n\\). # glm(y ~ X, family = &quot;binomial&quot;) # will not converge We then use a lasso penalty to fit penalized logistic regression. This minimizes \\[ \\sum_{i=1}^{n} L\\left(y_i, \\beta_0 + \\sum_{j=1}^{p} \\beta_j x_{ij}\\right) + \\lambda \\sum_{j=1}^{p} |\\beta_j| \\] where \\(L\\) is the appropriate negative log-likelihood. library(glmnet) fit_cv = cv.glmnet(X, y, family = &quot;binomial&quot;, alpha = 1) plot(fit_cv) head(coef(fit_cv), n = 10) ## 10 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## 1 ## (Intercept) 0.02397452 ## V1 0.59674958 ## V2 0.56251761 ## V3 0.60065105 ## V4 . ## V5 . ## V6 . ## V7 . ## V8 . ## V9 . fit_cv$nzero ## s0 s1 s2 s3 s4 s5 s6 s7 s8 s9 s10 s11 s12 s13 s14 s15 s16 s17 ## 0 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 ## s18 s19 s20 s21 s22 s23 s24 s25 s26 s27 s28 s29 s30 s31 s32 s33 s34 s35 ## 3 3 3 3 3 3 3 3 3 3 3 3 4 6 7 10 18 24 ## s36 s37 s38 s39 s40 s41 s42 s43 s44 s45 s46 s47 s48 s49 s50 s51 s52 s53 ## 35 54 65 75 86 100 110 129 147 168 187 202 221 241 254 269 283 298 ## s54 s55 s56 s57 s58 s59 s60 s61 s62 s63 s64 s65 s66 s67 s68 s69 s70 s71 ## 310 324 333 350 364 375 387 400 411 429 435 445 453 455 462 466 475 481 ## s72 s73 s74 s75 s76 s77 s78 s79 s80 s81 s82 s83 s84 s85 s86 s87 s88 s89 ## 487 491 496 498 502 504 512 518 523 526 528 536 543 550 559 561 563 566 ## s90 s91 s92 s93 s94 s95 s96 s97 s98 ## 570 571 576 582 586 590 596 596 600 Notice, only the first three predictors generated are truly significant, and that is exactly what the suggested model finds. fit_1se = glmnet(X, y, family = &quot;binomial&quot;, lambda = fit_cv$lambda.1se) which(as.vector(as.matrix(fit_1se$beta)) != 0) ## [1] 1 2 3 We can also see in the following plots, the three features entering the model well ahead of the irrelevant features. plot(glmnet(X, y, family = &quot;binomial&quot;)) plot(glmnet(X, y, family = &quot;binomial&quot;), xvar = &quot;lambda&quot;) We can extract the two relevant \\(\\lambda\\) values. fit_cv$lambda.min ## [1] 0.03087158 fit_cv$lambda.1se ## [1] 0.0514969 Since cv.glmnet() does not calculate prediction accuracy for classification, we take the \\(\\lambda\\) values and create a grid for caret to search in order to obtain prediction accuracy with train(). We set \\(\\alpha = 1\\) in this grid, as glmnet can actually tune over the \\(\\alpha = 1\\) parameter. (More on that later.) Note that we have to force y to be a factor, so that train() recognizes we want to have a binomial response. The train() function in caret use the type of variable in y to determine if you want to use family = &quot;binomial&quot; or family = &quot;gaussian&quot;. library(caret) cv_5 = trainControl(method = &quot;cv&quot;, number = 5) lasso_grid = expand.grid(alpha = 1, lambda = c(fit_cv$lambda.min, fit_cv$lambda.1se)) lasso_grid ## alpha lambda ## 1 1 0.03087158 ## 2 1 0.05149690 sim_data = data.frame(y, X) fit_lasso = train( y ~ ., data = sim_data, method = &quot;glmnet&quot;, trControl = cv_5, tuneGrid = lasso_grid ) fit_lasso$results ## alpha lambda Accuracy Kappa AccuracySD KappaSD ## 1 1 0.03087158 0.7609903 0.5218887 0.01486223 0.03000986 ## 2 1 0.05149690 0.7659604 0.5319189 0.01807380 0.03594319 14.5 External Links glmnet Web Vingette - Details from the package developers. 14.6 RMarkdown The RMarkdown file for this chapter can be found here. The file was created using R version 3.4.1 and the following packages: Base Packages, Attached ## [1] &quot;methods&quot; &quot;stats&quot; &quot;graphics&quot; &quot;grDevices&quot; &quot;utils&quot; &quot;datasets&quot; ## [7] &quot;base&quot; Additional Packages, Attached ## [1] &quot;caret&quot; &quot;ggplot2&quot; &quot;lattice&quot; &quot;broom&quot; &quot;glmnet&quot; &quot;foreach&quot; &quot;Matrix&quot; Additional Packages, Not Attached ## [1] &quot;reshape2&quot; &quot;splines&quot; &quot;colorspace&quot; &quot;htmltools&quot; ## [5] &quot;stats4&quot; &quot;yaml&quot; &quot;mgcv&quot; &quot;rlang&quot; ## [9] &quot;ModelMetrics&quot; &quot;e1071&quot; &quot;nloptr&quot; &quot;foreign&quot; ## [13] &quot;glue&quot; &quot;bindrcpp&quot; &quot;bindr&quot; &quot;plyr&quot; ## [17] &quot;stringr&quot; &quot;MatrixModels&quot; &quot;munsell&quot; &quot;gtable&quot; ## [21] &quot;codetools&quot; &quot;psych&quot; &quot;evaluate&quot; &quot;knitr&quot; ## [25] &quot;SparseM&quot; &quot;class&quot; &quot;quantreg&quot; &quot;pbkrtest&quot; ## [29] &quot;parallel&quot; &quot;Rcpp&quot; &quot;backports&quot; &quot;scales&quot; ## [33] &quot;lme4&quot; &quot;mnormt&quot; &quot;digest&quot; &quot;stringi&quot; ## [37] &quot;bookdown&quot; &quot;dplyr&quot; &quot;grid&quot; &quot;rprojroot&quot; ## [41] &quot;tools&quot; &quot;magrittr&quot; &quot;lazyeval&quot; &quot;tibble&quot; ## [45] &quot;tidyr&quot; &quot;car&quot; &quot;pkgconfig&quot; &quot;MASS&quot; ## [49] &quot;assertthat&quot; &quot;minqa&quot; &quot;rmarkdown&quot; &quot;iterators&quot; ## [53] &quot;R6&quot; &quot;nnet&quot; &quot;nlme&quot; &quot;compiler&quot; "],
["elastic-net.html", "Chapter 15 Elastic Net 15.1 Hitters Data 15.2 Elastic Net for Regression 15.3 Elastic Net for Classification 15.4 External Links 15.5 RMarkdown", " Chapter 15 Elastic Net We again use the Hitters dataset from the ISLR package to explore another shrinkage method, elastic net, which combines the ridge and lasso methods from the previous chapter. 15.1 Hitters Data data(Hitters, package = &quot;ISLR&quot;) Hitters = na.omit(Hitters) We again remove the missing data, which was all in the response variable, Salary. tibble::as_tibble(Hitters) ## # A tibble: 263 x 20 ## AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns ## * &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 315 81 7 24 38 39 14 3449 835 69 321 ## 2 479 130 18 66 72 76 3 1624 457 63 224 ## 3 496 141 20 65 78 37 11 5628 1575 225 828 ## 4 321 87 10 39 42 30 2 396 101 12 48 ## 5 594 169 4 74 51 35 11 4408 1133 19 501 ## 6 185 37 1 23 8 21 2 214 42 1 30 ## 7 298 73 0 24 24 7 3 509 108 0 41 ## 8 323 81 6 26 32 8 2 341 86 6 32 ## 9 401 92 17 49 66 65 13 5206 1332 253 784 ## 10 574 159 21 107 75 59 10 4631 1300 90 702 ## # ... with 253 more rows, and 9 more variables: CRBI &lt;int&gt;, CWalks &lt;int&gt;, ## # League &lt;fctr&gt;, Division &lt;fctr&gt;, PutOuts &lt;int&gt;, Assists &lt;int&gt;, ## # Errors &lt;int&gt;, Salary &lt;dbl&gt;, NewLeague &lt;fctr&gt; dim(Hitters) ## [1] 263 20 Because this dataset isn’t particularly large, we will forego a test-train split, and simply use all of the data as training data. # this is a temporary workaround for an issue with glmnet, Matrix, and R version 3.3.3 # see here: http://stackoverflow.com/questions/43282720/r-error-in-validobject-object-when-running-as-script-but-not-in-console library(methods) library(caret) library(glmnet) Since he have loaded caret, we also have access to the lattice package which has a nice histogram function. histogram(Hitters$Salary, xlab = &quot;Salary, $1000s&quot;, main = &quot;Baseball Salaries, 1986 - 1987&quot;) 15.2 Elastic Net for Regression Like ridge and lasso, we again attempt to minimize the residual sum of squares plus some penalty term. \\[ \\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij} \\right) ^ 2 + \\lambda\\left[(1-\\alpha)||\\beta||_2^2/2 + \\alpha ||\\beta||_1\\right] \\] Here, \\(||\\beta||_1\\) is called the \\(l_1\\) norm. \\[ ||\\beta||_1 = \\sum_{j=1}^{p} |\\beta_j| \\] Similarly, \\(||\\beta||_2\\) is called the \\(l_2\\), or Euclidean norm. \\[ ||\\beta||_2 = \\sqrt{\\sum_{j=1}^{p} \\beta_j^2} \\] These both quantify how “large” the coefficients are. Like lasso and ridge, the intercept is not penalized and glment takes care of standardization internally. Also reported coefficients are on the original scale. The new penalty is \\(\\frac{\\lambda \\cdot (1-\\alpha)}{2}\\) times the ridge penalty plus \\(\\lambda \\cdot \\alpha\\) times the lasso lasso penalty. (Dividing the ridge penalty by 2 is a mathematical convenience for optimization.) Essentially, with the correct choice of \\(\\lambda\\) and \\(\\alpha\\) these two “penalty coefficients” can be any positive numbers. Often it is more useful to simply think of \\(\\alpha\\) as controlling the mixing between the two penalties and \\(\\lambda\\) controlling the amount of penalization. \\(\\alpha\\) takes values between 0 and 1. Using \\(\\alpha = 1\\) gives the lasso that we have seen before. Similarly, \\(\\alpha = 0\\) gives ridge. We used these two before with glmnet() to specify which to method we wanted. Now we also allow for \\(\\alpha\\) values in between. set.seed(430) cv_5 = trainControl(method = &quot;cv&quot;, number = 5) We first setup our cross-validation strategy, which will be 5 fold. We then use train() with method = &quot;glmnet&quot; which is actually fitting the elastic net. hit_elnet = train( Salary ~ ., data = Hitters, method = &quot;glmnet&quot;, trControl = cv_5 ) First, note that since we are using caret() directly, it is taking care of dummy variable creation. So unlike before when we used glmnet(), we do not need to manually create a model matrix. Also note that we have allowed caret to choose the tuning parameters for us. hit_elnet ## glmnet ## ## 263 samples ## 19 predictor ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 211, 211, 210, 210, 210 ## Resampling results across tuning parameters: ## ## alpha lambda RMSE Rsquared ## 0.10 0.5106 346.2 0.4628 ## 0.10 5.1056 342.6 0.4709 ## 0.10 51.0564 339.2 0.4767 ## 0.55 0.5106 346.7 0.4617 ## 0.55 5.1056 342.5 0.4697 ## 0.55 51.0564 338.2 0.4831 ## 1.00 0.5106 347.1 0.4606 ## 1.00 5.1056 342.6 0.4677 ## 1.00 51.0564 344.9 0.4703 ## ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were alpha = 0.55 and lambda = 51.06. Notice a few things with these results. First, we have tried three \\(\\alpha\\) values, 0.1, 0.55, and 1. It is not entirely clear why caret doesn’t use 0. It likely uses 0.1 to fit a model close to ridge, but with some potential for sparsity. Here, the best result uses \\(\\alpha = 0.55\\), so this result is somewhere between ridge and lasso. hit_elnet_int = train( Salary ~ . ^ 2, data = Hitters, method = &quot;glmnet&quot;, trControl = cv_5, tuneLength = 10 ) Now we try a much larger model search. First, we’re expanding the feature space to include all interactions. Since we are using penalized regression, we don’t have to worry as much about overfitting. If many of the added variables are not useful, we will likely use a model close to lasso which makes many of them 0. We’re also using a larger tuning grid. By setting tuneLength = 10, we will search 10 \\(\\alpha\\) values and 10 \\(\\lambda\\) values for each. Because of this larger tuning grid, the results will be very large. To deal with this, we write a quick helper function to extract the row with the best tuning parameters. get_best_result = function(caret_fit) { best_result = caret_fit$results[as.numeric(rownames(caret_fit$bestTune)), ] rownames(best_result) = NULL best_result } We then call this function on the trained object. get_best_result(hit_elnet_int) ## alpha lambda RMSE Rsquared RMSESD RsquaredSD ## 1 1 4.135 313.9 0.5476 29.05 0.1232 We see that the best result uses \\(\\alpha = 1\\), which makes since. With \\(\\alpha = 1\\), many of the added interaction coefficients are likely set to zero. (Unfortunately, obtaining this information after using caret with glmnet isn’t easy. The two don’t actually play very nice together. We’ll use cv.glmnet() with the expanded feature space to explore this.) Also, this CV-RMSE is better than the lasso and ridge from the previous chapter that did not use the expanded feature space. We also perform a quick analysis using cv.glmnet() instead. Due in part to randomness in cross validation, and differences in how cv.glmnet() and train() search for \\(\\lambda\\), the results are slightly different. set.seed(430) X = model.matrix(Salary ~ . ^ 2, Hitters)[, -1] y = Hitters$Salary fit_lasso_cv = cv.glmnet(X, y, alpha = 1) sqrt(fit_lasso_cv$cvm[fit_lasso_cv$lambda == fit_lasso_cv$lambda.min]) # CV-RMSE minimum ## [1] 304.1 The commented line is not run, since it produces a lot of output, but if run, it will show that the fast majority of the coefficients are zero! (Also, you’ll notice that cv.glmnet() does not respect the usual predictor hierarchy. Not a problem for prediction, but a massive interpretation issue!) #coef(fit_lasso_cv) 15.3 Elastic Net for Classification Above, we have performed a regression task. But like lasso and ridge, elastic net can also be used for classification by using the deviance instead of the residual sum of squares. This essentially happens automatically in caret if the response variable is a factor. We’ll test this using the familiar Default dataset, which we first test-train split. data(Default, package = &quot;ISLR&quot;) set.seed(430) default_idx = createDataPartition(Default$default, p = 0.75, list = FALSE) default_trn = Default[default_idx, ] default_tst = Default[-default_idx, ] We then fit an elastic net with a default tuning grid. def_elnet = train( default ~ ., data = default_trn, method = &quot;glmnet&quot;, trControl = cv_5 ) def_elnet ## glmnet ## ## 7501 samples ## 3 predictor ## 2 classes: &#39;No&#39;, &#39;Yes&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 6000, 6001, 6001, 6001, 6001 ## Resampling results across tuning parameters: ## ## alpha lambda Accuracy Kappa ## 0.10 0.0001242 0.9725 0.39713 ## 0.10 0.0012424 0.9725 0.36692 ## 0.10 0.0124239 0.9679 0.09249 ## 0.55 0.0001242 0.9727 0.40200 ## 0.55 0.0012424 0.9724 0.37378 ## 0.55 0.0124239 0.9685 0.12567 ## 1.00 0.0001242 0.9728 0.40289 ## 1.00 0.0012424 0.9724 0.38125 ## 1.00 0.0124239 0.9689 0.15106 ## ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were alpha = 1 and lambda = 0.0001242. Since the best model used \\(\\alpha = 1\\), this is a lasso model. We also try an expanded feature space, and a larger tuning grid. def_elnet_int = train( default ~ . ^ 2, data = default_trn, method = &quot;glmnet&quot;, trControl = cv_5, tuneLength = 10 ) Since the result here will return 100 models, we again use are helper function to simply extract the best result. get_best_result(def_elnet_int) ## alpha lambda Accuracy Kappa AccuracySD KappaSD ## 1 0.3 0.0008174 0.9732 0.4 0.001275 0.01547 Here we see \\(\\alpha = 0.3\\), which is a mix between ridge and lasso. accuracy = function(actual, predicted) { mean(actual == predicted) } Evaluating the test accuracy of this model, we obtain one of the highest accuracies for this dataset of all methods we have tried. # test acc accuracy(actual = default_tst$default, predicted = predict(def_elnet_int, newdata = default_tst)) ## [1] 0.9752 15.4 External Links glmnet Web Vingette - Details from the package developers. glmnet with caret - Some details on Elastic Net tuning in the caret package. 15.5 RMarkdown The RMarkdown file for this chapter can be found here. The file was created using R version 3.4.1 and the following packages: Base Packages, Attached ## [1] &quot;methods&quot; &quot;stats&quot; &quot;graphics&quot; &quot;grDevices&quot; &quot;utils&quot; &quot;datasets&quot; ## [7] &quot;base&quot; Additional Packages, Attached ## [1] &quot;glmnet&quot; &quot;foreach&quot; &quot;Matrix&quot; &quot;caret&quot; &quot;ggplot2&quot; &quot;lattice&quot; Additional Packages, Not Attached ## [1] &quot;Rcpp&quot; &quot;compiler&quot; &quot;nloptr&quot; &quot;plyr&quot; ## [5] &quot;class&quot; &quot;iterators&quot; &quot;tools&quot; &quot;digest&quot; ## [9] &quot;lme4&quot; &quot;evaluate&quot; &quot;tibble&quot; &quot;gtable&quot; ## [13] &quot;nlme&quot; &quot;mgcv&quot; &quot;rlang&quot; &quot;yaml&quot; ## [17] &quot;parallel&quot; &quot;SparseM&quot; &quot;e1071&quot; &quot;stringr&quot; ## [21] &quot;knitr&quot; &quot;MatrixModels&quot; &quot;stats4&quot; &quot;rprojroot&quot; ## [25] &quot;grid&quot; &quot;nnet&quot; &quot;rmarkdown&quot; &quot;bookdown&quot; ## [29] &quot;minqa&quot; &quot;reshape2&quot; &quot;car&quot; &quot;magrittr&quot; ## [33] &quot;backports&quot; &quot;scales&quot; &quot;codetools&quot; &quot;ModelMetrics&quot; ## [37] &quot;htmltools&quot; &quot;MASS&quot; &quot;splines&quot; &quot;pbkrtest&quot; ## [41] &quot;colorspace&quot; &quot;quantreg&quot; &quot;stringi&quot; &quot;lazyeval&quot; ## [45] &quot;munsell&quot; "],
["regularized-discriminant-analysis.html", "Chapter 16 Regularized Discriminant Analysis 16.1 Sonar Data 16.2 RDA 16.3 RDA with Grid Search 16.4 RDA with Random Search Search 16.5 Comparison to Elastic Net 16.6 Results 16.7 External Links 16.8 RMarkdown", " Chapter 16 Regularized Discriminant Analysis We now use the Sonar dataset from the mlbench package to explore a new regularization method, regularized discriminant analysis (RDA), which combines the LDA and QDA. This is similar to how elastic net combines the ridge and lasso. 16.1 Sonar Data # this is a temporary workaround for an issue with glmnet, Matrix, and R version 3.3.3 # see here: http://stackoverflow.com/questions/43282720/r-error-in-validobject-object-when-running-as-script-but-not-in-console library(methods) library(mlbench) library(caret) library(glmnet) library(klaR) data(Sonar) #View(Sonar) table(Sonar$Class) / nrow(Sonar) ## ## M R ## 0.5336538 0.4663462 ncol(Sonar) - 1 ## [1] 60 16.2 RDA Regularized discriminant analysis uses the same general setup as LDA and QDA but estimates the covariance in a new way, which combines the covariance of QDA \\((\\hat{\\Sigma}_k)\\) with the covariance of LDA \\((\\hat{\\Sigma})\\) using a tuning parameter \\(\\lambda\\). \\[ \\hat{\\Sigma}_k(\\lambda) = (1-\\lambda)\\hat{\\Sigma}_k + \\lambda \\hat{\\Sigma} \\] Using the rda() function from the klaR package, which caret utilizes, makes an additional modification to the covariance matrix, which also has a tuning parameter \\(\\gamma\\). \\[ \\hat{\\Sigma}_k(\\lambda,\\gamma) = (1 -\\gamma) \\hat{\\Sigma}_k(\\lambda) + \\gamma \\frac{1}{p} \\text{tr}(\\hat{\\Sigma}_k(\\lambda)) I \\] Both \\(\\gamma\\) and \\(\\lambda\\) can be thought of as mixing parameters, as they both take values between 0 and 1. For the four extremes of \\(\\gamma\\) and \\(\\lambda\\), the covariance structure reduces to special cases: \\((\\gamma=0, \\lambda=0)\\): QDA - individual covariance for each group. \\((\\gamma=0, \\lambda=1)\\): LDA - a common covariance matrix. \\((\\gamma=1, \\lambda=0)\\): Conditional independent variables - similar to Naive Bayes, but variable variances within group (main diagonal elements) are all equal. \\((\\gamma=1, \\lambda=1)\\): Classification using euclidean distance - as in previous case, but variances are the same for all groups. Objects are assigned to group with nearest mean. 16.3 RDA with Grid Search set.seed(1337) cv_5_grid = trainControl(method = &quot;cv&quot;, number = 5) set.seed(1337) fit_rda_grid = train(Class ~ ., data = Sonar, method = &quot;rda&quot;, trControl = cv_5_grid) fit_rda_grid ## Regularized Discriminant Analysis ## ## 208 samples ## 60 predictor ## 2 classes: &#39;M&#39;, &#39;R&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 166, 167, 166, 167, 166 ## Resampling results across tuning parameters: ## ## gamma lambda Accuracy Kappa ## 0.0 0.0 0.7261324 0.4397685 ## 0.0 0.5 0.7648084 0.5279282 ## 0.0 1.0 0.7406504 0.4796821 ## 0.5 0.0 0.7842044 0.5641761 ## 0.5 0.5 0.8130081 0.6226443 ## 0.5 1.0 0.7649245 0.5284504 ## 1.0 0.0 0.6873403 0.3728292 ## 1.0 0.5 0.6922184 0.3830140 ## 1.0 1.0 0.6922184 0.3829488 ## ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were gamma = 0.5 and lambda = 0.5. plot(fit_rda_grid) 16.4 RDA with Random Search Search set.seed(1337) cv_5_rand = trainControl(method = &quot;cv&quot;, number = 5, search = &quot;random&quot;) fit_rda_rand = train(Class ~ ., data = Sonar, method = &quot;rda&quot;, trControl = cv_5_rand, tuneLength = 9) fit_rda_rand ## Regularized Discriminant Analysis ## ## 208 samples ## 60 predictor ## 2 classes: &#39;M&#39;, &#39;R&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 166, 167, 166, 167, 166 ## Resampling results across tuning parameters: ## ## gamma lambda Accuracy Kappa ## 0.2091218 0.9853343 0.7986063 0.5944959 ## 0.2306276 0.8632831 0.8177700 0.6328588 ## 0.3223120 0.3194769 0.8275261 0.6509822 ## 0.5074480 0.8843909 0.7842044 0.5654024 ## 0.5274011 0.4747535 0.8178862 0.6323459 ## 0.6146998 0.6471883 0.7937282 0.5828269 ## 0.7031213 0.1969985 0.8178862 0.6304034 ## 0.7363932 0.2499440 0.8177700 0.6295909 ## 0.9860836 0.2297174 0.7207898 0.4384828 ## ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were gamma = 0.322312 and lambda ## = 0.3194769. ggplot(fit_rda_rand) 16.5 Comparison to Elastic Net set.seed(1337) fit_elnet_grid = train(Class ~ ., data = Sonar, method = &quot;glmnet&quot;, trControl = cv_5_grid, tuneLength = 10) set.seed(1337) fit_elnet_int_grid = train(Class ~ . ^ 2, data = Sonar, method = &quot;glmnet&quot;, trControl = cv_5_grid, tuneLength = 10) 16.6 Results get_best_result = function(caret_fit) { best_result = caret_fit$results[as.numeric(rownames(caret_fit$bestTune)), ] rownames(best_result) = NULL best_result } knitr::kable(rbind( get_best_result(fit_rda_grid), get_best_result(fit_rda_rand))) gamma lambda Accuracy Kappa AccuracySD KappaSD 0.500000 0.5000000 0.8130081 0.6226443 0.0553439 0.1099849 0.322312 0.3194769 0.8275261 0.6509822 0.0650432 0.1322546 knitr::kable(rbind( get_best_result(fit_elnet_grid), get_best_result(fit_elnet_int_grid))) alpha lambda Accuracy Kappa AccuracySD KappaSD 1.0 0.0350306 0.7984901 0.5953995 0.0652593 0.1311529 0.1 0.0243225 0.8321719 0.6617794 0.0744795 0.1480774 16.7 External Links Random Search for Hyper-Parameter Optimization - Paper justifying random tuning parameter search. Random Hyperparameter Search - Details on random tuning parameter search in caret. 16.8 RMarkdown The RMarkdown file for this chapter can be found here. The file was created using R version 3.4.1 and the following packages: Base Packages, Attached ## [1] &quot;methods&quot; &quot;stats&quot; &quot;graphics&quot; &quot;grDevices&quot; &quot;utils&quot; &quot;datasets&quot; ## [7] &quot;base&quot; Additional Packages, Attached ## [1] &quot;klaR&quot; &quot;MASS&quot; &quot;glmnet&quot; &quot;foreach&quot; &quot;Matrix&quot; &quot;caret&quot; &quot;ggplot2&quot; ## [8] &quot;lattice&quot; &quot;mlbench&quot; Additional Packages, Not Attached ## [1] &quot;Rcpp&quot; &quot;highr&quot; &quot;nloptr&quot; &quot;compiler&quot; ## [5] &quot;plyr&quot; &quot;class&quot; &quot;iterators&quot; &quot;tools&quot; ## [9] &quot;digest&quot; &quot;lme4&quot; &quot;evaluate&quot; &quot;tibble&quot; ## [13] &quot;gtable&quot; &quot;nlme&quot; &quot;mgcv&quot; &quot;rlang&quot; ## [17] &quot;parallel&quot; &quot;yaml&quot; &quot;SparseM&quot; &quot;e1071&quot; ## [21] &quot;stringr&quot; &quot;knitr&quot; &quot;MatrixModels&quot; &quot;combinat&quot; ## [25] &quot;stats4&quot; &quot;rprojroot&quot; &quot;grid&quot; &quot;nnet&quot; ## [29] &quot;rmarkdown&quot; &quot;bookdown&quot; &quot;minqa&quot; &quot;reshape2&quot; ## [33] &quot;car&quot; &quot;magrittr&quot; &quot;backports&quot; &quot;scales&quot; ## [37] &quot;codetools&quot; &quot;ModelMetrics&quot; &quot;htmltools&quot; &quot;splines&quot; ## [41] &quot;pbkrtest&quot; &quot;colorspace&quot; &quot;labeling&quot; &quot;quantreg&quot; ## [45] &quot;stringi&quot; &quot;lazyeval&quot; &quot;munsell&quot; "],
["non-linear-models.html", "Chapter 17 Non-Linear Models 17.1 Polynomial Regression 17.2 Logistic Regression, Polynomial Terms 17.3 Step Functions 17.4 Local Regression 17.5 Generalized Additive Models (GAMs) 17.6 External Links 17.7 RMarkdown", " Chapter 17 Non-Linear Models Some notes: Currently, most of this code is similar to that of the non-linear chapter of ISLR. It will likely change in the future. GAMs and caret have some issues working together! These are currently notes without narrative. library(ISLR) 17.1 Polynomial Regression fit_poly_4 = lm(wage ~ poly(age, 4), data = Wage) summary(fit_poly_4) ## ## Call: ## lm(formula = wage ~ poly(age, 4), data = Wage) ## ## Residuals: ## Min 1Q Median 3Q Max ## -98.707 -24.626 -4.993 15.217 203.693 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 111.7036 0.7287 153.283 &lt; 2e-16 *** ## poly(age, 4)1 447.0679 39.9148 11.201 &lt; 2e-16 *** ## poly(age, 4)2 -478.3158 39.9148 -11.983 &lt; 2e-16 *** ## poly(age, 4)3 125.5217 39.9148 3.145 0.00168 ** ## poly(age, 4)4 -77.9112 39.9148 -1.952 0.05104 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 39.91 on 2995 degrees of freedom ## Multiple R-squared: 0.08626, Adjusted R-squared: 0.08504 ## F-statistic: 70.69 on 4 and 2995 DF, p-value: &lt; 2.2e-16 fit_poly_4_raw = lm(wage ~ poly(age, 4, raw = TRUE), data = Wage) summary(fit_poly_4_raw) ## ## Call: ## lm(formula = wage ~ poly(age, 4, raw = TRUE), data = Wage) ## ## Residuals: ## Min 1Q Median 3Q Max ## -98.707 -24.626 -4.993 15.217 203.693 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.842e+02 6.004e+01 -3.067 0.002180 ** ## poly(age, 4, raw = TRUE)1 2.125e+01 5.887e+00 3.609 0.000312 *** ## poly(age, 4, raw = TRUE)2 -5.639e-01 2.061e-01 -2.736 0.006261 ** ## poly(age, 4, raw = TRUE)3 6.811e-03 3.066e-03 2.221 0.026398 * ## poly(age, 4, raw = TRUE)4 -3.204e-05 1.641e-05 -1.952 0.051039 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 39.91 on 2995 degrees of freedom ## Multiple R-squared: 0.08626, Adjusted R-squared: 0.08504 ## F-statistic: 70.69 on 4 and 2995 DF, p-value: &lt; 2.2e-16 coef(fit_poly_4) ## (Intercept) poly(age, 4)1 poly(age, 4)2 poly(age, 4)3 poly(age, 4)4 ## 111.70361 447.06785 -478.31581 125.52169 -77.91118 coef(fit_poly_4_raw) ## (Intercept) poly(age, 4, raw = TRUE)1 ## -1.841542e+02 2.124552e+01 ## poly(age, 4, raw = TRUE)2 poly(age, 4, raw = TRUE)3 ## -5.638593e-01 6.810688e-03 ## poly(age, 4, raw = TRUE)4 ## -3.203830e-05 plot(fitted(fit_poly_4), fitted(fit_poly_4_raw)) age_lower = range(Wage$age)[1] age_upper = range(Wage$age)[2] age_grid = seq(from = age_lower, to = age_upper, by = 1) age_pred = predict(fit_poly_4, newdata = data.frame(age = age_grid), se = TRUE) age_se_bands = cbind(age_pred$fit + 2 * age_pred$se.fit, age_pred$fit - 2 * age_pred$se.fit) plot(wage ~ age, data = Wage, cex = .5, col = &quot;darkgrey&quot;, xlab = &quot;Age&quot;, ylab = &quot;Wage&quot; ) lines(age_grid, age_pred$fit, lwd = 2, col = &quot;dodgerblue&quot;) matlines(age_grid, age_se_bands, lwd = 1, col = &quot;dodgerblue&quot;, lty = 3) 17.1.1 ANOVA fit_a = lm(wage ~ education, data = Wage) fit_b = lm(wage ~ education + age, data = Wage) fit_c = lm(wage ~ education + poly(age, 2), data = Wage) fit_d = lm(wage ~ education + poly(age, 3), data = Wage) anova(fit_a, fit_b, fit_c, fit_d) ## Analysis of Variance Table ## ## Model 1: wage ~ education ## Model 2: wage ~ education + age ## Model 3: wage ~ education + poly(age, 2) ## Model 4: wage ~ education + poly(age, 3) ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 2995 3995721 ## 2 2994 3867992 1 127729 102.7378 &lt;2e-16 *** ## 3 2993 3725395 1 142597 114.6969 &lt;2e-16 *** ## 4 2992 3719809 1 5587 4.4936 0.0341 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 17.2 Logistic Regression, Polynomial Terms glm_poly_4 = glm(I(wage &gt; 250) ~ poly(age, 4), data = Wage, family = binomial) summary(glm_poly_4) ## ## Call: ## glm(formula = I(wage &gt; 250) ~ poly(age, 4), family = binomial, ## data = Wage) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.3110 -0.2607 -0.2488 -0.1791 3.7859 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -4.3012 0.3451 -12.465 &lt; 2e-16 *** ## poly(age, 4)1 71.9642 26.1176 2.755 0.00586 ** ## poly(age, 4)2 -85.7729 35.9043 -2.389 0.01690 * ## poly(age, 4)3 34.1626 19.6890 1.735 0.08272 . ## poly(age, 4)4 -47.4008 24.0909 -1.968 0.04912 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 730.53 on 2999 degrees of freedom ## Residual deviance: 701.22 on 2995 degrees of freedom ## AIC: 711.22 ## ## Number of Fisher Scoring iterations: 9 glm_pred = predict(glm_poly_4, newdata = data.frame(age = age_grid), se = TRUE) glm_se_bands = cbind(fit = glm_pred$fit, lower = glm_pred$fit - 2 * glm_pred$se.fit, upper = glm_pred$fit + 2 * glm_pred$se.fit) glm_prob_bands = exp(glm_se_bands) / (1 + exp(glm_se_bands)) matplot(age_grid, glm_prob_bands, lwd = c(2, 1, 1), lty = c(1, 2, 2), type = &quot;l&quot;, col = &quot;dodgerblue&quot;, ylim = c(0, 0.1)) points(jitter(Wage$age), I((Wage$wage &gt; 250) / 10), cex = .5, pch = &quot;|&quot;, col = &quot;darkgrey&quot; ) 17.3 Step Functions table(cut(Wage$age, 4)) ## ## (17.9,33.5] (33.5,49] (49,64.5] (64.5,80.1] ## 750 1399 779 72 step_fit = lm(wage ~ cut(age, 4), data = Wage) age_pred = predict(step_fit, newdata = data.frame(age = age_grid), se = TRUE) coef(summary(step_fit)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 94.158392 1.476069 63.789970 0.000000e+00 ## cut(age, 4)(33.5,49] 24.053491 1.829431 13.148074 1.982315e-38 ## cut(age, 4)(49,64.5] 23.664559 2.067958 11.443444 1.040750e-29 ## cut(age, 4)(64.5,80.1] 7.640592 4.987424 1.531972 1.256350e-01 plot(wage ~ age, data = Wage, cex = .5, col = &quot;darkgrey&quot;, xlab = &quot;Age&quot;, ylab = &quot;Wage&quot; ) lines(age_grid, age_pred$fit, col = &quot;dodgerblue&quot;, lwd = 3) 17.3.1 Smoothing Splines library(splines) ss_age = smooth.spline(Wage$age, Wage$wage, df = 28) plot(wage ~ age, data = Wage, cex = .5, col = &quot;darkgrey&quot;, xlab = &quot;Age&quot;, ylab = &quot;Wage&quot; ) lines(ss_age, col = &quot;darkorange&quot;, lwd = 2) ss_age_cv = smooth.spline(Wage$age, Wage$wage, cv = TRUE) ss_age_cv ## Call: ## smooth.spline(x = Wage$age, y = Wage$wage, cv = TRUE) ## ## Smoothing Parameter spar= 0.6988943 lambda= 0.02792303 (12 iterations) ## Equivalent Degrees of Freedom (Df): 6.794596 ## Penalized Criterion (RSS): 75215.9 ## PRESS(l.o.o. CV): 1593.383 lines(ss_age_cv, col = &quot;dodgerblue&quot;, lwd = 2) 17.4 Local Regression plot(wage ~ age, data = Wage, cex = .5, col = &quot;darkgrey&quot;, xlab = &quot;Age&quot;, ylab = &quot;Wage&quot; ) title(&quot;Local Regression&quot;) local_span_01 = loess(wage ~ age, span = .1, data = Wage) local_span_09 = loess(wage ~ age, span = .9, data = Wage) lines(age_grid, predict(local_span_01, data.frame(age = age_grid)), col = &quot;darkorange&quot;, lwd = 2) lines(age_grid, predict(local_span_09, data.frame(age = age_grid)), col = &quot;dodgerblue&quot;, lwd = 2) legend(&quot;topright&quot;, legend = c(&quot;Span = 0.1&quot;, &quot;Span = 0.9&quot;), col = c(&quot;darkorange&quot;, &quot;dodgerblue&quot;), lty = 1, lwd = 2, cex = .8) 17.5 Generalized Additive Models (GAMs) library(gam) ## Loading required package: foreach ## Loaded gam 1.14-4 gam_fit = gam(wage ~ s(age, 4) + s(year, 4) + education, data = Wage) par(mfrow = c(1, 3)) plot(gam_fit, se = TRUE, col = &quot;darkorange&quot;, lwd = 2) gam_fit_small = gam(wage ~ s(age, 4) + education, data = Wage) anova(gam_fit_small, gam_fit, test = &quot;F&quot;) ## Analysis of Deviance Table ## ## Model 1: wage ~ s(age, 4) + education ## Model 2: wage ~ s(age, 4) + s(year, 4) + education ## Resid. Df Resid. Dev Df Deviance F Pr(&gt;F) ## 1 2991 3714465 ## 2 2987 3692824 4 21641 4.3761 0.001573 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 gam_log = gam(I(wage &gt; 250) ~ s(age, 4) + s(year, 4) + education, family = binomial, data = Wage) par(mfrow = c(1, 3)) plot(gam_log) par(mfrow = c(1, 3)) plot(gam_log, se = TRUE, col = &quot;dodgerblue&quot;) 17.5.1 GAMs in caret set.seed(430) library(caret) library(MASS) bos_idx = createDataPartition(Boston$medv, p = 0.75, list = FALSE) bos_trn = Boston[bos_idx, ] bos_tst = Boston[-bos_idx, ] cv_5 = trainControl(method = &quot;cv&quot;, number = 5) gam_grid = expand.grid(df = 1:10) gam_train = train(medv ~ ., data = bos_trn, trControl = cv_5, method = &quot;gamSpline&quot;, tuneGrid = gam_grid) plot(gam_train) gam_train ## Generalized Additive Model using Splines ## ## 381 samples ## 13 predictor ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 306, 304, 306, 304, 304 ## Resampling results across tuning parameters: ## ## df RMSE Rsquared ## 1 4.804025 0.7249995 ## 2 3.885811 0.8137612 ## 3 3.737718 0.8278223 ## 4 3.719837 0.8296394 ## 5 3.739772 0.8277413 ## 6 3.789250 0.8229085 ## 7 3.858568 0.8160195 ## 8 3.940364 0.8078791 ## 9 4.028412 0.7993586 ## 10 4.116986 0.7912003 ## ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was df = 4. 17.6 External Links GAM: The Predictive Modeling Silver Bullet 17.7 RMarkdown The RMarkdown file for this chapter can be found here. The file was created using R version 3.4.1 and the following packages: Base Packages, Attached ## [1] &quot;methods&quot; &quot;splines&quot; &quot;stats&quot; &quot;graphics&quot; &quot;grDevices&quot; &quot;utils&quot; ## [7] &quot;datasets&quot; &quot;base&quot; Additional Packages, Attached ## [1] &quot;MASS&quot; &quot;caret&quot; &quot;ggplot2&quot; &quot;lattice&quot; &quot;gam&quot; &quot;foreach&quot; &quot;ISLR&quot; Additional Packages, Not Attached ## [1] &quot;Rcpp&quot; &quot;nloptr&quot; &quot;compiler&quot; &quot;plyr&quot; ## [5] &quot;iterators&quot; &quot;tools&quot; &quot;digest&quot; &quot;lme4&quot; ## [9] &quot;evaluate&quot; &quot;tibble&quot; &quot;gtable&quot; &quot;nlme&quot; ## [13] &quot;mgcv&quot; &quot;rlang&quot; &quot;Matrix&quot; &quot;parallel&quot; ## [17] &quot;yaml&quot; &quot;SparseM&quot; &quot;stringr&quot; &quot;knitr&quot; ## [21] &quot;MatrixModels&quot; &quot;stats4&quot; &quot;rprojroot&quot; &quot;grid&quot; ## [25] &quot;nnet&quot; &quot;rmarkdown&quot; &quot;bookdown&quot; &quot;minqa&quot; ## [29] &quot;reshape2&quot; &quot;car&quot; &quot;magrittr&quot; &quot;backports&quot; ## [33] &quot;scales&quot; &quot;codetools&quot; &quot;ModelMetrics&quot; &quot;htmltools&quot; ## [37] &quot;pbkrtest&quot; &quot;colorspace&quot; &quot;quantreg&quot; &quot;stringi&quot; ## [41] &quot;lazyeval&quot; &quot;munsell&quot; "],
["trees.html", "Chapter 18 Trees 18.1 Classification Trees 18.2 Regression Trees 18.3 rpart Package 18.4 External Links 18.5 RMarkdown", " Chapter 18 Trees library(tree) In this document, we will use the package tree for both classification and regression trees. Note that there are many packages to do this in R. rpart may be the most common, however, we will use tree for simplicity. 18.1 Classification Trees library(ISLR) To understand classification trees, we will use the Carseat dataset from the ISLR package. We will first modify the response variable Sales from its original use as a numerical variable, to a categorical variable with High for high sales, and Low for low sales. data(Carseats) #?Carseats str(Carseats) ## &#39;data.frame&#39;: 400 obs. of 11 variables: ## $ Sales : num 9.5 11.22 10.06 7.4 4.15 ... ## $ CompPrice : num 138 111 113 117 141 124 115 136 132 132 ... ## $ Income : num 73 48 35 100 64 113 105 81 110 113 ... ## $ Advertising: num 11 16 10 4 3 13 0 15 0 0 ... ## $ Population : num 276 260 269 466 340 501 45 425 108 131 ... ## $ Price : num 120 83 80 97 128 72 108 120 124 124 ... ## $ ShelveLoc : Factor w/ 3 levels &quot;Bad&quot;,&quot;Good&quot;,&quot;Medium&quot;: 1 2 3 3 1 1 3 2 3 3 ... ## $ Age : num 42 65 59 55 38 78 71 67 76 76 ... ## $ Education : num 17 10 12 14 13 16 15 10 10 17 ... ## $ Urban : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 2 2 2 1 2 2 1 1 ... ## $ US : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 2 2 1 2 1 2 1 2 ... Carseats$Sales = as.factor(ifelse(Carseats$Sales &lt;= 8, &quot;Low&quot;, &quot;High&quot;)) str(Carseats) ## &#39;data.frame&#39;: 400 obs. of 11 variables: ## $ Sales : Factor w/ 2 levels &quot;High&quot;,&quot;Low&quot;: 1 1 1 2 2 1 2 1 2 2 ... ## $ CompPrice : num 138 111 113 117 141 124 115 136 132 132 ... ## $ Income : num 73 48 35 100 64 113 105 81 110 113 ... ## $ Advertising: num 11 16 10 4 3 13 0 15 0 0 ... ## $ Population : num 276 260 269 466 340 501 45 425 108 131 ... ## $ Price : num 120 83 80 97 128 72 108 120 124 124 ... ## $ ShelveLoc : Factor w/ 3 levels &quot;Bad&quot;,&quot;Good&quot;,&quot;Medium&quot;: 1 2 3 3 1 1 3 2 3 3 ... ## $ Age : num 42 65 59 55 38 78 71 67 76 76 ... ## $ Education : num 17 10 12 14 13 16 15 10 10 17 ... ## $ Urban : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 2 2 2 1 2 2 1 1 ... ## $ US : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 2 2 1 2 1 2 1 2 ... We first fit an unpruned classification tree using all of the predictors. Details of this process can be found using ?tree and ?tree.control seat_tree = tree(Sales ~ ., data = Carseats) # seat_tree = tree(Sales ~ ., data = Carseats, # control = tree.control(nobs = nrow(Carseats), minsize = 10)) summary(seat_tree) ## ## Classification tree: ## tree(formula = Sales ~ ., data = Carseats) ## Variables actually used in tree construction: ## [1] &quot;ShelveLoc&quot; &quot;Price&quot; &quot;US&quot; &quot;Income&quot; &quot;CompPrice&quot; ## [6] &quot;Population&quot; &quot;Advertising&quot; &quot;Age&quot; ## Number of terminal nodes: 27 ## Residual mean deviance: 0.4575 = 170.7 / 373 ## Misclassification error rate: 0.09 = 36 / 400 We see this tree has 27 terminal nodes and a misclassification rate of 0.09. plot(seat_tree) text(seat_tree, pretty = 0) title(main = &quot;Unpruned Classification Tree&quot;) Above we plot the tree. Below we output the details of the splits. seat_tree ## node), split, n, deviance, yval, (yprob) ## * denotes terminal node ## ## 1) root 400 541.500 Low ( 0.41000 0.59000 ) ## 2) ShelveLoc: Good 85 90.330 High ( 0.77647 0.22353 ) ## 4) Price &lt; 135 68 49.260 High ( 0.88235 0.11765 ) ## 8) US: No 17 22.070 High ( 0.64706 0.35294 ) ## 16) Price &lt; 109 8 0.000 High ( 1.00000 0.00000 ) * ## 17) Price &gt; 109 9 11.460 Low ( 0.33333 0.66667 ) * ## 9) US: Yes 51 16.880 High ( 0.96078 0.03922 ) * ## 5) Price &gt; 135 17 22.070 Low ( 0.35294 0.64706 ) ## 10) Income &lt; 46 6 0.000 Low ( 0.00000 1.00000 ) * ## 11) Income &gt; 46 11 15.160 High ( 0.54545 0.45455 ) * ## 3) ShelveLoc: Bad,Medium 315 390.600 Low ( 0.31111 0.68889 ) ## 6) Price &lt; 92.5 46 56.530 High ( 0.69565 0.30435 ) ## 12) Income &lt; 57 10 12.220 Low ( 0.30000 0.70000 ) ## 24) CompPrice &lt; 110.5 5 0.000 Low ( 0.00000 1.00000 ) * ## 25) CompPrice &gt; 110.5 5 6.730 High ( 0.60000 0.40000 ) * ## 13) Income &gt; 57 36 35.470 High ( 0.80556 0.19444 ) ## 26) Population &lt; 207.5 16 21.170 High ( 0.62500 0.37500 ) * ## 27) Population &gt; 207.5 20 7.941 High ( 0.95000 0.05000 ) * ## 7) Price &gt; 92.5 269 299.800 Low ( 0.24535 0.75465 ) ## 14) Advertising &lt; 13.5 224 213.200 Low ( 0.18304 0.81696 ) ## 28) CompPrice &lt; 124.5 96 44.890 Low ( 0.06250 0.93750 ) ## 56) Price &lt; 106.5 38 33.150 Low ( 0.15789 0.84211 ) ## 112) Population &lt; 177 12 16.300 Low ( 0.41667 0.58333 ) ## 224) Income &lt; 60.5 6 0.000 Low ( 0.00000 1.00000 ) * ## 225) Income &gt; 60.5 6 5.407 High ( 0.83333 0.16667 ) * ## 113) Population &gt; 177 26 8.477 Low ( 0.03846 0.96154 ) * ## 57) Price &gt; 106.5 58 0.000 Low ( 0.00000 1.00000 ) * ## 29) CompPrice &gt; 124.5 128 150.200 Low ( 0.27344 0.72656 ) ## 58) Price &lt; 122.5 51 70.680 High ( 0.50980 0.49020 ) ## 116) ShelveLoc: Bad 11 6.702 Low ( 0.09091 0.90909 ) * ## 117) ShelveLoc: Medium 40 52.930 High ( 0.62500 0.37500 ) ## 234) Price &lt; 109.5 16 7.481 High ( 0.93750 0.06250 ) * ## 235) Price &gt; 109.5 24 32.600 Low ( 0.41667 0.58333 ) ## 470) Age &lt; 49.5 13 16.050 High ( 0.69231 0.30769 ) * ## 471) Age &gt; 49.5 11 6.702 Low ( 0.09091 0.90909 ) * ## 59) Price &gt; 122.5 77 55.540 Low ( 0.11688 0.88312 ) ## 118) CompPrice &lt; 147.5 58 17.400 Low ( 0.03448 0.96552 ) * ## 119) CompPrice &gt; 147.5 19 25.010 Low ( 0.36842 0.63158 ) ## 238) Price &lt; 147 12 16.300 High ( 0.58333 0.41667 ) ## 476) CompPrice &lt; 152.5 7 5.742 High ( 0.85714 0.14286 ) * ## 477) CompPrice &gt; 152.5 5 5.004 Low ( 0.20000 0.80000 ) * ## 239) Price &gt; 147 7 0.000 Low ( 0.00000 1.00000 ) * ## 15) Advertising &gt; 13.5 45 61.830 High ( 0.55556 0.44444 ) ## 30) Age &lt; 54.5 25 25.020 High ( 0.80000 0.20000 ) ## 60) CompPrice &lt; 130.5 14 18.250 High ( 0.64286 0.35714 ) ## 120) Income &lt; 100 9 12.370 Low ( 0.44444 0.55556 ) * ## 121) Income &gt; 100 5 0.000 High ( 1.00000 0.00000 ) * ## 61) CompPrice &gt; 130.5 11 0.000 High ( 1.00000 0.00000 ) * ## 31) Age &gt; 54.5 20 22.490 Low ( 0.25000 0.75000 ) ## 62) CompPrice &lt; 122.5 10 0.000 Low ( 0.00000 1.00000 ) * ## 63) CompPrice &gt; 122.5 10 13.860 Low ( 0.50000 0.50000 ) ## 126) Price &lt; 125 5 0.000 High ( 1.00000 0.00000 ) * ## 127) Price &gt; 125 5 0.000 Low ( 0.00000 1.00000 ) * We now test-train split the data so we can evaluate how well our tree is working. We use 200 observations for each. dim(Carseats) ## [1] 400 11 set.seed(2) seat_idx = sample(1:nrow(Carseats), 200) seat_trn = Carseats[seat_idx,] seat_tst = Carseats[-seat_idx,] seat_tree = tree(Sales ~ ., data = seat_trn) summary(seat_tree) ## ## Classification tree: ## tree(formula = Sales ~ ., data = seat_trn) ## Variables actually used in tree construction: ## [1] &quot;ShelveLoc&quot; &quot;Price&quot; &quot;Population&quot; &quot;Advertising&quot; &quot;Income&quot; ## [6] &quot;Age&quot; &quot;CompPrice&quot; ## Number of terminal nodes: 19 ## Residual mean deviance: 0.4282 = 77.51 / 181 ## Misclassification error rate: 0.105 = 21 / 200 Note that, the tree is not using all of the available variables. summary(seat_tree)$used ## [1] ShelveLoc Price Population Advertising Income Age ## [7] CompPrice ## 11 Levels: &lt;leaf&gt; CompPrice Income Advertising Population ... US names(Carseats)[which(!(names(Carseats) %in%summary(seat_tree)$used))] ## [1] &quot;Sales&quot; &quot;Education&quot; &quot;Urban&quot; &quot;US&quot; Also notice that, this new tree is slightly different than the tree fit to all of the data. plot(seat_tree) text(seat_tree, pretty = 0) title(main = &quot;Unpruned Classification Tree&quot;) When using the predict() function on a tree, the default type is vector which gives predicted probabilities for both classes. We will use type = class to directly obtain classes. We first fit the tree using the training data (above), then obtain predictions on both the train and test set, then view the confusion matrix for both. seat_trn_pred = predict(seat_tree, seat_trn, type = &quot;class&quot;) seat_tst_pred = predict(seat_tree, seat_tst, type = &quot;class&quot;) #predict(seat_tree, seat_trn, type = &quot;vector&quot;) #predict(seat_tree, seat_tst, type = &quot;vector&quot;) # train confusion table(predicted = seat_trn_pred, actual = seat_trn$Sales) ## actual ## predicted High Low ## High 66 10 ## Low 14 110 # test confusion table(predicted = seat_tst_pred, actual = seat_tst$Sales) ## actual ## predicted High Low ## High 57 29 ## Low 27 87 accuracy = function(actual, predicted) { mean(actual == predicted) } # train acc accuracy(predicted = seat_trn_pred, actual = seat_trn$Sales) ## [1] 0.88 # test acc accuracy(predicted = seat_tst_pred, actual = seat_tst$Sales) ## [1] 0.72 Here it is easy to see that the tree has been over-fit. The train set performs much better than the test set. We will now use cross-validation to find a tree by considering trees of different sizes which have been pruned from our original tree. set.seed(3) seat_tree_cv = cv.tree(seat_tree, FUN = prune.misclass) # index of tree with minimum error min_idx = which.min(seat_tree_cv$dev) min_idx ## [1] 5 # number of terminal nodes in that tree seat_tree_cv$size[min_idx] ## [1] 9 # misclassification rate of each tree seat_tree_cv$dev / length(seat_idx) ## [1] 0.275 0.275 0.265 0.260 0.250 0.280 0.345 0.325 0.400 par(mfrow = c(1, 2)) # default plot plot(seat_tree_cv) # better plot plot(seat_tree_cv$size, seat_tree_cv$dev / nrow(seat_trn), type = &quot;b&quot;, xlab = &quot;Tree Size&quot;, ylab = &quot;CV Misclassification Rate&quot;) It appears that a tree of size 9 has the fewest misclassifications of the considered trees, via cross-validation. We use prune.misclass() to obtain that tree from our original tree, and plot this smaller tree. seat_tree_prune = prune.misclass(seat_tree, best = 9) summary(seat_tree_prune) ## ## Classification tree: ## snip.tree(tree = seat_tree, nodes = c(223L, 4L, 12L, 54L)) ## Variables actually used in tree construction: ## [1] &quot;ShelveLoc&quot; &quot;Price&quot; &quot;Advertising&quot; &quot;Age&quot; &quot;CompPrice&quot; ## Number of terminal nodes: 9 ## Residual mean deviance: 0.8103 = 154.8 / 191 ## Misclassification error rate: 0.155 = 31 / 200 plot(seat_tree_prune) text(seat_tree_prune, pretty = 0) title(main = &quot;Pruned Classification Tree&quot;) We again obtain predictions using this smaller tree, and evaluate on the test and train sets. # train seat_prune_trn_pred = predict(seat_tree_prune, seat_trn, type = &quot;class&quot;) table(predicted = seat_prune_trn_pred, actual = seat_trn$Sales) ## actual ## predicted High Low ## High 59 10 ## Low 21 110 accuracy(predicted = seat_prune_trn_pred, actual = seat_trn$Sales) ## [1] 0.845 # test seat_prune_tst_pred = predict(seat_tree_prune, seat_tst, type = &quot;class&quot;) table(predicted = seat_prune_tst_pred, actual = seat_tst$Sales) ## actual ## predicted High Low ## High 60 22 ## Low 24 94 accuracy(predicted = seat_prune_tst_pred, actual = seat_tst$Sales) ## [1] 0.77 The train set has performed almost as well as before, and there was a small improvement in the test set, but it is still obvious that we have over-fit. Trees tend to do this. We will look at several ways to fix this, including: bagging, boosting and random forests. 18.2 Regression Trees To demonstrate regression trees, we will use the Boston data. Recall medv is the response. We first split the data in half. library(MASS) set.seed(18) boston_idx = sample(1:nrow(Boston), nrow(Boston) / 2) boston_trn = Boston[boston_idx,] boston_tst = Boston[-boston_idx,] Then fit an unpruned regression tree to the training data. boston_tree = tree(medv ~ ., data = boston_trn) summary(boston_tree) ## ## Regression tree: ## tree(formula = medv ~ ., data = boston_trn) ## Variables actually used in tree construction: ## [1] &quot;rm&quot; &quot;lstat&quot; &quot;crim&quot; ## Number of terminal nodes: 9 ## Residual mean deviance: 12.35 = 3013 / 244 ## Distribution of residuals: ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -13.600 -1.832 -0.120 0.000 1.348 26.350 plot(boston_tree) text(boston_tree, pretty = 0) title(main = &quot;Unpruned Regression Tree&quot;) As with classification trees, we can use cross-validation to select a good pruning of the tree. set.seed(18) boston_tree_cv = cv.tree(boston_tree) plot(boston_tree_cv$size, sqrt(boston_tree_cv$dev / nrow(boston_trn)), type = &quot;b&quot;, xlab = &quot;Tree Size&quot;, ylab = &quot;CV-RMSE&quot;) While the tree of size 9 does have the lowest RMSE, we’ll prune to a size of 7 as it seems to perform just as well. (Otherwise we would not be pruning.) The pruned tree is, as expected, smaller and easier to interpret. boston_tree_prune = prune.tree(boston_tree, best = 7) summary(boston_tree_prune) ## ## Regression tree: ## snip.tree(tree = boston_tree, nodes = c(11L, 8L)) ## Variables actually used in tree construction: ## [1] &quot;rm&quot; &quot;lstat&quot; &quot;crim&quot; ## Number of terminal nodes: 7 ## Residual mean deviance: 14.05 = 3455 / 246 ## Distribution of residuals: ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -13.60000 -2.12000 0.01731 0.00000 1.88000 28.02000 plot(boston_tree_prune) text(boston_tree_prune, pretty = 0) title(main = &quot;Pruned Regression Tree&quot;) Let’s compare this regression tree to an additive linear model and use RMSE as our metric. rmse = function(actual, predicted) { sqrt(mean((actual - predicted) ^ 2)) } We obtain predictions on the train and test sets from the pruned tree. We also plot actual vs predicted. This plot may look odd. We’ll compare it to a plot for linear regression below. # training RMSE two ways sqrt(summary(boston_tree_prune)$dev / nrow(boston_trn)) ## [1] 3.695598 boston_prune_trn_pred = predict(boston_tree_prune, newdata = boston_trn) rmse(boston_prune_trn_pred, boston_trn$medv) ## [1] 3.695598 # test RMSE boston_prune_tst_pred = predict(boston_tree_prune, newdata = boston_tst) rmse(boston_prune_tst_pred, boston_tst$medv) ## [1] 5.331457 plot(boston_prune_tst_pred, boston_tst$medv, xlab = &quot;Predicted&quot;, ylab = &quot;Actual&quot;) abline(0, 1) Here, using an additive linear regression the actual vs predicted looks much more like what we are used to. bostom_lm = lm(medv ~ ., data = boston_trn) boston_lm_pred = predict(bostom_lm, newdata = boston_tst) plot(boston_lm_pred, boston_tst$medv, xlab = &quot;Predicted&quot;, ylab = &quot;Actual&quot;) abline(0, 1) rmse(boston_lm_pred, boston_tst$medv) ## [1] 5.125877 We also see a lower test RMSE. The most obvious linear regression beats the tree! Again, we’ll improve on this tree soon. Also note the summary of the additive linear regression below. Which is easier to interpret, that output, or the small tree above? coef(bostom_lm) ## (Intercept) crim zn indus chas ## 43.340158284 -0.113490889 0.046881038 0.018046856 3.557944155 ## nox rm age dis rad ## -21.904534125 3.486780787 -0.010592511 -1.766227892 0.354167931 ## tax ptratio black lstat ## -0.015036451 -0.830144898 0.003722857 -0.576134200 18.3 rpart Package The rpart package is an alternative method for fitting trees in R. It is much more feature rich, including fitting multiple cost complexities and performing cross-validation by default. It also has the ability to produce much nicer trees. Based on its default settings, it will often result in smaller trees than using the tree package. See the references below for more information. rpart can also be tuned via caret. library(rpart) set.seed(430) # Fit a decision tree using rpart # Note: when you fit a tree using rpart, the fitting routine automatically # performs 10-fold CV and stores the errors for later use # (such as for pruning the tree) # fit a tree using rpart seat_rpart = rpart(Sales ~ ., data = seat_trn, method = &quot;class&quot;) # plot the cv error curve for the tree # rpart tries different cost-complexities by default # also stores cv results plotcp(seat_rpart) # find best value of cp min_cp = seat_rpart$cptable[which.min(seat_rpart$cptable[,&quot;xerror&quot;]),&quot;CP&quot;] min_cp ## [1] 0.02083333 # prunce tree using best cp seat_rpart_prune = prune(seat_rpart, cp = min_cp) # nicer plots library(rpart.plot) prp(seat_rpart_prune) prp(seat_rpart_prune, type = 4) rpart.plot(seat_rpart_prune) 18.4 External Links An Introduction to Recursive Partitioning Using the rpart Routines - Details of the rpart package. rpart.plot Package - Detailed manual on plotting with rpart using the rpart.plot package. 18.5 RMarkdown The RMarkdown file for this chapter can be found here. The file was created using R version 3.4.1 and the following packages: Base Packages, Attached ## [1] &quot;stats&quot; &quot;graphics&quot; &quot;grDevices&quot; &quot;utils&quot; &quot;datasets&quot; &quot;base&quot; Additional Packages, Attached ## [1] &quot;rpart.plot&quot; &quot;rpart&quot; &quot;MASS&quot; &quot;ISLR&quot; &quot;tree&quot; Additional Packages, Not Attached ## [1] &quot;Rcpp&quot; &quot;bookdown&quot; &quot;digest&quot; &quot;rprojroot&quot; &quot;backports&quot; ## [6] &quot;magrittr&quot; &quot;evaluate&quot; &quot;stringi&quot; &quot;rmarkdown&quot; &quot;tools&quot; ## [11] &quot;stringr&quot; &quot;yaml&quot; &quot;compiler&quot; &quot;htmltools&quot; &quot;knitr&quot; ## [16] &quot;methods&quot; "],
["ensemble-methods.html", "Chapter 19 Ensemble Methods 19.1 Regression 19.2 Classification 19.3 Tuning 19.4 Tree versus Ensemble Boundaries 19.5 External Links 19.6 RMarkdown", " Chapter 19 Ensemble Methods We’ll now consider ensembles of trees. 19.1 Regression We first consider the regression case, using the Boston data from the MASS package. We will use RMSE as our metric, so we write a function which will help us along the way. rmse = function(actual, predicted) { sqrt(mean((actual - predicted) ^ 2)) } We also load all of the packages that we will need. library(tree) library(MASS) library(ISLR) library(randomForest) library(gbm) library(caret) We first test-train split the data and fit the same pruned tree as before. (Note: When pruning the tree, the best tree is actually the unpruned tree. View the results of cv.tree to see this. However, we select the tree of size 7 as the best of the pruned trees.) set.seed(18) boston_idx = sample(1:nrow(Boston), nrow(Boston) / 2) boston_trn = Boston[boston_idx,] boston_tst = Boston[-boston_idx,] 19.1.1 Tree Model boston_tree = tree(medv ~ ., data = boston_trn) set.seed(18) boston_tree_cv = cv.tree(boston_tree) boston_tree_prune = prune.tree(boston_tree, best = 7) boston_prune_tst_pred = predict(boston_tree_prune, newdata = boston_tst) plot(boston_prune_tst_pred, boston_tst$medv, xlab = &quot;Predicted&quot;, ylab = &quot;Actual&quot;, main = &quot;Predicted vs Actual: Tree, Test Data&quot;) abline(0, 1, col = &quot;red&quot;, lwd = 2) (tree_tst_rmse = rmse(boston_prune_tst_pred, boston_tst$medv)) ## [1] 5.331457 19.1.2 Linear Model Last time, we also fit an additive linear model, which we found to work better than the tree. The test RMSE is lower, and the predicted vs actual plot looks much better. bostom_lm = lm(medv ~ ., data = boston_trn) boston_lm_tst_pred = predict(bostom_lm, newdata = boston_tst) plot(boston_lm_tst_pred, boston_tst$medv, xlab = &quot;Predicted&quot;, ylab = &quot;Actual&quot;, main = &quot;Predicted vs Actual: Linear Model, Test Data&quot; ) abline(0, 1, col = &quot;red&quot;, lwd = 2) (lm_tst_rmse = rmse(boston_lm_tst_pred, boston_tst$medv)) ## [1] 5.125877 19.1.3 Bagging We now fit a bagged model, using the randomForest package. Bagging is actually a special case of a random forest where mtry is equal to \\(p\\), the number of predictors. boston_bag = randomForest(medv ~ ., data = boston_trn, mtry = 13, importance = TRUE, ntrees = 500) boston_bag ## ## Call: ## randomForest(formula = medv ~ ., data = boston_trn, mtry = 13, importance = TRUE, ntrees = 500) ## Type of random forest: regression ## Number of trees: 500 ## No. of variables tried at each split: 13 ## ## Mean of squared residuals: 14.20299 ## % Var explained: 80.92 boston_bag_tst_pred = predict(boston_bag, newdata = boston_tst) plot(boston_bag_tst_pred,boston_tst$medv, xlab = &quot;Predicted&quot;, ylab = &quot;Actual&quot;, main = &quot;Predicted vs Actual: Bagged Model, Test Data&quot; ) abline(0, 1, col = &quot;red&quot;, lwd = 2) (bag_tst_rmse = rmse(boston_bag_tst_pred, boston_tst$medv)) ## [1] 3.814368 Here we see two interesting results. First, the predicted versus actual plot no longer has a small number of predicted values. Second, our test error has dropped dramatically. Also note that the “Mean of squared residuals” which is output by randomForest is the Out of Bag estimate of the error. plot(boston_bag) 19.1.4 Random Forest We now try a random forest. For regression, the suggestion is to use mtry equal to \\(p/3\\). boston_forest = randomForest(medv ~ ., data = boston_trn, mtry = 4, importance = TRUE, ntrees = 500) boston_forest ## ## Call: ## randomForest(formula = medv ~ ., data = boston_trn, mtry = 4, importance = TRUE, ntrees = 500) ## Type of random forest: regression ## Number of trees: 500 ## No. of variables tried at each split: 4 ## ## Mean of squared residuals: 12.80737 ## % Var explained: 82.79 #importance(boston_forest) #varImpPlot(boston_forest) boston_forest_tst_pred = predict(boston_forest, newdata = boston_tst) plot(boston_forest_tst_pred, boston_tst$medv, xlab = &quot;Predicted&quot;, ylab = &quot;Actual&quot;, main = &quot;Predicted vs Actual: Random Forest, Test Data&quot; ) abline(0, 1, col = &quot;red&quot;, lwd = 2) (forest_tst_rmse = rmse(boston_forest_tst_pred, boston_tst$medv)) ## [1] 3.73447 boston_forest_trn_pred = predict(boston_forest, newdata = boston_trn) forest_trn_rmse = rmse(boston_forest_trn_pred, boston_trn$medv) forest_oob_rmse = rmse(boston_forest$predicted, boston_trn$medv) Here we note three RMSEs. The training RMSE (which is optimistic), the OOB RMSE (which is a good estimate of the test error) and the test RMSE. Also note that variables importance was calculated, however, the results are not shown here. (The code to view the results is commented out.) ## Data Error ## 1 Training 1.563253 ## 2 OOB 3.578738 ## 3 Test 3.734470 19.1.5 Boosting Lastly, we try a boosted model, which by default will produce a nice variable importance plot as well as plots of the marginal effects of the predictors. We use the gbm package. booston_boost = gbm(medv ~ ., data = boston_trn, distribution = &quot;gaussian&quot;, n.trees = 5000, interaction.depth = 4, shrinkage = 0.01) booston_boost ## gbm(formula = medv ~ ., distribution = &quot;gaussian&quot;, data = boston_trn, ## n.trees = 5000, interaction.depth = 4, shrinkage = 0.01) ## A gradient boosted model with gaussian loss function. ## 5000 iterations were performed. ## There were 13 predictors of which 13 had non-zero influence. summary(booston_boost) ## var rel.inf ## rm rm 33.1305117 ## lstat lstat 32.0413077 ## dis dis 10.1450348 ## crim crim 6.4535978 ## black black 4.7076459 ## nox nox 4.1647148 ## age age 3.4933055 ## ptratio ptratio 2.1056374 ## tax tax 1.3320146 ## indus indus 0.9248479 ## rad rad 0.7800278 ## chas chas 0.5731764 ## zn zn 0.1481777 par(mfrow = c(1, 2)) plot(booston_boost, i = &quot;rm&quot;) plot(booston_boost, i = &quot;lstat&quot;) boston_boost_tst_pred = predict(booston_boost, newdata = boston_tst, n.trees = 5000) (boost_tst_rmse = rmse(boston_boost_tst_pred, boston_tst$medv)) ## [1] 3.437024 plot(boston_boost_tst_pred, boston_tst$medv, xlab = &quot;Predicted&quot;, ylab = &quot;Actual&quot;, main = &quot;Predicted vs Actual: Boosted Model, Test Data&quot; ) abline(0, 1, col = &quot;red&quot;, lwd = 2) 19.1.6 Results (boston_rmse = data.frame( Model = c(&quot;Single Tree&quot;, &quot;Linear Model&quot;, &quot;Bagging&quot;, &quot;Random Forest&quot;, &quot;Boosting&quot;), TestError = c(tree_tst_rmse, lm_tst_rmse, bag_tst_rmse, forest_tst_rmse, boost_tst_rmse) ) ) ## Model TestError ## 1 Single Tree 5.331457 ## 2 Linear Model 5.125877 ## 3 Bagging 3.814368 ## 4 Random Forest 3.734470 ## 5 Boosting 3.437024 While a single tree does not beat linear regression, each of the ensemble methods perform much better! 19.2 Classification We now return to the Carseats dataset and the classification setting. We see that an additive logistic regression performs much better than a single tree, but we expect ensemble methods to bring trees closer to the logistic regression. Can they do better? We now use prediction accuracy as our metric: accuracy = function(actual, predicted) { mean(actual == predicted) } data(Carseats) Carseats$Sales = as.factor(ifelse(Carseats$Sales &lt;= 8, &quot;Low&quot;, &quot;High&quot;)) set.seed(2) seat_idx = sample(1:nrow(Carseats), 200) seat_trn = Carseats[seat_idx,] seat_tst = Carseats[-seat_idx,] 19.2.1 Tree Model seat_tree = tree(Sales ~ ., data = seat_trn) set.seed(3) seat_tree_cv = cv.tree(seat_tree, FUN = prune.misclass) seat_tree_prune = prune.misclass(seat_tree, best = 9) seat_prune_tst_pred = predict(seat_tree_prune, seat_tst, type = &quot;class&quot;) table(predicted = seat_prune_tst_pred, actual = seat_tst$Sales) ## actual ## predicted High Low ## High 60 22 ## Low 24 94 (tree_tst_acc = accuracy(predicted = seat_prune_tst_pred, actual = seat_tst$Sales)) ## [1] 0.77 19.2.2 Logistic Regression seat_glm = glm(Sales ~ ., data = seat_trn, family = &quot;binomial&quot;) seat_glm_tst_pred = ifelse(predict(seat_glm, seat_tst, &quot;response&quot;) &gt; 0.5, &quot;Low&quot;, &quot;High&quot;) table(predicted = seat_glm_tst_pred, actual = seat_tst$Sales) ## actual ## predicted High Low ## High 75 9 ## Low 9 107 (glm_tst_acc = accuracy(predicted = seat_glm_tst_pred, actual = seat_tst$Sales)) ## [1] 0.91 19.2.3 Bagging seat_bag = randomForest(Sales ~ ., data = seat_trn, mtry = 10, importance = TRUE, ntrees = 500) seat_bag ## ## Call: ## randomForest(formula = Sales ~ ., data = seat_trn, mtry = 10, importance = TRUE, ntrees = 500) ## Type of random forest: classification ## Number of trees: 500 ## No. of variables tried at each split: 10 ## ## OOB estimate of error rate: 21.5% ## Confusion matrix: ## High Low class.error ## High 51 29 0.3625000 ## Low 14 106 0.1166667 seat_bag_tst_pred = predict(seat_bag, newdata = seat_tst) table(predicted = seat_bag_tst_pred, actual = seat_tst$Sales) ## actual ## predicted High Low ## High 68 21 ## Low 16 95 (bag_tst_acc = accuracy(predicted = seat_bag_tst_pred, actual = seat_tst$Sales)) ## [1] 0.815 19.2.4 Random Forest For classification, the suggested mtry for a random forest is \\(\\sqrt{p}.\\) seat_forest = randomForest(Sales ~ ., data = seat_trn, mtry = 3, importance = TRUE, ntrees = 500) seat_forest ## ## Call: ## randomForest(formula = Sales ~ ., data = seat_trn, mtry = 3, importance = TRUE, ntrees = 500) ## Type of random forest: classification ## Number of trees: 500 ## No. of variables tried at each split: 3 ## ## OOB estimate of error rate: 22% ## Confusion matrix: ## High Low class.error ## High 49 31 0.3875000 ## Low 13 107 0.1083333 seat_forest_tst_perd = predict(seat_forest, newdata = seat_tst) table(predicted = seat_forest_tst_perd, actual = seat_tst$Sales) ## actual ## predicted High Low ## High 63 16 ## Low 21 100 (forest_tst_acc = accuracy(predicted = seat_forest_tst_perd, actual = seat_tst$Sales)) ## [1] 0.815 19.2.5 Boosting To perform boosting, we modify the response to be 0 and 1 to work with gbm. Later we will use caret to fit gbm models, which will avoid this annoyance. seat_trn_mod = seat_trn seat_trn_mod$Sales = as.numeric(ifelse(seat_trn_mod$Sales == &quot;Low&quot;, &quot;0&quot;, &quot;1&quot;)) seat_boost = gbm(Sales ~ ., data = seat_trn_mod, distribution = &quot;bernoulli&quot;, n.trees = 5000, interaction.depth = 4, shrinkage = 0.01) seat_boost ## gbm(formula = Sales ~ ., distribution = &quot;bernoulli&quot;, data = seat_trn_mod, ## n.trees = 5000, interaction.depth = 4, shrinkage = 0.01) ## A gradient boosted model with bernoulli loss function. ## 5000 iterations were performed. ## There were 10 predictors of which 10 had non-zero influence. seat_boost_tst_pred = ifelse(predict(seat_boost, seat_tst, n.trees = 5000, &quot;response&quot;) &gt; 0.5, &quot;High&quot;, &quot;Low&quot;) table(predicted = seat_boost_tst_pred, actual = seat_tst$Sales) ## actual ## predicted High Low ## High 70 17 ## Low 14 99 (boost_tst_acc = accuracy(predicted = seat_boost_tst_pred, actual = seat_tst$Sales)) ## [1] 0.845 19.2.6 Results (seat_acc = data.frame( Model = c(&quot;Single Tree&quot;, &quot;Logistic Regression&quot;, &quot;Bagging&quot;, &quot;Random Forest&quot;, &quot;Boosting&quot;), TestAccuracy = c(tree_tst_acc, glm_tst_acc, bag_tst_acc, forest_tst_acc, boost_tst_acc) ) ) ## Model TestAccuracy ## 1 Single Tree 0.770 ## 2 Logistic Regression 0.910 ## 3 Bagging 0.815 ## 4 Random Forest 0.815 ## 5 Boosting 0.845 Here we see each of the ensemble methods performing better than a single tree, however, they still fall behind logistic regression. Sometimes a simple linear model will beat more complicated models! This is why you should always try a logistic regression for classification. 19.3 Tuning So far we fit bagging, boosting and random forest models, but did not tune any of them, we simply used certain, somewhat arbitrary, parameters. Now we will see how to modify the tuning parameters to make these models better. Bagging: Actually just a subset of Random Forest with mtry = \\(p\\). Random Forest: mtry Boosting: n.trees, interaction.depth, shrinkage, n.minobsinnode We will use the caret package to accomplish this. Technically ntrees is a tuning parameter for both bagging and random forest, but caret will use 500 by default and there is no easy way to tune it. This will not make a big difference since for both we simply need “enough” and 500 seems to do the trick. While mtry is a tuning parameter, there are suggested values for classification and regression: Regression: mtry = \\(p/3.\\) Classification: mtry = \\(\\sqrt{p}.\\) Also note that with these tree-based ensemble methods there are two resampling solutions for tuning the model: Out of Bag Cross-Validation Using Out of Bag samples is advantageous with these methods as compared to Cross-Validation since it removes the need to refit the model and is thus much more computationally efficient. Unfortunately OOB methods cannot be used with gbm models. See the caret documentation for details. 19.3.1 Random Forest and Bagging Here we setup training control for both OOB and cross-validation methods. Note we specify verbose = FALSE which suppresses output related to progress. You may wish to set this to TRUE when first tuning a model since it will give you an idea of how long the tuning process will take. (Which can sometimes be a long time.) oob = trainControl(method = &quot;oob&quot;) cv_5 = trainControl(method = &quot;cv&quot;, number = 5) To tune a Random Forest in caret we will use method = &quot;rf&quot; which uses the randomForest function in the background. Here we elect to use the OOB training control that we created. We could also use Cross-Validation, however it will likely select a similar model, but requiring more time. We setup a grid of mtry values which include all possible values since there are \\(10\\) predictors in the dataset. An mtry of \\(10\\) is actually bagging. dim(seat_trn) ## [1] 200 11 rf_grid = expand.grid(mtry = 1:10) set.seed(825) seat_rf_tune = train(Sales ~ ., data = seat_trn, method = &quot;rf&quot;, trControl = oob, verbose = FALSE, tuneGrid = rf_grid) seat_rf_tune ## Random Forest ## ## 200 samples ## 10 predictor ## 2 classes: &#39;High&#39;, &#39;Low&#39; ## ## No pre-processing ## Resampling results across tuning parameters: ## ## mtry Accuracy Kappa ## 1 0.750 0.4318182 ## 2 0.785 0.5295405 ## 3 0.785 0.5315904 ## 4 0.805 0.5824411 ## 5 0.785 0.5356371 ## 6 0.810 0.5957447 ## 7 0.810 0.5922747 ## 8 0.805 0.5842217 ## 9 0.785 0.5396146 ## 10 0.795 0.5591398 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was mtry = 6. accuracy(predict(seat_rf_tune, seat_tst), seat_tst$Sales) ## [1] 0.81 The results returned are based on the OOB samples. (Coincidentally, the test accuracy is the same as the best accuracy found using OOB samples.) Note that when using OOB, for some reason the default plot is not what you would expect and is not at all useful. (Which is why it is omitted here.) seat_rf_tune$bestTune ## mtry ## 6 6 Based on these results, we would select the random forest model with an mtry of 6. Note that based on the OOB estimates, the bagging model is expected to perform worse than this select model, however, based on our results above, that is not what we find to be true in our test set. Also note that method = &quot;ranger&quot; would also fit a random forest model. Ranger is a newer R package for random forests that has been shown to be much faster, especially when there are a larger number of predictors. 19.3.2 Boosting We now tune a boosted tree model. We will use the cross-validation tune control setup above. We will fit the model using gbm with caret. To setup the tuning grid, we must specify four parameters to tune: interaction.depth: How many splits to use with each tree. n.trees: The number of trees to use. shrinkage: The shrinkage parameters, which controls how fast the method learns. n.minobsinnode: The minimum number of observations in a node of the tree. (caret requires us to specify this. This is actually a tuning parameter of the trees, not boosting, and we would normally just accept the default.) Finally, expand.grid comes in handy, as we can specify a vector of values for each parameter, then we get back a matrix of all possible combinations. gbm_grid = expand.grid(interaction.depth = 1:5, n.trees = (1:6) * 500, shrinkage = c(0.001, 0.01, 0.1), n.minobsinnode = 10) We now train the model using all possible combinations of the tuning parameters we just specified. seat_gbm_tune = train(Sales ~ ., data = seat_trn, method = &quot;gbm&quot;, trControl = cv_5, verbose = FALSE, tuneGrid = gbm_grid) The additional verbose = FALSE in the train call suppresses additional output from each gbm call. By default, calling plot here will produce a nice graphic of the results. #seat_gbm_tune plot(seat_gbm_tune) accuracy(predict(seat_gbm_tune, seat_tst), seat_tst$Sales) ## [1] 0.845 We see our tuned model does no better on the test set than the arbitrary boosted model we had fit above, with the slightly different parameters seen below. We could perhaps try a larger tuning grid, but at this point it seems unlikely that we could find a much better model. There seems to be no way to get a tree method to out-perform logistic regression in this dataset. seat_gbm_tune$bestTune ## n.trees interaction.depth shrinkage n.minobsinnode ## 61 500 1 0.1 10 19.4 Tree versus Ensemble Boundaries library(mlbench) set.seed(42) sim_trn = mlbench.circle(n = 1000, d = 2) sim_trn = data.frame(sim_trn$x, class = as.factor(sim_trn$classes)) sim_tst = mlbench.circle(n = 1000, d = 2) sim_tst = data.frame(sim_tst$x, class = as.factor(sim_tst$classes)) plot(sim_trn$X1, sim_trn$X2, col = sim_trn$class, xlab = &quot;X1&quot;, ylab = &quot;X2&quot;) cv_5 = trainControl(method = &quot;cv&quot;, number = 5) oob = trainControl(method = &quot;oob&quot;) sim_tree_cv = train(class ~ ., data = sim_trn, trControl = cv_5, method = &quot;rpart&quot;) ## Loading required package: rpart library(rpart.plot) prp(sim_tree_cv$finalModel) rf_grid = expand.grid(mtry = c(1, 2)) sim_rf_oob = train(class ~ ., data = sim_trn, trControl = oob, tuneGrid = rf_grid) gbm_grid = expand.grid(interaction.depth = 1:5, n.trees = (1:6) * 500, shrinkage = c(0.001, 0.01, 0.1), n.minobsinnode = 10) sim_gbm_cv = train(class ~ ., data = sim_trn, method = &quot;gbm&quot;, trControl = cv_5, verbose = FALSE, tuneGrid = gbm_grid) plot_grid = expand.grid( X1 = seq(min(sim_tst$X1), max(sim_tst$X1), by = 0.01), X2 = seq(min(sim_tst$X2), max(sim_tst$X2), by = 0.01) ) tree_pred = predict(sim_tree_cv, plot_grid) rf_pred = predict(sim_rf_oob, plot_grid) gbm_pred = predict(sim_gbm_cv, plot_grid) par(mfrow = c(1, 3)) plot(plot_grid$X1, plot_grid$X2, col = tree_pred, xlab = &quot;X1&quot;, ylab = &quot;X2&quot;, pch = 20, main = &quot;Single Tree&quot;) plot(plot_grid$X1, plot_grid$X2, col = rf_pred, xlab = &quot;X1&quot;, ylab = &quot;X2&quot;, pch = 20, main = &quot;Random Forest&quot;) plot(plot_grid$X1, plot_grid$X2, col = gbm_pred, xlab = &quot;X1&quot;, ylab = &quot;X2&quot;, pch = 20, main = &quot;Boosted Trees&quot;) 19.5 External Links Classification and Regression by randomForest - Introduction to the randomForest package in R news. ranger: A Fast Implementation of Random Forests - Alternative package for fitting random forests with potentially better speed. On ranger’s respect.unordered.factors Argument - A note on handling of categorical variables with random forests. Extremely Randomized Trees extraTrees Method for Classificationand Regression XGBoost - Scalable and Flexible Gradient Boosting XGBoost R Tutorial 19.6 RMarkdown The RMarkdown file for this chapter can be found here. The file was created using R version 3.4.1 and the following packages: Base Packages, Attached ## [1] &quot;methods&quot; &quot;parallel&quot; &quot;splines&quot; &quot;stats&quot; &quot;graphics&quot; &quot;grDevices&quot; ## [7] &quot;utils&quot; &quot;datasets&quot; &quot;base&quot; Additional Packages, Attached ## [1] &quot;rpart.plot&quot; &quot;rpart&quot; &quot;mlbench&quot; &quot;plyr&quot; ## [5] &quot;caret&quot; &quot;ggplot2&quot; &quot;gbm&quot; &quot;lattice&quot; ## [9] &quot;survival&quot; &quot;randomForest&quot; &quot;ISLR&quot; &quot;MASS&quot; ## [13] &quot;tree&quot; Additional Packages, Not Attached ## [1] &quot;Rcpp&quot; &quot;nloptr&quot; &quot;compiler&quot; &quot;class&quot; ## [5] &quot;iterators&quot; &quot;tools&quot; &quot;lme4&quot; &quot;digest&quot; ## [9] &quot;evaluate&quot; &quot;tibble&quot; &quot;gtable&quot; &quot;nlme&quot; ## [13] &quot;mgcv&quot; &quot;rlang&quot; &quot;Matrix&quot; &quot;foreach&quot; ## [17] &quot;yaml&quot; &quot;SparseM&quot; &quot;e1071&quot; &quot;stringr&quot; ## [21] &quot;knitr&quot; &quot;MatrixModels&quot; &quot;stats4&quot; &quot;nnet&quot; ## [25] &quot;rprojroot&quot; &quot;grid&quot; &quot;rmarkdown&quot; &quot;bookdown&quot; ## [29] &quot;minqa&quot; &quot;car&quot; &quot;reshape2&quot; &quot;magrittr&quot; ## [33] &quot;backports&quot; &quot;scales&quot; &quot;codetools&quot; &quot;ModelMetrics&quot; ## [37] &quot;htmltools&quot; &quot;pbkrtest&quot; &quot;colorspace&quot; &quot;quantreg&quot; ## [41] &quot;stringi&quot; &quot;lazyeval&quot; &quot;munsell&quot; "],
["support-vector-machines.html", "Chapter 20 Support Vector Machines 20.1 R Packages 20.2 Classification 20.3 Linear, Separable Example 20.4 Non-Linear, Non-Separable Example 20.5 External Links 20.6 RMarkdown", " Chapter 20 Support Vector Machines 20.1 R Packages library(randomForest) library(caret) library(kernlab) For working with SVMs in R we will use the kernlab package instead of e1071. For examples of e1071 see the relevant chapter in ISL. We do not discuss the details of the method, however show how the method can be tuned. 20.2 Classification SVMs are mostly used for classification. Note that they can be modified for regression but we will not do that here. Since we are performing classification, we will use accuracy as our metric. accuracy = function(actual, predicted) { mean(actual == predicted) } 20.3 Linear, Separable Example 20.3.1 Data Simulation sim_sep = function(n = 1000) { x1 = runif(n) x2 = runif(n) keep = x1 + 0.1 &lt; x2 | x1 - 0.1 &gt; x2 x1 = x1[keep] x2 = x2[keep] y = 1 * (x1 - x2 &gt; 0) y = ifelse(y == 1, &quot;Orange&quot;, &quot;Blue&quot;) data.frame(y = as.factor(y), x1 = x1, x2 = x2) } set.seed(42) train_data = sim_sep(n = 50) plot(x2 ~ x1, data = train_data, col = as.character(y), pch = 19) test_data = sim_sep(n = 500) str(train_data) ## &#39;data.frame&#39;: 41 obs. of 3 variables: ## $ y : Factor w/ 2 levels &quot;Blue&quot;,&quot;Orange&quot;: 2 2 1 2 1 2 2 1 1 2 ... ## $ x1: num 0.915 0.937 0.286 0.642 0.519 ... ## $ x2: num 0.3334 0.3467 0.3985 0.0389 0.7488 ... 20.3.2 Linear Kernel, Parameter C lin_svm_fit = ksvm(y ~ ., data = train_data, kernel = &#39;vanilladot&#39;, C = 0.1) ## Setting default kernel parameters plot(lin_svm_fit, data = train_data) # train accuracy accuracy(actual = train_data$y, predicted = predict(lin_svm_fit, train_data)) ## [1] 1 # test accuracy accuracy(actual = test_data$y, predicted = predict(lin_svm_fit, test_data)) ## [1] 0.9872449 lin_svm_fit = ksvm(y ~ ., data = train_data, kernel = &#39;vanilladot&#39;, C = 1) ## Setting default kernel parameters plot(lin_svm_fit, data = train_data) # train accuracy accuracy(actual = train_data$y, predicted = predict(lin_svm_fit, train_data)) ## [1] 1 # test accuracy accuracy(actual = test_data$y, predicted = predict(lin_svm_fit, test_data)) ## [1] 1 lin_svm_fit = ksvm(y ~ ., data = train_data, kernel = &#39;vanilladot&#39;, C = 10) ## Setting default kernel parameters plot(lin_svm_fit, data = train_data) # train accuracy accuracy(actual = train_data$y, predicted = predict(lin_svm_fit, train_data)) ## [1] 1 # test accuracy accuracy(actual = test_data$y, predicted = predict(lin_svm_fit, test_data)) ## [1] 1 20.3.3 Radial Kernel set.seed(42) rad_svm_fit = ksvm(y ~ ., data = train_data, kernel = &#39;rbfdot&#39;, C = 1) plot(rad_svm_fit, data = train_data) # train accuracy accuracy(actual = train_data$y, predicted = predict(rad_svm_fit, train_data)) ## [1] 1 # test accuracy accuracy(actual = test_data$y, predicted = predict(rad_svm_fit, test_data)) ## [1] 1 20.3.4 Tuning with caret svm_grid = expand.grid(C = c(2 ^ (-5:5))) svm_control = trainControl(method = &quot;cv&quot;, number = 5, returnResamp = &quot;all&quot;, verbose = FALSE) set.seed(42) lin_svm_fit = train(y ~ ., data = train_data, method = &quot;svmLinear&quot;, trControl = svm_control, tuneGrid = svm_grid) lin_svm_fit ## Support Vector Machines with Linear Kernel ## ## 41 samples ## 2 predictor ## 2 classes: &#39;Blue&#39;, &#39;Orange&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 33, 33, 33, 33, 32 ## Resampling results across tuning parameters: ## ## C Accuracy Kappa ## 0.03125 0.850 0.6197802 ## 0.06250 0.950 0.8928571 ## 0.12500 0.975 0.9500000 ## 0.25000 0.975 0.9500000 ## 0.50000 0.975 0.9500000 ## 1.00000 1.000 1.0000000 ## 2.00000 0.975 0.9500000 ## 4.00000 1.000 1.0000000 ## 8.00000 1.000 1.0000000 ## 16.00000 1.000 1.0000000 ## 32.00000 1.000 1.0000000 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was C = 1. lin_svm_fit$bestTune ## C ## 6 1 # train accuracy accuracy(actual = train_data$y, predicted = predict(lin_svm_fit, train_data)) ## [1] 1 # test accuracy accuracy(actual = test_data$y, predicted = predict(lin_svm_fit, test_data)) ## [1] 1 20.3.5 Compare: Random Forest set.seed(42) rf_grid = expand.grid(mtry = 1:2) rf_fit = train(y ~ ., data = train_data, method = &quot;rf&quot;, trControl = svm_control, tuneGrid = rf_grid) rf_fit$bestTune ## mtry ## 1 1 # train accuracy accuracy(actual = train_data$y, predicted = predict(rf_fit, train_data)) ## [1] 1 # test accuracy accuracy(actual = test_data$y, predicted = predict(rf_fit, test_data)) ## [1] 0.9591837 20.4 Non-Linear, Non-Separable Example 20.4.1 Data Simulation sim_rad = function(n = 1000) { x1 = runif(n, -1, 1) x2 = runif(n, -1, 1) y = 1 * (x1 ^ 2 + x2 ^ 2 + rnorm(n, 0, 0.25) &lt; 0.5) y = ifelse(y == 1, &quot;Orange&quot;, &quot;Blue&quot;) data.frame(y = as.factor(y), x1 = x1, x2 = x2) } set.seed(42) train_data = sim_rad(n = 250) plot(x2 ~ x1, data = train_data, col = as.character(y), pch = 19) test_data = sim_rad(n = 2000) 20.4.2 Radial Kernel, Parameter C rad_svm_fit = ksvm(y ~., data = train_data, kernel = &#39;rbfdot&#39;, C = 0.1, kpar = list(sigma = 1)) plot(rad_svm_fit, data = train_data) # train accuracy accuracy(actual = train_data$y, predicted = predict(rad_svm_fit, train_data)) ## [1] 0.852 # test accuracy accuracy(actual = test_data$y, predicted = predict(rad_svm_fit, test_data)) ## [1] 0.812 rad_svm_fit = ksvm(y ~., data = train_data, kernel = &#39;rbfdot&#39;, C = 1, kpar = list(sigma = 1)) plot(rad_svm_fit, data = train_data) # train accuracy accuracy(actual = train_data$y, predicted = predict(rad_svm_fit, train_data)) ## [1] 0.876 # test accuracy accuracy(actual = test_data$y, predicted = predict(rad_svm_fit, test_data)) ## [1] 0.833 rad_svm_fit = ksvm(y ~., data = train_data, kernel = &#39;rbfdot&#39;, C = 10, kpar = list(sigma = 1)) plot(rad_svm_fit, data = train_data) # train accuracy accuracy(actual = train_data$y, predicted = predict(rad_svm_fit, train_data)) ## [1] 0.884 # test accuracy accuracy(actual = test_data$y, predicted = predict(rad_svm_fit, test_data)) ## [1] 0.8285 20.4.3 Radial Kernel, Parameter sigma rad_svm_fit = ksvm(y ~., data = train_data, kernel = &#39;rbfdot&#39;, C = 1, kpar = list(sigma = 0.5)) plot(rad_svm_fit, data = train_data) # train accuracy accuracy(actual = train_data$y, predicted = predict(rad_svm_fit, train_data)) ## [1] 0.864 # test accuracy accuracy(actual = test_data$y, predicted = predict(rad_svm_fit, test_data)) ## [1] 0.8365 rad_svm_fit = ksvm(y ~., data = train_data, kernel = &#39;rbfdot&#39;, C = 1, kpar = list(sigma = 1)) plot(rad_svm_fit, data = train_data) # train accuracy accuracy(actual = train_data$y, predicted = predict(rad_svm_fit, train_data)) ## [1] 0.876 # test accuracy accuracy(actual = test_data$y, predicted = predict(rad_svm_fit, test_data)) ## [1] 0.833 rad_svm_fit = ksvm(y ~., data = train_data, kernel = &#39;rbfdot&#39;, C = 1, kpar = list(sigma = 2)) plot(rad_svm_fit, data = train_data) # train accuracy accuracy(actual = train_data$y, predicted = predict(rad_svm_fit, train_data)) ## [1] 0.884 # test accuracy accuracy(actual = test_data$y, predicted = predict(rad_svm_fit, test_data)) ## [1] 0.828 20.4.4 Radial Kernel, Tuning svm_grid = expand.grid( C = c(2 ^ (-5:5)), sigma = c(2 ^ (-3:3))) set.seed(42) rad_svm_fit = train(y ~ ., data = train_data, method = &quot;svmRadial&quot;, trControl = svm_control, tuneGrid = svm_grid) #rad_svm_fit rad_svm_fit$bestTune ## sigma C ## 65 0.25 16 rad_svm_fit = ksvm(y ~., data = train_data, kernel = &#39;rbfdot&#39;, C = 16, kpar = list(sigma = 0.25)) plot(rad_svm_fit, data = train_data) # train accuracy accuracy(actual = train_data$y, predicted = predict(rad_svm_fit, train_data)) ## [1] 0.876 # test accuracy accuracy(actual = test_data$y, predicted = predict(rad_svm_fit, test_data)) ## [1] 0.838 20.4.5 Polynomial Kernel, Tuning set.seed(42) poly_svm_fit = train(y ~ ., data = train_data, method = &quot;svmPoly&quot;, trControl = svm_control) #poly_svm_fit poly_svm_fit$bestTune ## degree scale C ## 27 3 0.1 1 poly_svm_fit = ksvm(y ~., data = train_data, kernel = &#39;polydot&#39;, C = 1, kpar = list(scale = 0.1, degree = 3)) plot(poly_svm_fit, data = train_data) # train accuracy accuracy(actual = train_data$y, predicted = predict(poly_svm_fit, train_data)) ## [1] 0.864 # test accuracy accuracy(actual = test_data$y, predicted = predict(poly_svm_fit, test_data)) ## [1] 0.8425 20.4.6 Linear Kernel, Tuning svm_grid = expand.grid(C = c(2 ^ (-5:5))) set.seed(42) lin_svm_fit = train(y ~ ., data = train_data, method = &quot;svmLinear&quot;, trControl = svm_control, tuneGrid = svm_grid) lin_svm_fit ## Support Vector Machines with Linear Kernel ## ## 250 samples ## 2 predictor ## 2 classes: &#39;Blue&#39;, &#39;Orange&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 200, 200, 201, 199, 200 ## Resampling results across tuning parameters: ## ## C Accuracy Kappa ## 0.03125 0.5920144 0 ## 0.06250 0.5920144 0 ## 0.12500 0.5920144 0 ## 0.25000 0.5920144 0 ## 0.50000 0.5920144 0 ## 1.00000 0.5920144 0 ## 2.00000 0.5920144 0 ## 4.00000 0.5920144 0 ## 8.00000 0.5920144 0 ## 16.00000 0.5920144 0 ## 32.00000 0.5920144 0 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was C = 0.03125. lin_svm_fit$bestTune ## C ## 1 0.03125 lin_svm_fit = ksvm(y ~., data = train_data, kernel = &#39;vanilladot&#39;, C = lin_svm_fit$bestTune) ## Setting default kernel parameters plot(lin_svm_fit, data = train_data) # train accuracy accuracy(actual = train_data$y, predicted = predict(lin_svm_fit, train_data)) ## [1] 0.592 # test accuracy accuracy(actual = test_data$y, predicted = predict(lin_svm_fit, test_data)) ## [1] 0.6005 20.4.7 Compare: Random Forest set.seed(42) rf_grid = expand.grid(mtry = 1:2) rf_fit = train(y ~ ., data = train_data, method = &quot;rf&quot;, trControl = svm_control, tuneGrid = rf_grid) rf_fit$bestTune ## mtry ## 2 2 # train accuracy accuracy(actual = train_data$y, predicted = predict(rf_fit, train_data)) ## [1] 1 # test accuracy accuracy(actual = test_data$y, predicted = predict(rf_fit, test_data)) ## [1] 0.809 20.5 External Links SVM with Polynomial Kernel Visualization - The kernel idea in one simple video. 20.6 RMarkdown The RMarkdown file for this chapter can be found here. The file was created using R version 3.4.1 and the following packages: Base Packages, Attached ## [1] &quot;methods&quot; &quot;stats&quot; &quot;graphics&quot; &quot;grDevices&quot; &quot;utils&quot; &quot;datasets&quot; ## [7] &quot;base&quot; Additional Packages, Attached ## [1] &quot;kernlab&quot; &quot;caret&quot; &quot;ggplot2&quot; &quot;lattice&quot; ## [5] &quot;randomForest&quot; Additional Packages, Not Attached ## [1] &quot;Rcpp&quot; &quot;nloptr&quot; &quot;compiler&quot; &quot;plyr&quot; ## [5] &quot;class&quot; &quot;iterators&quot; &quot;tools&quot; &quot;digest&quot; ## [9] &quot;lme4&quot; &quot;evaluate&quot; &quot;tibble&quot; &quot;gtable&quot; ## [13] &quot;nlme&quot; &quot;mgcv&quot; &quot;rlang&quot; &quot;Matrix&quot; ## [17] &quot;foreach&quot; &quot;parallel&quot; &quot;yaml&quot; &quot;SparseM&quot; ## [21] &quot;e1071&quot; &quot;stringr&quot; &quot;knitr&quot; &quot;MatrixModels&quot; ## [25] &quot;stats4&quot; &quot;rprojroot&quot; &quot;grid&quot; &quot;nnet&quot; ## [29] &quot;rmarkdown&quot; &quot;bookdown&quot; &quot;minqa&quot; &quot;reshape2&quot; ## [33] &quot;car&quot; &quot;magrittr&quot; &quot;backports&quot; &quot;scales&quot; ## [37] &quot;codetools&quot; &quot;ModelMetrics&quot; &quot;htmltools&quot; &quot;MASS&quot; ## [41] &quot;splines&quot; &quot;pbkrtest&quot; &quot;colorspace&quot; &quot;quantreg&quot; ## [45] &quot;stringi&quot; &quot;lazyeval&quot; &quot;munsell&quot; "],
["unsupervised-learning.html", "Chapter 21 Unsupervised Learning 21.1 Methods 21.2 Examples 21.3 External Links 21.4 RMarkdown", " Chapter 21 Unsupervised Learning 21.1 Methods 21.1.1 Principal Component Analysis To perform PCA in R we will use prcomp(). See ?prcomp() for details. 21.1.2 \\(k\\)-Means Clustering To perform \\(k\\)-means in R we will use kmeans(). See ?kmeans() for details. 21.1.3 Hierarchical Clustering To perform hierarchical clustering in R we will use hclust(). See ?hclust() for details. 21.2 Examples 21.2.1 US Arrests library(ISLR) data(USArrests) apply(USArrests, 2, mean) ## Murder Assault UrbanPop Rape ## 7.788 170.760 65.540 21.232 apply(USArrests, 2, sd) ## Murder Assault UrbanPop Rape ## 4.355510 83.337661 14.474763 9.366385 “Before” performing PCA, we will scale the data. (This will actually happen inside the prcomp() function.) USArrests_pca = prcomp(USArrests, scale = TRUE) A large amount of information is stored in the output of prcomp(), some of which can neatly be displayed with summary(). names(USArrests_pca) ## [1] &quot;sdev&quot; &quot;rotation&quot; &quot;center&quot; &quot;scale&quot; &quot;x&quot; summary(USArrests_pca) ## Importance of components%s: ## PC1 PC2 PC3 PC4 ## Standard deviation 1.5749 0.9949 0.59713 0.41645 ## Proportion of Variance 0.6201 0.2474 0.08914 0.04336 ## Cumulative Proportion 0.6201 0.8675 0.95664 1.00000 USArrests_pca$center ## Murder Assault UrbanPop Rape ## 7.788 170.760 65.540 21.232 USArrests_pca$scale ## Murder Assault UrbanPop Rape ## 4.355510 83.337661 14.474763 9.366385 USArrests_pca$rotation ## PC1 PC2 PC3 PC4 ## Murder -0.5358995 0.4181809 -0.3412327 0.64922780 ## Assault -0.5831836 0.1879856 -0.2681484 -0.74340748 ## UrbanPop -0.2781909 -0.8728062 -0.3780158 0.13387773 ## Rape -0.5434321 -0.1673186 0.8177779 0.08902432 We see that $center and $scale give the mean and standard deviations for the original variables. $rotation gives the loading vectors that are used to rotate the original data to obtain the principal components. dim(USArrests_pca$x) ## [1] 50 4 dim(USArrests) ## [1] 50 4 head(USArrests_pca$x) ## PC1 PC2 PC3 PC4 ## Alabama -0.9756604 1.1220012 -0.43980366 0.154696581 ## Alaska -1.9305379 1.0624269 2.01950027 -0.434175454 ## Arizona -1.7454429 -0.7384595 0.05423025 -0.826264240 ## Arkansas 0.1399989 1.1085423 0.11342217 -0.180973554 ## California -2.4986128 -1.5274267 0.59254100 -0.338559240 ## Colorado -1.4993407 -0.9776297 1.08400162 0.001450164 The dimension of the principal components is the same as the original data. The principal components are stored in $x. scale(as.matrix(USArrests))[1, ] %*% USArrests_pca$rotation[, 1] ## [,1] ## [1,] -0.9756604 scale(as.matrix(USArrests))[1, ] %*% USArrests_pca$rotation[, 2] ## [,1] ## [1,] 1.122001 scale(as.matrix(USArrests))[1, ] %*% USArrests_pca$rotation[, 3] ## [,1] ## [1,] -0.4398037 scale(as.matrix(USArrests))[1, ] %*% USArrests_pca$rotation[, 4] ## [,1] ## [1,] 0.1546966 head(scale(as.matrix(USArrests)) %*% USArrests_pca$rotation[,1]) ## [,1] ## Alabama -0.9756604 ## Alaska -1.9305379 ## Arizona -1.7454429 ## Arkansas 0.1399989 ## California -2.4986128 ## Colorado -1.4993407 head(USArrests_pca$x[, 1]) ## Alabama Alaska Arizona Arkansas California Colorado ## -0.9756604 -1.9305379 -1.7454429 0.1399989 -2.4986128 -1.4993407 sum(USArrests_pca$rotation[, 1] ^ 2) ## [1] 1 USArrests_pca$rotation[, 1] %*% USArrests_pca$rotation[, 2] ## [,1] ## [1,] -1.387779e-16 USArrests_pca$rotation[, 1] %*% USArrests_pca$rotation[, 3] ## [,1] ## [1,] -5.551115e-17 USArrests_pca$x[, 1] %*% USArrests_pca$x[, 2] ## [,1] ## [1,] -2.062239e-14 USArrests_pca$x[, 1] %*% USArrests_pca$x[, 3] ## [,1] ## [1,] 5.384582e-15 The above verifies some of the “math” of PCA. We see how the loadings obtain the principal components from the original data. We check that the loading vectors are normalized. We also check for orthogonality of both the loading vectors and the principal components. (Note the above inner products aren’t exactly 0, but that is simply a numerical issue.) biplot(USArrests_pca, scale = 0, cex = 0.5) A biplot can be used to visualize both the principal component scores and the principal component loadings. (Note the two scales for each axis.) USArrests_pca$sdev ## [1] 1.5748783 0.9948694 0.5971291 0.4164494 USArrests_pca$sdev ^ 2 / sum(USArrests_pca$sdev ^ 2) ## [1] 0.62006039 0.24744129 0.08914080 0.04335752 Frequently we will be interested in the proportion of variance explained by a principal component. get_PVE = function(pca_out) { pca_out$sdev ^ 2 / sum(pca_out$sdev ^ 2) } So frequently, we would be smart to write a function to do so. pve = get_PVE(USArrests_pca) pve ## [1] 0.62006039 0.24744129 0.08914080 0.04335752 plot( pve, xlab = &quot;Principal Component&quot;, ylab = &quot;Proportion of Variance Explained&quot;, ylim = c(0, 1), type = &#39;b&#39; ) We can then plot the proportion of variance explained for each PC. As expected, we see the PVE decrease. cumsum(pve) ## [1] 0.6200604 0.8675017 0.9566425 1.0000000 plot( cumsum(pve), xlab = &quot;Principal Component&quot;, ylab = &quot;Cumulative Proportion of Variance Explained&quot;, ylim = c(0, 1), type = &#39;b&#39; ) Often we are interested in the cumulative proportion. A common use of PCA outside of visualization is dimension reduction for modeling. If \\(p\\) is large, PCA is performed, and the principal components that account for a large proportion of variation, say 95%, are used for further analysis. In certain situations that can reduce the dimensionality of data significantly. This can be done almost automatically using caret: library(caret) library(mlbench) data(Sonar) set.seed(18) using_pca = train(Class ~ ., data = Sonar, method = &quot;knn&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 5), preProcess = &quot;pca&quot;, tuneGrid = expand.grid(k = c(1, 3, 5, 7, 9))) regular_scaling = train(Class ~ ., data = Sonar, method = &quot;knn&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 5), preProcess = c(&quot;center&quot;, &quot;scale&quot;), tuneGrid = expand.grid(k = c(1, 3, 5, 7, 9))) max(using_pca$results$Accuracy) ## [1] 0.8652729 max(regular_scaling$results$Accuracy) ## [1] 0.8558653 using_pca$preProcess ## Created from 208 samples and 60 variables ## ## Pre-processing: ## - centered (60) ## - ignored (0) ## - principal component signal extraction (60) ## - scaled (60) ## ## PCA needed 30 components to capture 95 percent of the variance It won’t always outperform simply using the original predictors, but here using 30 of 60 principal components shows a slight advantage over using all 60 predictors. In other situation, it may result in a large perform gain. 21.2.2 Simulated Data library(MASS) set.seed(42) n = 180 p = 10 clust_data = rbind( mvrnorm(n = n / 3, sample(c(1, 2, 3, 4), p, replace = TRUE), diag(p)), mvrnorm(n = n / 3, sample(c(1, 2, 3, 4), p, replace = TRUE), diag(p)), mvrnorm(n = n / 3, sample(c(1, 2, 3, 4), p, replace = TRUE), diag(p)) ) Above we simulate data for clustering. Note that, we did this in a way that will result in three clusters. true_clusters = c(rep(3, n / 3), rep(1, n / 3), rep(2, n / 3)) We label the true clusters 1, 2, and 3 in a way that will “match” output from \\(k\\)-means. (Which is somewhat arbitrary.) kmean_out = kmeans(clust_data, centers = 3, nstart = 10) names(kmean_out) ## [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; ## [5] &quot;tot.withinss&quot; &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; ## [9] &quot;ifault&quot; Notice that we used nstart = 10 which will give us a more stable solution by attempting 10 random starting positions for the means. Also notice we chose to use centers = 3. (The \\(k\\) in \\(k\\)-mean). How did we know to do this? We’ll find out on the homework. (It will involve looking at tot.withinss) kmean_out ## K-means clustering with 3 clusters of sizes 61, 60, 59 ## ## Cluster means: ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## 1 3.997352 4.085592 0.7846534 2.136643 4.059886 3.2490887 1.747697 ## 2 1.008138 2.881229 4.3102354 4.094867 3.022989 0.8878413 4.002270 ## 3 3.993468 4.049505 1.9553560 4.037748 2.825907 2.9960855 3.026397 ## [,8] [,9] [,10] ## 1 1.8341976 0.8193371 4.043725 ## 2 3.8085492 2.0905060 0.977065 ## 3 0.8992179 3.0041820 2.931030 ## ## Clustering vector: ## [1] 3 3 3 3 3 3 3 3 1 3 3 3 3 3 3 3 3 3 3 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 ## [36] 3 3 3 3 3 3 3 3 3 3 3 3 3 1 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 ## [71] 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [106] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## [141] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## [176] 2 2 2 2 2 ## ## Within cluster sum of squares by cluster: ## [1] 609.2674 581.8780 568.3845 ## (between_SS / total_SS = 54.0 %) ## ## Available components: ## ## [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; ## [5] &quot;tot.withinss&quot; &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; ## [9] &quot;ifault&quot; kmeans_clusters = kmean_out$cluster table(true_clusters, kmeans_clusters) ## kmeans_clusters ## true_clusters 1 2 3 ## 1 58 0 2 ## 2 0 60 0 ## 3 3 0 57 We check how well the clustering is working. dim(clust_data) ## [1] 180 10 This data is “high dimensional” so it is difficult to visualize. (Anything more than 2 is hard to visualize.) plot( clust_data[, 1], clust_data[, 2], pch = 20, xlab = &quot;First Variable&quot;, ylab = &quot;Second Variable&quot; ) Plotting the first and second variables simply results in a blob. plot( clust_data[, 1], clust_data[, 2], col = true_clusters, pch = 20, xlab = &quot;First Variable&quot;, ylab = &quot;Second Variable&quot; ) Even when using their true clusters for coloring, this plot isn’t very helpful. clust_data_pca = prcomp(clust_data, scale = TRUE) plot( clust_data_pca$x[, 1], clust_data_pca$x[, 2], pch = 0, xlab = &quot;First Principal Component&quot;, ylab = &quot;Second Principal Component&quot; ) If we instead plot the first two principal components, we see, even without coloring, one blob that is clearly separate from the rest. plot( clust_data_pca$x[, 1], clust_data_pca$x[, 2], col = true_clusters, pch = 0, xlab = &quot;First Principal Component&quot;, ylab = &quot;Second Principal Component&quot;, cex = 2 ) points(clust_data_pca$x[, 1], clust_data_pca$x[, 2], col = kmeans_clusters, pch = 20, cex = 1.5) Now adding the true colors (boxes) and the \\(k\\)-means results (circles), we obtain a nice visualization. clust_data_pve = get_PVE(clust_data_pca) plot( cumsum(clust_data_pve), xlab = &quot;Principal Component&quot;, ylab = &quot;Cumulative Proportion of Variance Explained&quot;, ylim = c(0, 1), type = &#39;b&#39; ) The above visualization works well because the first two PCs explain a large proportion of the variance. #install.packages(&#39;sparcl&#39;) library(sparcl) To create colored dendrograms we will use ColorDendrogram() in the sparcl package. clust_data_hc = hclust(dist(scale(clust_data)), method = &quot;complete&quot;) clust_data_cut = cutree(clust_data_hc , 3) ColorDendrogram(clust_data_hc, y = clust_data_cut, labels = names(clust_data_cut), main = &quot;Simulated Data, Complete Linkage&quot;, branchlength = 1.5) Here we apply hierarchical clustering to the scaled data. The dist() function is used to calculate pairwise distances between the (scaled in this case) observations. We use complete linkage. We then use the cutree() function to cluster the data into 3 clusters. The ColorDendrogram() function is then used to plot the dendrogram. Note that the branchlength argument is somewhat arbitrary (the length of the colored bar) and will need to be modified for each dendrogram. table(true_clusters, clust_data_cut) ## clust_data_cut ## true_clusters 1 2 3 ## 1 9 51 0 ## 2 1 0 59 ## 3 59 1 0 We see in this case hierarchical clustering doesn’t “work” as well as \\(k\\)-means. clust_data_hc = hclust(dist(scale(clust_data)), method = &quot;single&quot;) clust_data_cut = cutree(clust_data_hc , 3) ColorDendrogram(clust_data_hc, y = clust_data_cut, labels = names(clust_data_cut), main = &quot;Simulated Data, Single Linkage&quot;, branchlength = 0.5) table(true_clusters, clust_data_cut) ## clust_data_cut ## true_clusters 1 2 3 ## 1 59 1 0 ## 2 59 0 1 ## 3 60 0 0 clust_data_hc = hclust(dist(scale(clust_data)), method = &quot;average&quot;) clust_data_cut = cutree(clust_data_hc , 3) ColorDendrogram(clust_data_hc, y = clust_data_cut, labels = names(clust_data_cut), main = &quot;Simulated Data, Average Linkage&quot;, branchlength = 1) table(true_clusters, clust_data_cut) ## clust_data_cut ## true_clusters 1 2 3 ## 1 1 59 0 ## 2 1 0 59 ## 3 58 2 0 We also try single and average linkage. Single linkage seems to perform poorly here, while average linkage seems to be working well. 21.2.3 Iris Data str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... iris_pca = prcomp(iris[,-5], scale = TRUE) iris_pca$rotation ## PC1 PC2 PC3 PC4 ## Sepal.Length 0.5210659 -0.37741762 0.7195664 0.2612863 ## Sepal.Width -0.2693474 -0.92329566 -0.2443818 -0.1235096 ## Petal.Length 0.5804131 -0.02449161 -0.1421264 -0.8014492 ## Petal.Width 0.5648565 -0.06694199 -0.6342727 0.5235971 lab_to_col = function (labels){ cols = rainbow (length(unique(labels))) cols[as.numeric (as.factor(labels))] } plot(iris_pca$x[,1], iris_pca$x[,2], col = lab_to_col(iris$Species), pch = 20) plot(iris_pca$x[,3], iris_pca$x[,4], col = lab_to_col(iris$Species), pch = 20) iris_pve = get_PVE(iris_pca) plot( cumsum(iris_pve), xlab = &quot;Principal Component&quot;, ylab = &quot;Cumulative Proportion of Variance Explained&quot;, ylim = c(0, 1), type = &#39;b&#39; ) iris_kmeans = kmeans(iris[,-5], centers = 3, nstart = 10) table(iris_kmeans$clust, iris[,5]) ## ## setosa versicolor virginica ## 1 0 48 14 ## 2 0 2 36 ## 3 50 0 0 iris_hc = hclust(dist(scale(iris[,-5])), method = &quot;complete&quot;) iris_cut = cutree(iris_hc , 3) ColorDendrogram(iris_hc, y = iris_cut, labels = names(iris_cut), main = &quot;Iris, Complete Linkage&quot;, branchlength = 1.5) table(iris_cut, iris[,5]) ## ## iris_cut setosa versicolor virginica ## 1 49 0 0 ## 2 1 21 2 ## 3 0 29 48 table(iris_cut, iris_kmeans$clust) ## ## iris_cut 1 2 3 ## 1 0 0 49 ## 2 23 0 1 ## 3 39 38 0 iris_hc = hclust(dist(scale(iris[,-5])), method = &quot;single&quot;) iris_cut = cutree(iris_hc , 3) ColorDendrogram(iris_hc, y = iris_cut, labels = names(iris_cut), main = &quot;Iris, Single Linkage&quot;, branchlength = 0.3) iris_hc = hclust(dist(scale(iris[,-5])), method = &quot;average&quot;) iris_cut = cutree(iris_hc , 3) ColorDendrogram(iris_hc, y = iris_cut, labels = names(iris_cut), main = &quot;Iris, Average Linkage&quot;, branchlength = 1) 21.3 External Links Hierarchical Cluster Analysis on Famous Data Sets - Using the dendextend package for in depth hierarchical cluster K-means Clustering is Not a Free Lunch - Comments on the assumptions made by \\(K\\)-means clustering. Principal Component Analysis - Explained Visually - Interactive PCA visualizations. 21.4 RMarkdown The RMarkdown file for this chapter can be found here. The file was created using R version 3.4.1 and the following packages: Base Packages, Attached ## [1] &quot;methods&quot; &quot;stats&quot; &quot;graphics&quot; &quot;grDevices&quot; &quot;utils&quot; &quot;datasets&quot; ## [7] &quot;base&quot; Additional Packages, Attached ## [1] &quot;sparcl&quot; &quot;MASS&quot; &quot;mlbench&quot; &quot;caret&quot; &quot;ggplot2&quot; &quot;lattice&quot; &quot;ISLR&quot; Additional Packages, Not Attached ## [1] &quot;Rcpp&quot; &quot;nloptr&quot; &quot;compiler&quot; &quot;plyr&quot; ## [5] &quot;class&quot; &quot;iterators&quot; &quot;tools&quot; &quot;digest&quot; ## [9] &quot;lme4&quot; &quot;evaluate&quot; &quot;tibble&quot; &quot;gtable&quot; ## [13] &quot;nlme&quot; &quot;mgcv&quot; &quot;rlang&quot; &quot;Matrix&quot; ## [17] &quot;foreach&quot; &quot;parallel&quot; &quot;yaml&quot; &quot;SparseM&quot; ## [21] &quot;e1071&quot; &quot;stringr&quot; &quot;knitr&quot; &quot;MatrixModels&quot; ## [25] &quot;stats4&quot; &quot;rprojroot&quot; &quot;grid&quot; &quot;nnet&quot; ## [29] &quot;rmarkdown&quot; &quot;bookdown&quot; &quot;minqa&quot; &quot;reshape2&quot; ## [33] &quot;car&quot; &quot;magrittr&quot; &quot;backports&quot; &quot;scales&quot; ## [37] &quot;codetools&quot; &quot;ModelMetrics&quot; &quot;htmltools&quot; &quot;splines&quot; ## [41] &quot;pbkrtest&quot; &quot;colorspace&quot; &quot;quantreg&quot; &quot;stringi&quot; ## [45] &quot;lazyeval&quot; &quot;munsell&quot; "]
]
