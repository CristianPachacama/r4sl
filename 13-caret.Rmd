# The `caret` Package

**Instructor's Note: This chapter is currently missing the usual narrative text. Hopefully it will be added later.**

Now that we have seen a number of classification (and regression) methods, and introduced cross-validation, we see the general outline of a predictive analysis:

- Select a method
- Test-train split the available data
- Decide on a set of candidate models via tuning parameters
- Select the best model (tuning parameters) using a cross-validated metric
- Use chosen model to make predictions
- Calculate relevant metrics on the test data

At face value it would seem like it should be easy to repeat this process for a number of different methods, however we have run into a number of difficulties attempting to do so with `R`.

- The `predict()` function seems to have a different behavior for each new method we see.
- Many methods have different cross-validation functions, or worse yet, no built-in process for cross-validation.
- Different methods have different handling of categorical predictors.

Thankfully, the `R` community has essentially provided a silver bullet for these issues, the [`caret`](http://topepo.github.io/caret/) package. Returning to the above list, we will see that a number of these tasks are directly addressed in the `caret` package.

- Test-train split the available data
    - `createDataPartition()` will take the place of our manual data splitting. It will also do some extra work to ensure that the train and test samples are somewhat similar.
- Decide on a set of candidate models via tuning parameters
    - `expand.grid()` is not a function in `caret`, but we will get in the habit of using it to specify a grid of tuning parameters.
- Select the best model (tuning parameters) using a cross-validated metric
    - `trainControl()` will setup cross-validation
    - `train()` is the workhorse of `caret`. It takes the following information then trains the requested model:
        - `form`, a formula, such as `y ~ .`
        - `data`
        - `method`, from a long list of possibilities
        - `preProcess` which allows for specification of things such as centering and scaling
        - `tuneGrid` which specifies the tuning parameters to train over
        - `trControl` which specifies the resampling scheme, that is, how cross-validation should be performed
- Use chosen model to make predictions
    - `predict()` used on objects of type `train` will be magical!

To illustrate `caret`, we return to our familiar `Default` data.

```{r}
data(Default, package = "ISLR")
```

```{r, message = FALSE, warning = FALSE}
library(caret)
```

We first test-train split the data using `createDataPartition`. Here we are using 75% of the data for training.

```{r}
set.seed(430)
default_idx = createDataPartition(Default$default, p = 0.75, list = FALSE)
default_trn = Default[default_idx, ]
default_tst = Default[-default_idx, ]
```

```{r}
default_glm = train(
  form = default ~ .,
  data = default_trn,
  method = "glm",
  family = "binomial",
  trControl = trainControl(method = "cv", number = 5)
)
```

```{r}
default_glm
```

```{r}
names(default_glm)
```

```{r}
default_glm$results
```

```{r}
default_glm$finalModel
```

```{r}
accuracy = function(actual, predicted) {
  mean(actual == predicted)
}
```

```{r}
# make predictions
head(predict(default_glm, newdata = default_trn))
```

```{r}
# train acc
accuracy(actual = default_trn$default,
         predicted = predict(default_glm, newdata = default_trn))
```

```{r}
# test acc
accuracy(actual = default_tst$default,
         predicted = predict(default_glm, newdata = default_tst))
```

```{r}
# get probs
head(predict(default_glm, newdata = default_trn, type = "prob"))
```

```{r}
default_knn = train(
  default ~ .,
  data = default_trn,
  method = "knn",
  trControl = trainControl(method = "cv", number = 5)
)
```

```{r}
default_knn
```

```{r}
default_knn = train(
  default ~ .,
  data = default_trn,
  method = "knn",
  trControl = trainControl(method = "cv", number = 5),
  preProcess = c("center", "scale"),
  tuneGrid = expand.grid(k = seq(1, 100, by = 1))
)
```

```{r}
default_knn
```

```{r}
plot(default_knn)
```

```{r}
ggplot(default_knn) + theme_bw()
```

```{r}
default_knn$bestTune
```

```{r}
get_best_result = function(caret_fit) {
  best_result = caret_fit$results[as.numeric(rownames(caret_fit$bestTune)), ]
  rownames(best_result) = NULL
  best_result
}
```

```{r}
get_best_result(default_knn)
```

```{r}
default_knn$finalModel
```

Notes to add later:

- Fewer ties with CV than simple test-train approach
- Default grid vs specified grid. `tuneLength`
- Create table summarizing results for `knn()` and `glm()`. Test, train, and CV accuracy. Maybe also show SD for CV.


## External Links

- [The `caret` Package](http://topepo.github.io/caret/index.html) - Reference documentation for the `caret` package in `bookdown` format.
- [`caret` Model List](http://topepo.github.io/caret/available-models.html) - List of available models in `caret`.


## RMarkdown

The RMarkdown file for this chapter can be found [**here**](13-caret.Rmd.Rmd). The file was created using `R` version `r paste0(version$major, "." ,version$minor)` and the following packages:

- Base Packages, Attached

```{r, echo = FALSE}
sessionInfo()$basePkgs
```

- Additional Packages, Attached

```{r, echo = FALSE}
names(sessionInfo()$otherPkgs)
```

- Additional Packages, Not Attached

```{r, echo = FALSE}
names(sessionInfo()$loadedOnly)
```




