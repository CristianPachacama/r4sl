# Regression for Statistical Learning

When using linear models in the past, we often emphasised distributional results, which were useful for creating and performing hypothesis tests. Frequently, when developing a linear regression model, part of our goal was to **explain** a relationship.

Now, we will ignore much of what we have learned and instead simply use regression as a tool to **predict**. Instead of a model which explains relationships, we seek a model which minimizes errors.

![](../images/regression.png)

First, note that a linear model is one of many methods used in regression. **Regression** is a form of **supervised learning**. Supervised learning deals with problems where there are both an input and an output. Regression problems are the subset of supervised learning problems with a **numeric** output.

Often one of the biggest differences between *statistical learning*, *machine learning*, *artificial intelligence* are the names use to describe variables and methods.

- The **input** can be called: input vector, feature vector, or predictors. The elements of these would be an input, feature, or predictor. The individual features can be either numeric or categorical.
- The **output** may be called: output, response, outcome, or target. The response must be numeric.

As an aside, some textbooks and statisticians use the terms independent and dependent variables to describe the response and the predictors. However, this practice can be confusing as those terms have specific meanings in probability theory.

*Our goal is to find a rule, algorithm, or function which takes as input a feature vector, and outputs a response which is as close to the true value as possible.* We often write the true, unknown relationship between the input and ouput $f(\bf{x})$. The relationship we learn, based on data, is written $\hat{f}(\bf{x})$.

From a statistical learning point-of-view, we write,

$$
Y = f(\bf{x}) + \epsilon
$$

to indicate that the true response is a function of both the unknown relationship, as well as some unlearnable noise.

```{r, message = FALSE, warning = FALSE, echo = FALSE}
library(readr)
Advertising = read_csv("../data/Advertising.csv")
```

To discuss linear models in the context of prediction, we return to the `Advertising` data from the previous chapter.

```{r}
Advertising
```

```{r, fig.height = 4, fig.width = 10, message = FALSE, warning = FALSE}
library(caret)
featurePlot(x = Advertising[ , c("TV", "Radio", "Newspaper")], y = Advertising$Sales)
```


## Assesing Model Accuracy

There are many metrics to assess the accuracy of a regression model. Most of these measure in some way the average error that the model makes. The metric that we will be most interested in is the root-mean-square error.

$$
\text{RMSE}(\hat{f}, \text{Data}) = \sqrt{\frac{1}{n}\displaystyle\sum_{i = 1}^{n}\left(y_i - \hat{f}(x_i)\right)^2}
$$

While for the sake of comparing models, the choice between RMSE and MSE is arbitraty, we have a preference for RMSE, as it has the same units as the response variable. Also, notice that in the prediction context MSE, refers to an average, whereas in an ANOVA context, the denominator for MSE may not be $n$.

For a linear model , the estimate of $f$, $\hat{f}$, is given by the fitted regression line.

$$
\hat{y}_i = \hat{f}(x_i)
$$

We can write an `R` function that will be useful for performing this calculation.

```{r}
rmse = function(actual, predicted) {
  sqrt(mean((actual - predicted) ^ 2))
}
```


## Model Complexity

Aside from how well a model predicts, we will also be very interested in the complexity (flexibility) of a model. For now, we will only considered nested linear models for simplicity. Then in that case, the more predictors that a model has, the more complex the model. For the sake of assigning a numerical value to the complexity of a linear model, we will use the number of predictors, $p$.

We write a simply `R` function to extract this information from a model.

```{r}
get_complexity = function(model) {
  length(coef(model)) - 1
}
```


## Test-Train Split

There is an issue with fitting a model to all availible data then using RMSE to determine how well the model predicts. It is essentially cheating! As a linear model becomes more complex, the RSS, thus RMSE, can never go up. It will only go down, or in very specific cases, stay the same.

This would suggest that to predict well, we should use the largest possible model! However, in reality we have hard fit to a specific dataset, but as soon as we see new data, a large model may in fact predict poorly. This is called **overfitting**.

Frequently we will take a dataset of interest and split it in two. One part of the datasets will be used to fit (train) a model, which we will call the **training** data. The remainder of the original data will be used to assess how well the model is predicting, which we will call the **test** data. Test data should *never* be used to train a model.

Note that sometimes the terms *evaluation set* and *test set* are used interchangably. We will give somewhat specific definitions to these later. For now we will simply use a single test set for a training set.

Here we use the `sample()` function to obtain a random sample of the rows of the original data. We then use those row numbers (and remaining row numbers) to split the data accordingly. Notice we used the `set.seed()` function to allow use to reproduce the same random split each time we perform this analysis.

```{r}
set.seed(9)
train_size = nrow(Advertising)

train_index = sample(1:train_size, size = trunc(0.50 * train_size))
train_data = Advertising[train_index, ]
test_data = Advertising[-train_index, ]
```

We will look at two measures that assess how well a model is predicting, the **train RMSE** and the **test RMSE**.

$$
\text{RMSE}_{\text{Train}} = \text{RMSE}(\hat{f}, \text{Train Data}) = \sqrt{\frac{1}{n_{\text{Tr}}}\displaystyle\sum_{i \in \text{Train}}^{}\left(y_i - \hat{f}(x_i)\right)^2}
$$

Here $n_{Tr}$ is the number of observations in the train set. Train RMSE will still always go down (or stay the same) as the complexity of a linear model increases. That means train RMSE will not be useful for comparing models, but checking that it decreases is a useful sanity check.

$$
\text{RMSE}_{\text{Test}} = \text{RMSE}(\hat{f}, \text{Train Data}) = \sqrt{\frac{1}{n_{\text{Te}}}\displaystyle\sum_{i \in \text{Test}}^{}\left(y_i - \hat{f}(x_i)\right)^2}
$$

Here $n_{Te}$ is the number of observations in the train set. Test RMSE uses the model fit using the training data, but evaluated on the unused test data. What happens to test RMSE as the size of the model increases? That is what we will investigate.

We will start with the simplest possible linear model, that is, a model with no predictors.

```{r}
fit_0 = lm(Sales ~ 1, data = train_data)
get_complexity(fit_0)

# train MSE
sqrt(mean((train_data$Sales - predict(fit_0, train_data)) ^ 2))
# test MSE
sqrt(mean((test_data$Sales - predict(fit_0, test_data)) ^ 2)) 
```

The previous two operations obtain the train and test RMSE. Since these are operations we are about to use repeatedly, we should use the function that we happen to have already written.

```{r}
# train MSE
rmse(actual = train_data$Sales, predicted = predict(fit_0, train_data))
# test MSE
rmse(actual = test_data$Sales, predicted = predict(fit_0, test_data))
```

This function can actually be improved for the inputs that we are using. We would like to obtain train and test RMSE for a fitted model, given a train or test dataset, and the appropriate response variable.

```{r}
get_rmse = function(model, data, response) {
  rmse(actual = data[, response], 
       predicted = predict(model, data))
}
```

By using this function, our code becomes easier to read, and it is more obvious what task we are accomnplishing. 

```{r}
get_rmse(model = fit_0, data = train_data, response = "Sales") # train MSE
get_rmse(model = fit_0, data = test_data, response = "Sales") # test MSE
```


## Adding Flexibility to Linear Models

Each successive model we fit will be more and more flexible using both interactions and polynomial terms. We will see the training error decrease each time the model is made more flexible. We expect the test error to decrease a number of times, then eventually start going up, as a result of overfitting.

```{r}
fit_1 = lm(Sales ~ ., data = train_data)
get_complexity(fit_1)

get_rmse(model = fit_1, data = train_data, response = "Sales") # train MSE
get_rmse(model = fit_1, data = test_data, response = "Sales") # test MSE
```

```{r}
fit_2 = lm(Sales ~ Radio * Newspaper * TV, data = train_data)
get_complexity(fit_2)

get_rmse(model = fit_2, data = train_data, response = "Sales") # train MSE
get_rmse(model = fit_2, data = test_data, response = "Sales") # test MSE
```

```{r}
fit_3 = lm(Sales ~ Radio * Newspaper * TV + I(TV ^ 2), data = train_data)
get_complexity(fit_3)

get_rmse(model = fit_3, data = train_data, response = "Sales") # train MSE
get_rmse(model = fit_3, data = test_data, response = "Sales") # test MSE
```

```{r}
fit_4 = lm(Sales ~ Radio * Newspaper * TV + 
           I(TV ^ 2) + I(Radio ^ 2) + I(Newspaper ^ 2), data = train_data)
get_complexity(fit_4)

get_rmse(model = fit_4, data = train_data, response = "Sales") # train MSE
get_rmse(model = fit_4, data = test_data, response = "Sales") # test MSE
```

```{r}
fit_5 = lm(Sales ~ Radio * Newspaper * TV +
           I(TV ^ 2) * I(Radio ^ 2) * I(Newspaper ^ 2), data = train_data)
get_complexity(fit_5)

get_rmse(model = fit_5, data = train_data, response = "Sales") # train MSE
get_rmse(model = fit_5, data = test_data, response = "Sales") # test MSE
```

## Choosing a Model

```{r, eval = FALSE}
fit_1 = lm(Sales ~ ., data = train_data)
fit_2 = lm(Sales ~ Radio * Newspaper * TV, data = train_data)
fit_3 = lm(Sales ~ Radio * Newspaper * TV + I(TV ^ 2), data = train_data)
fit_4 = lm(Sales ~ Radio * Newspaper * TV + 
           I(TV ^ 2) + I(Radio ^ 2) + I(Newspaper ^ 2), data = train_data)
fit_5 = lm(Sales ~ Radio * Newspaper * TV +
           I(TV ^ 2) * I(Radio ^ 2) * I(Newspaper ^ 2), data = train_data)
```

```{r}
model_list = list(fit_1, fit_2, fit_3, fit_4, fit_5)
```

```{r}
model_complexity = sapply(model_list, get_complexity)

train_rmse = sapply(model_list, get_rmse, data = train_data, response = "Sales")
test_rmse = sapply(model_list, get_rmse, data = test_data, response = "Sales")

# test_rmse = c(get_rmse(fit_1, test_data, "Sales"),
#               get_rmse(fit_2, test_data, "Sales"),
#               get_rmse(fit_3, test_data, "Sales"),
#               get_rmse(fit_4, test_data, "Sales"),
#               get_rmse(fit_5, test_data, "Sales"))
```


```{r}
plot(model_complexity, train_rmse, type = "b", 
     ylim = c(min(c(train_rmse, test_rmse)) - 0.02, 
              max(c(train_rmse, test_rmse)) + 0.02), 
     col = "dodgerblue", 
     xlab = "Model Size",
     ylab = "RMSE")
lines(model_complexity, test_rmse, type = "b", col = "darkorange")
```



Here we see the results summarized as a table. `fit_1` is the least flexible, and `fit_5` is the most flexible. We see the Train MSE decrease as flexibility increases. We see that the Test MSE is smallest for `fit_3`. (Note this may not be the best model, but it is the best model of the models we have seen in this example.)


| Model   | Train RMSE        | Test RMSE        | Predictors              |
|---------|-------------------|------------------|-------------------------|
| `fit_1` | `r train_rmse[1]` | `r test_rmse[1]` | `r model_complexity[1]` |
| `fit_2` | `r train_rmse[2]` | `r test_rmse[2]` | `r model_complexity[2]` |
| `fit_3` | `r train_rmse[3]` | `r test_rmse[3]` | `r model_complexity[3]` |
| `fit_4` | `r train_rmse[4]` | `r test_rmse[4]` | `r model_complexity[4]` |
| `fit_5` | `r train_rmse[5]` | `r test_rmse[5]` | `r model_complexity[5]` |

To summarize:

- **Underfitting:** *High* Train MSE, *High* Test MSE. Seen in `fit_1`.
- **Overfitting:** *Low* Train MSE, *High* Test MSE. Seen in `fit_5`.

- TODO: note about "echo"

- TODO: garunatee train
- TODO: note for test
- TODO: note about which is higher

- TODO: dependence on seed. will fix later

- TODO: relativity

- TODO: which model we would choose
- TODO: "generalize"

- TODO: exp vs pred, only look at test RMSE to decide. no hyp testing!
- TODO: collinearity, didn't even consider!
- TODO: assumptions? who cares? diagnostics! why bother? 
- TODO: fix relative reference
