# Regression for Statistical Learning


### Test-Train Split

Frequently we will take a dataset and split it in two. One part of the datasets will be used to fit a model, which we will call the **training** data. The remainder of the original data will be used to assess how well the model is predicting, which we will call the **testing** data. Test data should never be used to train a model.

TODO: note about train-evaluate-test difference. save for later.

Here we use the `sample()` function to obtain a random sample of the rows of the original data. We then use those row numbers (and remaining row numbers) to split the data accordingly.

TODO: note about `set.seed()`

```{r}
set.seed(1234)
dim(advertising)
n = nrow(advertising)

train_index = sample(1:n, size = trunc(.5*n))
train_data = advertising[train_index, ]
test_data = advertising[-train_index, ]
```


### Assesing Model Accuaracy

We will look at two measures that assess how well a model is predicting, the **train RMSE** and the **test RMSE**. (Root-mean-square error.) 

TODO: note why we prefer "root"

For linear models, training error will not increase as model flexibility increases, thus train RMSE is not useful for determining how well a model predicts. Instead we will only consider the test RMSE.

```{r}
fit = lm(Sales ~ ., data = train_data)
sqrt(mean((train_data$Sales - predict(fit, newdata = train_data)) ^ 2)) # train MSE
sqrt(mean((test_data$Sales - predict(fit, newdata = test_data)) ^ 2)) # test MSE
```

The previous two operations obtain the train and test MSE. Since these are operations we are about to use repeatedly, we write a function to make our code easier to write and read.

```{r}
get_rmse = function(actual, predicted) {
  sqrt(mean((actual - predicted) ^ 2))
}

get_mse(actual = train_data$Sales, predicted = predict(fit, newdata = train_data)) # train MSE
get_mse(actual = test_data$Sales, predicted = predict(fit, newdata = test_data)) # test MSE
```


### Adding Flexibility to Linear Models

Each successive model we fit will be more and more flexible using both interactions and polynomial terms. We will see the training error decrease each time the model is made more flexible. We expect the test error to decrease a number of times, then eventually start going up, as a result of overfitting.

```{r}
fit_1 <- lm(Sales ~ .^2, data = train_data)

#fit_1 <- lm(Sales ~ TV + Radio + Newspaper + TV:Radio + Radio:Newspaper + Newspaper:TV, data = train_data)
#summary(fit_1)
#fit_1 <- lm(Sales ~ TV*Radio + Radio*Newspaper + Newspaper*TV, data = train_data)
#summary(fit_1)

get_mse(train_data$Sales, predict(fit_1, train_data))
get_mse(test_data$Sales, predict(fit_1, test_data))
```

```{r}
fit_2 <- lm(Sales ~ Radio*Newspaper*TV, data = train_data)
get_mse(train_data$Sales, predict(fit_2, train_data))
get_mse(test_data$Sales, predict(fit_2, test_data))
```



```{r}
fit_3 <- lm(Sales ~ Radio*Newspaper*TV + I(TV ^ 2), data = train_data)
get_mse(train_data$Sales, predict(fit_3, train_data))
get_mse(test_data$Sales, predict(fit_3, test_data))
```



```{r}
fit_4 <- lm(Sales ~ Radio*Newspaper*TV + I(TV ^ 2) + I(Radio ^ 2) + I(Newspaper ^ 2), data = train_data)
get_mse(train_data$Sales, predict(fit_4, train_data))
get_mse(test_data$Sales, predict(fit_4, test_data))
```



```{r}
fit_5 <- lm(Sales ~ Radio*Newspaper*TV + I(TV ^ 2) * I(Radio ^ 2) * I(Newspaper ^ 2), data = train_data)
get_mse(train_data$Sales, predict(fit_5, train_data))
get_mse(test_data$Sales, predict(fit_5, test_data))
```









Here we see the results summarized as a table. `fit_1` is the least flexible, and `fit_5` is the most flexible. We see the Train MSE decrease as flexibility increases. We see that the Test MSE is smallest for `fit_3`. (Note this may not be the best model, but it is the best model of the models we have seen in this example.)


| Model   | Train MSE                                                 | Test MSE                                                |
|---------|-----------------------------------------------------------|---------------------------------------------------------|
| `fit_1` | `r get_mse(train_data$Sales, predict(fit_1, train_data))` | `r get_mse(test_data$Sales, predict(fit_1, test_data))` |
| `fit_2` | `r get_mse(train_data$Sales, predict(fit_2, train_data))` | `r get_mse(test_data$Sales, predict(fit_2, test_data))` |
| `fit_3` | `r get_mse(train_data$Sales, predict(fit_3, train_data))` | `r get_mse(test_data$Sales, predict(fit_3, test_data))` |
| `fit_4` | `r get_mse(train_data$Sales, predict(fit_4, train_data))` | `r get_mse(test_data$Sales, predict(fit_4, test_data))` |
| `fit_5` | `r get_mse(train_data$Sales, predict(fit_5, train_data))` | `r get_mse(test_data$Sales, predict(fit_5, test_data))` |



To summarize:

- **Underfitting:** *High* Train MSE, *High* Test MSE. Seen in `fit_1`.
- **Overfitting:** *Low* Train MSE, *High* Test MSE. Seen in `fit_5`.






TODO: exp vs pred
TODO: collinearity
TODO: not diagnostics!


TODO: note about transformations?


