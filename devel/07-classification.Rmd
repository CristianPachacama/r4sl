# Classification

**Classification** is a form of **supervised learning** where the response variable is categorical, as opposed to numeric for regression. *Our goal is to find a rule, algorithm, or function which takes as input a feature vector, and outputs a category which is the true category as often as possible.* 

![](../images/classification.png)

That is, the classifier $\hat{C}$ returns the predicted category $\hat{y}$.

$$
\hat{y}_i = \hat{C}(\bf x_i)
$$

To build our first classifier, we will use the `Default` dataset from the `ISLR` package.

```{r}
library(ISLR)
library(tibble)
as_tibble(Default)
```

Our goal is to properly classify individuals as defaulters based on student status, credit card balance, and income. Be aware that the response `default` is a factor, as is the predictor `student`. 

```{r}
is.factor(Default$default)
is.factor(Default$student)
```

As we did with regression, we test-train split our data. In this case, using 50% for each.

```{r}
set.seed(42)
train_index = sample(nrow(Default), 5000)
train_default = Default[train_index, ]
test_default = Default[-train_index, ]
```


## Visualization for Classification

Often, some simple visualizations can suggest simple classification rules. To quickly create some useful visualizations, we use the `featurePlot()` function from the `caret()` package.

```{r, message = FALSE, warning = FALSE}
library(caret)
```

A density plot can often suggest a simple split based on a numeric predictor. Essentially this plot graphs an estimate of density

$$
f_{X_i}(x_i \mid y = k)
$$

for each numeric predictor $x_i$ and each category $k$ for the response $y$.

```{r, fig.height = 5, fig.width = 10}


featurePlot(x = train_default[, c("balance", "income")], 
            y = train_default$default,
            plot = "density", 
            scales = list(x = list(relation = "free"), 
                          y = list(relation = "free")), 
            adjust = 1.5, 
            pch = "|", 
            layout = c(2, 1), 
            auto.key = list(columns = 2))
```

- `x` is a data from containing only **numeric predictors**. It would be nonsensical to estimate a density for a categorical predictor.
- `y` is the response variable. It needs to be a factor variable. If coded as `0` and `1`, you will need to coerce to factor for plotting.
- `plot` specifies the type of plot, here `density`.
- `scales` defines the scale of the axes for each plot. By default, the axis of each plot would be the same, which often is not useful, so the arguments here of a different axis for each plot will almost always be used.
- `adjust` specifies the amount of smoothing used for the density estimate.
- `pch` specifies the **p**lot **ch**aracter used for the bottom of the plot.
- `layout` places the individual plots into rows and columns. For some odd reason, it is given as (col, row).
- `auto.key` defines the key at the top of the plot. The number of columns should be the number of categories.

It seems that the income variable by itself is not particularly useful. However, there seems to be a big difference in default status at a `balance` of about 1400. We will use this information shortly.

```{r, fig.height = 5, fig.width = 10, message = FALSE, warning = FALSE}
featurePlot(x = train_default[, c("balance", "income")], 
            y = train_default$student,
            plot = "density", 
            scales = list(x = list(relation = "free"), 
                          y = list(relation = "free")), 
            adjust = 1.5, 
            pch = "|", 
            layout = c(2, 1), 
            auto.key = list(columns = 2))
```

Above, we create a similar plot, except with `student` as the response. We see that students often carry a slightly larger balance, and have far lower income. This will be useful to know when making more complicated classifiers.

```{r, fig.height = 6, fig.width = 6, message = FALSE, warning = FALSE}
featurePlot(x = train_default[, c("student", "balance", "income")], 
            y = train_default$default, 
            plot = "pairs",
            auto.key = list(columns = 2))
```

We can use `plot = "pairs"` to consider multiple variables at the same time. This plot reinforces using `balance` to create a classifier, and again shows that `income` seems not that useful.

```{r, fig.height = 6, fig.width = 6, message = FALSE, warning = FALSE}
library(ellipse)
featurePlot(x = train_default[, c("balance", "income")], 
            y = train_default$default, 
            plot = "ellipse",
            auto.key = list(columns = 2))
```

Similar to `pairs` is a plot of type `ellipse`, which requires the `ellipse` package. Here we only use numeric predictors, as essentially we are assuming multivariate normality. The ellipses plots and ellipses of equal density. This will be useful later when discussing LDA and QDA.


## A Simple Classifier

A very simple classifier is a rule based on a cutoff $c$ for a particular input variable $x$.

$$
\hat{C}(\text{x}) = 
\begin{cases} 
      1 & \text{x} > c \\
      0 & \text{x} \leq c 
   \end{cases}
$$

Based on the first plot, we believe we can use `balance` to create a reasonable classifier. In particular,

$$
\hat{C}(\text{balance}) = 
\begin{cases} 
      \text{Yes} & \text{balance} > 1400 \\
      \text{No} & \text{balance} \leq 1400 
   \end{cases}
$$

So we predict an individual is a defaulter if their `balance` is above 1400, and not a defaulter if the balance is 1400 or less.

```{r}
simple_class = function(x, cutoff) {
  ifelse(x > cutoff, "Yes", "No")
}
```
We write a simple `R` function that compares a variable to a cutoff, then use it to make predictions on the train and test sets with our chosen variable and cutoff.

```{r}
train_pred = simple_class(x = train_default$balance, cutoff = 1400)
test_pred = simple_class(x = train_default$balance, cutoff = 1400)
head(train_pred, n = 10)
```


## Metrics for Classification

In the classification setting, there are a large number of metrics to asses how well a classifier is performing.

One of the most obvious things to do is arrange predictions and true values in a cross table.

```{r}
(train_tab = table(predicted = train_pred, actual = train_default$default))
```

```{r}
(test_tab = table(predicted = test_pred, actual = test_default$default))
```

Often we give specific names to individual cells of these tables, and in the predictive setting, we would call this table a [**confusion matrix**](https://en.wikipedia.org/wiki/Confusion_matrix). Be aware, that the placement of Actual and Predicted values affects the names of the cells, and often the matrix may be presented transposed.

![](../images/confusion.png)

The `confusionMatrix()` function from the `caret` package can be used to obtain a wealth of additional information.

```{r}
train_con_mat = confusionMatrix(train_tab, positive = "Yes")
(test_con_mat = confusionMatrix(test_tab, positive = "Yes"))
```






$$
\text{Acc}(\hat{C}, \text{Data}) = \frac{1}{n}\sum_{i = 1}^{n}I(y_i = \hat{C}(\bf x_i))
$$

$$
\text{Acc}_{\text{Train}}(\hat{C}, \text{Train Data}) = \frac{1}{n_{Tr}}\sum_{i \in \text{Train}}^{}I(y_i = \hat{C}(\bf x_i))
$$

$$
\text{Acc}_{\text{Test}}(\hat{C}, \text{Test Data}) = \frac{1}{n_{Te}}\sum_{i \in \text{Test}}^{}I(y_i = \hat{C}(\bf x_i))
$$
```{r}
train_con_mat$overall["Accuracy"]
```

```{r}
test_con_mat$overall["Accuracy"]
```

- TODO: could also considerd error rate.
- TODO: Type I and Type II are hard to rememeber! these are easier!

$$
\text{Sens} = \text{True Positive Rate} = \frac{\text{TP}}{\text{P}} = \frac{\text{TP}}{\text{TP + FN}}
$$

```{r}
test_con_mat$byClass["Sensitivity"]
```

$$
\text{Spec} = \text{True Negative Rate} = \frac{\text{TN}}{\text{N}} = \frac{\text{TP}}{\text{TN + FP}}
$$

```{r}
test_con_mat$byClass["Specificity"]
```

from TEST table you get estimated probabilities

$$
\text{Prev} = \frac{\text{P}}{\text{Total Obs}}= \frac{\text{TP + FN}}{\text{Total Obs}}
$$

```{r}
train_con_mat$byClass["Prevalence"]
test_con_mat$byClass["Prevalence"]
```

```{r}

simpler_class = function(x, cutoff) {
  ifelse(x > cutoff, "No", "No")
}

```


```{r}
table(predicted = simpler_class(test_default$balance, cutoff = 1400), 
      actual = test_default$default)
```

```{r}
4835 / (4835 + 165) # test accuracy
1 - 0.0336 # 1 - (train prevelence)
1 - 0.033 # 1 - (test prevelence)
```


- TODO: confusion matrix won't work here

So in reality, to create a good classifier, we should obtain a test accuracy better than 0.967, which is obtained by simply manipulating the prevalence.

- TODO; difference of errors, what to expect based on prevalence, not 50% for binary class problem, BALANCE of problem

