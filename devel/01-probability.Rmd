---
output:
  pdf_document: default
  html_document: default
---
# Probability Review

We give a very brief review of some necessary probability concepts. As the treatment is less than complete, a list of references is given at the end of the chapter. For example, we ignore the usual recap of basic set theory.


## Probability Models

When discussing probability models, we speak of random **experiments** that produce one of a number of possible **outcomes**.

A **probability model** that describes the uncertainty of an experiment consists of two elements:

- The **sample space**, often denoted as $\Omega$, which is a set which contains all possible outcomes.
-  A **probability law** that assigns to an event $A$ a nonnegative number, $P[A]$, that represents how likely it is that event $A$ occurs as a result of the experiment.

We call $P[A]$ the **probability** of event $A$. An **event** $A$ could be any subset of the sample space, not necessarily a single possible outcome. The probability law must follow a number of rules, which are the result of a set of axioms that we introduce now.


## Probability Axioms

Given a sample space $\Omega$ for a particular experiment, the **probability law** associated with the experiement must satisfy the following axioms.

1. *Nonnegativity*: $P[A] \geq 0$ for any event $A \subset \Omega$.
2. *Normalization*: $P[\Omega] = 1$. That is, the probability of the entire space is 1.
3. *Additivity*: For mutually exclusive events $E_1, E_2, \ldots$
$$
P\left[\bigcup_{i = 1}^{\infty} E_i\right] = \sum_{i = 1}^{\infty} P[E_i]
$$

Using these axioms, many additional probability rules can easily be derived.


## Probability Rules

Complement

$$
P[A^c] = 1 - P[A]
$$

Addition Rule

$$
P[A \cup B] = P[A] + P[B] - P[A \cap B]
$$

$$
P[A \cup B] = P[A] + P[B]
$$

Conditional Probability

$$
P[A \mid B] = \frac{P[A \cap B]}{P[B]}
$$

Multiplication Rule 

$$
P[A \cap B] = P[A \mid B] \cdot P[B]
$$

$$
P\left[\textstyle\bigcap_{i = 1}^{n} E_i\right] = P[E_1] \cdot P[E_2 \mid E_1] \cdot P[E_3 \mid E_1 \cap E_2] \cdots P\left[E_n \mid \textstyle\bigcap_{i = 1}^{n - 1} E_i\right] 
$$

Bayes Rule


Independent Events


## Random Variables

- basic idea
- how events can be written in terms of RVs
- ind rvs


### Distributions

- basic, unmathematical defn
- discussion of parameters


### Continuous

- most commonly defined as possible values + pdf, can also use cdf or mgf
- normal as example


### Discrete

- most commonly defined as possible values + pmf (probabilites)
- binomial as example


### Several Random Variables

- joint, marginal, conditional


## Expectations

$$
\mathbb{E}[f(X)] \triangleq \sum_{x} f(x)p(x)
$$

$$
\mathbb{E}[f(X)] \triangleq \int f(x)p(x) dx
$$

$$
\mu_{X} = \text{mean}[X] \triangleq \mathbb{E}[X]
$$

$$
\sigma^2_{X} = \text{var}[X] \triangleq \mathbb{E}[(X - \mathbb{E}[X])^2]
$$

$$
\sigma_{X} = \text{sd}[X] \triangleq \sqrt{\sigma^2_{X}} = \sqrt{\text{var}[X]}
$$

- covariance


## Likelihood


## Bayesian

- post, prior, likelihood
- post ~ prior * likelihood


## References

- MIT book
- murphy bayes ML
- ross
- dasgupta  
- hogg tanis
- hogg craig
