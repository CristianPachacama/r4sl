# Shrinkage Methods



```{r}
data(Hitters, package = "ISLR")
```


```{r}
sum(is.na(Hitters))
sum(is.na(Hitters$Salary))
Hitters = na.omit(Hitters)
sum(is.na(Hitters))
```








We will use `glmnet()` and `cv.glmnet()` in the `glmnet` package to fit penalized regressions.

```{r}
library(glmnet)
```

The `glmnet` function does not allow the use of model formulas, so we setup the data for ease of use with `glmnet`.

```{r glmnet_setup}
X = model.matrix(Salary ~ ., Hitters)[, -1]
y = Hitters$Salary
```

First, we fit a regular linear regression, and note the size of the predictors' coefficients, and predictors' coefficients squared. (The two penalties we will use.)

```{r fit_lm}
fit = lm(Salary ~ ., Hitters)
coef(fit)
sum(abs(coef(fit)[-1]))
sum(coef(fit)[-1] ^ 2)
```


# Ridge Regression

We will now use **ridge regression**, which can be fit using `glmnet()` with `alpha = 0`. The two plots illustrate how much the coefficients are penalized for different values of $\lambda$. Notice none of the coefficients are forced to be zero.

```{r ridge}
fit_ridge = glmnet(X, y, alpha = 0)
plot(fit_ridge)
plot(fit_ridge, xvar = "lambda", label = TRUE)
dim(coef(fit_ridge))
```


We use cross-validation to select a good $\lambda$ value. The `cv.glmnet()`function uses 10 folds by default. The plot illustrates the MSE for the $\lambda$s considered. Two lines are drawn. The first is the $\lambda$ that gives the smallest MSE. The second is the $\lambda$ that gives an MSE within one standard error of the smallest.

```{r ridge_cv_plot}
fit_ridge_cv = cv.glmnet(X, y, alpha = 0)
plot(fit_ridge_cv)
```

The `cv.glmnet()` function returns several details of the fit for both $\lambda$ values in the plot. Notice the penalty terms are smaller than the full linear regression. (As we would expect.)

```{r ridge_cv_details}
coef(fit_ridge_cv)
coef(fit_ridge_cv, s = "lambda.min")
sum(coef(fit_ridge_cv, s = "lambda.min")[-1] ^ 2) # penalty term for lambda minimum
coef(fit_ridge_cv, s = "lambda.1se")
sum(coef(fit_ridge_cv, s = "lambda.1se")[-1] ^ 2) # penalty term for lambda one SE
#predict(fit_ridge_cv, X, s = "lambda.min")
#predict(fit_ridge_cv, X)
mean((y - predict(fit_ridge_cv, X)) ^ 2) # "train error"
sqrt(fit_ridge_cv$cvm) # CV-RMSEs
sqrt(fit_ridge_cv$cvm[fit_ridge_cv$lambda == fit_ridge_cv$lambda.min]) # CV-RMSE minimum
sqrt(fit_ridge_cv$cvm[fit_ridge_cv$lambda == fit_ridge_cv$lambda.1se]) # CV-RMSE one SE
```

```{r}
library(caret)
```

- TODO: can use carte train to use model formula, results are different due to randomization

```{r}
train(Salary ~ .,data = Hitters,
      method = "glmnet",
      trControl = trainControl(method = "cv", number = 10),
      tuneGrid = expand.grid(
        alpha = 0, 
        lambda = c(fit_ridge_cv$lambda.min, fit_ridge_cv$lambda.1se))
)
```


# Lasso

We will now use lasso, which can be fit using `glmnet()` with `alpha = 1`. The two plots illustrate how much the coefficients are penalized for different values of $\lambda$. Notice some of the coefficients are forced to be zero.

```{r lasso}
fit_lasso = glmnet(X, y, alpha = 1)
plot(fit_lasso)
plot(fit_lasso, xvar = "lambda", label = TRUE)
dim(coef(fit_lasso))
```

Again, to actually pick a $\lambda$, we will use cross-validation. `cv.glmnet()` by default uses 10 folds. The plot is similar to the ridge plot. Notice along the top is the number of features in the model. (Which changed in this plot.)

```{r lasso_cv_plot}
fit_lasso_cv = cv.glmnet(X, y, alpha = 1)
plot(fit_lasso_cv)
```

`cv.glmnet()` returns several details of the fit for both $\lambda$ values in the plot. Notice the penalty terms are again smaller than the full linear regression. (As we would expect.) Some coefficients are 0.

```{r lasso_cv_details}
coef(fit_lasso_cv)
coef(fit_lasso_cv, s = "lambda.min")
sum(abs(coef(fit_lasso_cv, s = "lambda.min")[-1])) # penalty term for lambda minimum
coef(fit_lasso_cv, s = "lambda.1se")
sum(abs(coef(fit_lasso_cv, s = "lambda.1se")[-1])) # penalty term for lambda one SE
#predict(fit_lasso_cv, X, s = "lambda.min")
#predict(fit_lasso_cv, X)
mean((y - predict(fit_lasso_cv, X)) ^ 2) # "train error"
sqrt(fit_lasso_cv$cvm)
sqrt(fit_lasso_cv$cvm[fit_lasso_cv$lambda == fit_lasso_cv$lambda.min]) # CV-RMSE minimum
sqrt(fit_lasso_cv$cvm[fit_lasso_cv$lambda == fit_lasso_cv$lambda.1se]) # CV-RMSE one SE
```

Sometimes, the output from `glmnet()` can be overwhelming. The `broom` package can help with that.

```{r broom}
library(broom)
#lassoModCV
tidy(fit_lasso_cv)
glance(fit_lasso_cv) # the two lambda values of interest
```



















## Simulation Study, p > n

```{r}
set.seed(1234)
n = 1000
p = 5500
X = replicate(p, rnorm(n = n))
beta = c(1, 1, 1, rep(0, 5497))
z = X %*% beta
prob = exp(z) / (1 + exp(z))
y = rbinom(length(z), size = 1, prob = prob)
```



```{r}
library(glmnet)
fit_cv = cv.glmnet(X, y, family = "binomial", alpha = 1)
plot(fit_cv)
```

```{r}
head(coef(fit_cv), n = 10)
```

```{r}
fit_cv$nzero
```

```{r}
fit_1se = glmnet(X, y, family = "binomial", lambda = fit_cv$lambda.1se)
which(as.vector(as.matrix(fit_1se$beta)) != 0)
```




```{r}
plot(glmnet(X, y, family = "binomial"))
```


```{r}
plot(glmnet(X, y, family = "binomial"), xvar = "lambda")
```




```{r}
fit_cv$lambda.min
fit_cv$lambda.1se
```





```{r}
library(caret)
cv_5 = trainControl(method = "cv", number = 5)
lasso_grid = expand.grid(alpha = 1, 
                         lambda = c(fit_cv$lambda.min, fit_cv$lambda.1se))
lasso_grid
```

```{r}
fit_lasso = train(
  X,
  as.factor(y),
  method = "glmnet",
  trControl = cv_5,
  tuneGrid = lasso_grid
  )
fit_lasso$results
```













## External Links

- [`glmnet` Web Vingette](https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html) - Deatils from the package developers.
- https://github.com/topepo/caret/issues/116


## RMarkdown

The RMarkdown file for this chapter can be found [**here**](15-shrink.Rmd). The file was created using `R` version `r paste0(version$major, "." ,version$minor)` and the following packages:

- Base Packages, Attached

```{r, echo = FALSE}
sessionInfo()$basePkgs
```

- Additonal Packages, Attached

```{r, echo = FALSE}
names(sessionInfo()$otherPkgs)
```

- Additonal Packages, Not Attached

```{r, echo = FALSE}
names(sessionInfo()$loadedOnly)
```
