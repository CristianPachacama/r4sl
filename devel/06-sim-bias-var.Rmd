# Simulating the Biasâ€“Variance Tradeoff

Consider the general regression setup 

$$
y = f(\mathbf x) + \epsilon
$$

with

$$
E[\epsilon] = 0 \quad \text{and} \quad \text{var}(\epsilon) = \sigma^2.
$$

## Bias-Variance Decomposition

Using $\hat{f}(\mathbf x)$, trained with data, to estimate $f(\mathbf x)$, we are interested in the expected prediction error. Specifically, considered making a prediction of $y_0 = f(\mathbf x_0) + \epsilon$ at the point $\mathbf x_0$.

In that case, we have

$$
E\left[\left(y_0 - \hat{f}(\mathbf x_0)\right)^2\right] = \text{bias}\left(\hat{f}(\mathbf x_0)\right)^2 + \text{var}\left(\hat{f}(\mathbf x_0)\right) + \sigma^2.
$$

Recall the definition of the bias of an estimate.

$$
\text{bias}\left(\hat{f}(\mathbf x_0)\right) = E\left[\hat{f}(\mathbf x_0)\right] - f(\mathbf x_0)
$$

So, we have decomposed the error into two types; **reducible** and **irreducible**. The reducible can be further decomposed into the squared **bias** and **variance** of the estimate. We can "control" these through our choice of model. The irreducible, is noise, that should not and cannot be modeled.

## Simulation

We will illustrate this decomposition, and the resulting bias-variance tradeoff through simulation. Suppose we would like a train a model to learn the function $f(x) = x^2$.

```{r}
f = function(x) {
  x ^ 2
}
```

More specifically,

$$
y = x^2 + \epsilon
$$
where

$$
\epsilon \sim N(\mu = 0, \sigma^2 = 0.3^2).
$$

We write a function which generates data accordingly.

```{r}
get_sim_data = function(f, sample_size = 100) {
  x = runif(n = sample_size, 0, 1)
  y = f(x) + rnorm(n = sample_size, mean = 0, sd = 0.3)
  data.frame(x, y)
}
```

To get a sense of the data, we generate one simulated dataset, and fit the four models that we will be interested in.

```{r}
sim_data = get_sim_data(f, sample_size = 100)

fit_1 = lm(y ~ 1, data = sim_data)
fit_2 = lm(y ~ poly(x, degree = 1), data = sim_data)
fit_3 = lm(y ~ poly(x, degree = 2), data = sim_data)
fit_4 = lm(y ~ poly(x, degree = 3), data = sim_data)
```

Plotting these four trained models, we see that the zero predictor model does very poorly. The single predictor model is reasonable, but we can see that the two and three predictor models seem more appropriate. Between these latter two, it is hard to see which seems more appropriate.

```{r, fig.height = 8, fig.width = 8}
plot(y ~ x, data = sim_data)
grid = seq(from = 0, to = 1, by = 0.01)
lines(grid, f(grid), col = "black", lwd = 5)
lines(grid, predict(fit_1, newdata = data.frame(x = grid)), col = "red", lwd = 2)
lines(grid, predict(fit_2, newdata = data.frame(x = grid)), col = "blue", lwd = 2)
lines(grid, predict(fit_3, newdata = data.frame(x = grid)), col = "green", lwd = 3)
lines(grid, predict(fit_4, newdata = data.frame(x = grid)), col = "orange", lwd = 2)
```

We will now use simulation to estimate the bias, variance, and mean squared error for the estimates given by these models at the point $x_0 = 0.9$. We use simulation to complete this task, as performing the exact calculations are always difficult, and often impossible.

```{r, fig.height = 8, fig.width = 8}
set.seed(1)
n_sims = 1000
n_models = 4
x0 = 0.9
predictions = matrix(0, nrow = n_sims, ncol = n_models)
sim_data = get_sim_data(f, sample_size = 100)
plot(y ~ x, data = sim_data, col = "white", xlim = c(0.75, 1), ylim = c(0, 1.5))

for (i in 1:n_sims) {
  
  sim_data = get_sim_data(f, sample_size = 100)

  fit_1 = lm(y ~ 1, data = sim_data)
  fit_2 = lm(y ~ poly(x, degree = 1), data = sim_data)
  fit_3 = lm(y ~ poly(x, degree = 2), data = sim_data)
  fit_4 = lm(y ~ poly(x, degree = 3), data = sim_data)
  
  lines(grid, predict(fit_1, newdata = data.frame(x = grid)), col = "red", lwd = 1)
  lines(grid, predict(fit_2, newdata = data.frame(x = grid)), col = "blue", lwd = 1)
  lines(grid, predict(fit_3, newdata = data.frame(x = grid)), col = "green", lwd = 1)
  lines(grid, predict(fit_4, newdata = data.frame(x = grid)), col = "orange", lwd = 1)

  predictions[i, ] <- c(
    predict(fit_1, newdata = data.frame(x = x0)),
    predict(fit_2, newdata = data.frame(x = x0)),
    predict(fit_3, newdata = data.frame(x = x0)),
    predict(fit_4, newdata = data.frame(x = x0))
  )
}

lines(grid, f(grid), col = "white", lwd = 5)
lines(grid, f(grid), col = "black", lwd = 5, lty = 2)
```

- TODO: new obs at particular point x0

```{r}
eps = rnorm(n = n_sims, mean = 0, sd = 0.3)
y0 = f(x0) + eps
```


## Bias-Variance Tradeoff

```{r}
get_bias = function(estimate, truth) {
  mean(estimate - truth)
}

get_mse = function(estimate, truth) {
  mean((estimate - truth) ^ 2)
}
```

```{r}
bias = apply(predictions, 2, get_bias, y0)
variance = apply(predictions, 2, var)
mse = apply(predictions, 2, get_mse, y0)
```

```{r}
bias ^ 2 + variance + var(eps)
mse
```

```{r, echo = FALSE}
options(scipen = 999)
```

| Model   | Squared Bias     | Variance (Of Estimate) | MSE        |
|---------|------------------|------------------------|------------|
| `fit_1` | `r bias[1] ^ 2`  | `r variance[1]`        | `r mse[1]` |
| `fit_2` | `r bias[2] ^ 2`  | `r variance[2]`        | `r mse[2]` |
| `fit_3` | `r bias[3] ^ 2`  | `r variance[3]`        | `r mse[3]` |
| `fit_4` | `r bias[4] ^ 2`  | `r variance[4]`        | `r mse[4]` |


```{r, echo = FALSE}
options(scipen = 0)
```


```{r}
all(diff(bias ^ 2) < 0)
all(diff(variance) > 0)
diff(mse)
```


- TODO: note about difference do to estimating with simulation


- TODO: summarize with table, talk about where balance is found, bias inc, var dec
