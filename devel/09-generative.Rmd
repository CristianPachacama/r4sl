# Generative Models

In this chapter, we continue our discussion of classification methods. We introduce three new methods, each a **generative** method. This in comparison to logistic regression, which is a **discriminative** method.

Generative methods model the joint probability, $p(x, y)$, often by assuming some distribution for the conditional distribution of $X$ given $Y$, $f(x \mid y)$. Bayes theorem is then applied to classify according to $p(y \mid x)$. Discriminative methods directly model this conditional, $p(y \mid x)$. A detailed discussion and analysis can be found in [Ng and Jordan, 2002](https://papers.nips.cc/paper/2020-on-discriminative-vs-generative-classifiers-a-comparison-of-logistic-regression-and-naive-bayes.pdf).

Each of the methods in this chapter will use Bayes theorem to build a classifier.

$$
p_k(x) = P[Y = k \mid {\mathbf X} = {\mathbf x}] = \frac{\pi_k \cdot f_k({\mathbf x})}{\sum_{i = 1 }^{K} \pi_k \cdot f_k({\mathbf x})}
$$

We call $p_k(x)$ the **posterior** probability, which we will estimate then use to create classifications. The $\pi_k$ are called the **prior** probabilities for each class $k$. That is, $P[y = k]$, unconditioned on $X$. The $f_k({\mathbf x})$ are called the **likelihoods**, which are indexed by $k$ to denote that they are conditional on the classes. The denominator is often referred to as a **normalizing constant**.

The methods will differ by placing different modeling assumptions to the likelihoods, $f_k({\mathbf x})$. For each method, the priors could be learned from data or pre-specified.

For each, classification are made to the class with the highest posterior probability, which is equivalent to the class with the largest

$$
\log(\hat{\pi}_k \cdot \hat{f}_k({\mathbf x})).
$$

By substituting the correct likelihoods, simplifying, and eliminating unnecessary terms, we could derive the discriminant function for each.

To illustrate these new methods, we return to the iris data, which you may remember has three classes. We create a number of plots to refresh our memory.

```{r}
set.seed(430)
iris_obs = nrow(iris)
iris_index = sample(iris_obs, size = trunc(0.50 * iris_obs))
# iris_index = sample(iris_obs, size = trunc(0.10 * iris_obs))
iris_train = iris[iris_index, ]
iris_test = iris[-iris_index, ]
```


```{r, fig.height=8, fig.width=8}
caret::featurePlot(x = iris_train[, c("Sepal.Length", "Sepal.Width", 
                               "Petal.Length", "Petal.Width")], 
            y = iris_train$Species,
            plot = "density", 
            scales = list(x = list(relation = "free"), 
                          y = list(relation = "free")), 
            adjust = 1.5, 
            pch = "|", 
            layout = c(2, 2), 
            auto.key = list(columns = 3))
```

```{r, fig.height=8, fig.width=8}

caret::featurePlot(x = iris_train[, c("Sepal.Length", "Sepal.Width", 
                               "Petal.Length", "Petal.Width")], 
            y = iris_train$Species,
            plot = "ellipse",
            auto.key = list(columns = 3))
```


```{r, fig.height=4, fig.width=7}
caret::featurePlot(x = iris_train[, c("Sepal.Length", "Sepal.Width", 
                               "Petal.Length", "Petal.Width")], 
            y = iris_train$Species,
                  plot = "box",
                  scales = list(y = list(relation="free"),
                                x = list(rot = 90)),
                  layout = c(4, 1))
```

Especially based on the pairs plot, we see that it should not be too difficult to find a good classifier.


## Linear Discriminant Analysis

LDA assumes that the predictors are multivariate normal conditioned on the classes.

$$
{\mathbf X} \mid Y = k \sim N(\mu_k, \Sigma)
$$

$$
f_k({\mathbf x}) = \frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}}\exp\left[-\frac{1}{2}(\mathbf x - \mu_k)^{\prime}\Sigma^{-1}(\mathbf x - \mu_k)\right]
$$

```{r}
library(MASS)
iris_lda = lda(Species ~ ., data = iris_train)
iris_lda
```

Here we see the estimated $\hat{\pi}_k$ and $\hat{\mu}_k$ for each class.

```{r}
names(predict(iris_lda, iris_train))
head(predict(iris_lda, iris_train)$class, n = 10)
head(predict(iris_lda, iris_train)$posterior, n = 10)
```

As we should come to expect, the `predict()` funciton 

- TODO: returns list


```{r}
iris_lda_train_pred = predict(iris_lda, iris_train)$class
iris_lda_test_pred = predict(iris_lda, iris_test)$class
```

- TODO: store preds


```{r}
accuracy = function(actual, predicted) {
  mean(actual == predicted)
}
```

- TODO: simple function

```{r}
accuracy(predicted = iris_lda_train_pred, actual = iris_train$Species)
accuracy(predicted = iris_lda_test_pred, actual = iris_test$Species)
```

- TODO; calculate

```{r}
table(predicted = iris_lda_test_pred, actual = iris_test$Species)
```

- TODO: predicted two classes perfectly

```{r}
iris_lda_flat = lda(Species ~ ., data = iris_train, prior = c(1, 1, 1) / 3)
iris_lda_flat
```

- TODO: specify prior


```{r}
iris_lda_flat_train_pred = predict(iris_lda_flat, iris_train)$class
iris_lda_flat_test_pred = predict(iris_lda_flat, iris_test)$class
```


```{r}
accuracy(predicted = iris_lda_flat_train_pred, actual = iris_train$Species)
accuracy(predicted = iris_lda_flat_test_pred, actual = iris_test$Species)
```







## Quadratic Discriminant Analysis

- TODO: likelihood assumptions

```{r}
iris_qda = qda(Species ~ ., data = iris_train)
iris_qda
```

- TODO: number of parameters vs number of obs in each class


```{r}
iris_qda_train_pred = predict(iris_qda, iris_train)$class
iris_qda_test_pred = predict(iris_qda, iris_test)$class
```



```{r}
accuracy(predicted = iris_qda_train_pred, actual = iris_train$Species)
accuracy(predicted = iris_qda_test_pred, actual = iris_test$Species)
```


```{r}
table(predicted = iris_qda_test_pred, actual = iris_test$Species)
```

- TODO: lda with quadratic terms


## Naive Bayes

- TODO: likelihood assumptions

```{r}
library(e1071)
iris_nb = naiveBayes(Species ~ ., data = iris_train)
iris_nb
```

- TODO: how mean estimates are the same as above

```{r}
head(predict(iris_nb, iris_train))
head(predict(iris_nb, iris_train, type = "class"))
head(predict(iris_nb, iris_train, type = "raw"))
```

```{r}
iris_nb_train_pred = predict(iris_nb, iris_train)
iris_nb_test_pred = predict(iris_nb, iris_test)
```



```{r}
accuracy(predicted = iris_nb_train_pred, actual = iris_train$Species)
accuracy(predicted = iris_nb_test_pred, actual = iris_test$Species)
```

```{r}
table(predicted = iris_nb_test_pred, actual = iris_test$Species)


```







- TODO: results comparison, not meaningful with this data, note differences to expect, nb good with large p, etc





