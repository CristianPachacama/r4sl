# Logistic Regression

```{r}
library(ISLR)
library(tibble)
as_tibble(Default)
```


```{r}
set.seed(42)
train_index = sample(nrow(Default), 5000)
train_default = Default[train_index, ]
test_default = Default[-train_index, ]
```



```{r}
Default_for_lm = Default
Default_for_lm$default = as.numeric(Default_for_lm$default) - 1

model_lm = lm(default ~ balance, data = Default)
```


```{r}
# repeat with glm
model_glm = glm(default ~ balance, data = train_default, family = "binomial")

mean(ifelse(predict(model_glm, newdata = train_default, type = "link") > 0, "Yes", "No") == train_default$default)
mean(ifelse(predict(model_glm, newdata = train_default, type = "response") > 0.5, "Yes", "No") == train_default$default)

```


```{r}
# add complexity, get test accuracy or error curve
```


```{r}
# roc curve
```


```{r}

```


```{r}
# multiclass, neural networks
```


```{r}

```

- a lot of this will only be done on the board:

- how linear regression makes sense mean(0/1 @ x) ~~ prob, but isn't limited to (0,1)
    - how coercion to 0/1 from factor works
    
- add something about bayes classifier, and how we are directly optimizing towards?


    
- how to read the "usual" logistic regression plot (which isn't a great plot)
    - note how ISL illustrates proportion at each x

- how probabilities create classifications
    - how chaning usual cutoff usually lowers acc, but changes sens and spec, possible for better
    - ROC curve?

- confounding example from slides
- multiple

- >2 outcomes
    - only show how to do in `R`
    - note its coming from a NN package
    - will discuss later

- boundries?, with two numeric x's


- prob cutoff of 0.5 is same as logodds curoff of 0

- decide on: a division of "doing in R" and handwritten notes and slides
