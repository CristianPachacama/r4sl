# k-Nearest Neighbors

- TODO: non-parametric method
- TODO: but tuning parameter

- TODO: no really training done, so `knn()` basically replaces `predict()`

- TODO: normalization
- TODO: `scale()`
- TODO: compare results

- TODO: split data into X and Y

```{r}
library(ISLR)
```

- TODO: note that input must be numeric

```{r}
set.seed(42)
Default$student = as.numeric(Default$student) - 1
default_index = sample(nrow(Default), 5000)
default_train = Default[default_index, ]
default_test = Default[-default_index, ]
```



```{r}
# training data
X_default_train = default_train[, -1]
y_default_train = default_train[, 1]

# testing data
X_default_test = default_test[, -1]
y_default_test = default_test[, 1]
```


```{r}
library(class)
knn(train = X_default_train, test = X_default_test, cl = y_default_train, k = 3)
```


```{r}
accuracy = function(actual, predicted) {
  mean(actual == predicted)
}
```

- TODO: change error to acc

```{r}
set.seed(42)
k_to_try = 100
acc_k = rep(0, k_to_try)
for(i in 1:k_to_try) {
  pred = knn(train = scale(X_default_train), 
             test = scale(X_default_test), 
             cl = y_default_train, 
             k = i)
  acc_k[i] = accuracy(y_default_test, pred)
}
min(acc_k)

```

- TODO: make plot nicer

```{r, fig.height=5, fig.width=10}
plot(acc_k, type = "b", col = "dodgerblue", cex = 1, pch = 20, 
     xlab = "k, number of neighbors", ylab = "classification accuracy",
     main = "Accuracy vs Neighbors")
abline(v = which(error_k == max(acc_k)), col = "darkorange", lwd = 1.5)
abline(h = max(acc_k), col = "grey", lty = 2)
abline(h = mean(y_default_test == "No"), col = "grey", lty = 2)
```

```{r}

```



- TODO: if ties, pick least flexible, highest k model

- TODO: eventually "averaging" over entire dataset, acc approached prevelence


```{r}
mean(y_default_test == "No")
```



- TODO: default data
- TODO: iris data


- TODO: plot acc vs k


- TODO: no need for predict

- TODO: how to obtain probabilites




```{r}
train = rbind(iris3[1:25, , 1], iris3[1:25, , 2], iris3[1:25, , 3])
test = rbind(iris3[26:50, , 1], iris3[26:50, , 2], iris3[26:50, , 3])
cl = factor(c(rep("s", 25), rep("c", 25), rep("v", 25)))
attributes(knn(train, test, cl, k = 3, prob = TRUE))
```








-TODO: plot which shows how CV follows test. see here: http://www.milanor.net/blog/cross-validation-for-predictive-analytics-using-r/





- TODO: regression data
- TODO: boston
- TODO: plot with fitting for various k, single predictor. back to boston. see old.


